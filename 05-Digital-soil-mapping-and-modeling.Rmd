# Digital Soil Mapping and Modeling

```{r setup-ch5, include=FALSE}
knitr::opts_chunk$set(
  echo = TRUE,
  message = FALSE,
  warning = FALSE,
  fig.width = 10,
  fig.height = 7
)

# ── Root path to the SoilFER-Training-Resources repository ──
# All data, scripts, and outputs are referenced relative to this path.
RESOURCES_ROOT <- "../SoilFER-Training-Resources"

# Convenience paths
DATA_DIR    <- file.path(RESOURCES_ROOT, "01_data", "module5")
SCRIPTS_DIR <- file.path(RESOURCES_ROOT, "02_scripts", "module5")
OUTPUTS_DIR <- file.path(RESOURCES_ROOT, "03_outputs", "module5")

# Figures are stored locally in the Manual repo so they render online (GitHub Pages)
FIGURES_DIR <- "images/module5"

# ── terra temporary directory (avoid filling system drive) ──
# Students on laptops with small C: drives can change this to a larger disk.
TERRA_TMPDIR <- file.path(tempdir(), "terra_module5")
dir.create(TERRA_TMPDIR, showWarnings = FALSE, recursive = TRUE)
if (requireNamespace("terra", quietly = TRUE)) {
  terra::terraOptions(tempdir = TERRA_TMPDIR)
}

# ── Check that the Resources repository exists ──
if (!dir.exists(RESOURCES_ROOT)) {
  stop(
    "SoilFER-Training-Resources repository not found at:\n  ",
    normalizePath(RESOURCES_ROOT, mustWork = FALSE), "\n\n",
    "Make sure both repositories are cloned side-by-side:\n",
    "  SoilFER-Org/\n",
    "    SoilFER-Training-Manual/\n",
    "    SoilFER-Training-Resources/\n"
  )
}

# ── Check that large raster files exist ──
required_tifs <- c("Env_Cov_250m_KANSAS.tif",
                   "HighRes_Cov_100m_KANSAS.tif",
                   "Cropland_Mask_KANSAS.tif")
missing_tifs <- required_tifs[!file.exists(file.path(DATA_DIR, required_tifs))]

if (length(missing_tifs) > 0) {
  message("------------------------------------------------------------")
  message("WARNING: Required raster files are missing from:")
  message("  ", DATA_DIR)
  message("")
  message("Missing: ", paste(missing_tifs, collapse = ", "))
  message("")
  message("To download them, run one of:")
  message("  source('", file.path(DATA_DIR, "download_module5_data.R"), "')")
  message("  # or in PowerShell:")
  message("  .\\download_module5_data.ps1")
  message("")
  message("See DATA_SOURCES.md in the module5 data folder for details.")
  message("------------------------------------------------------------")
}

# ── Check that pre-generated figures exist ──
if (!dir.exists(FIGURES_DIR) || length(list.files(FIGURES_DIR, pattern = "\\.png$")) == 0) {
  message("------------------------------------------------------------")
  message("WARNING: Pre-generated figures are missing from:")
  message("  ", FIGURES_DIR)
  message("")
  message("To generate them, run:")
  message("  Rscript ", file.path(SCRIPTS_DIR, "generate_chapter_figures.R"))
  message("------------------------------------------------------------")
}

# ── Check that model and validation outputs exist ──
required_models <- c(
  "models/selected_features_pH.rds",
  "models/ranger_model_pH.rds",
  "models/selected_features_Clay_pH_Class.rds",
  "models/ranger_model_Clay_pH_Class.rds",
  "models/ordinal_encoding.rds",
  "models/selected_features_pH_Class.rds",
  "models/ranger_model_pH_Class_ordinal.rds"
)
required_validation <- c(
  "validation/metrics_pH.csv",
  "validation/metrics_Clay_pH_Class.csv",
  "validation/metrics_pH_Class_ordinal.csv"
)
required_outputs <- c(required_models, required_validation)
missing_outputs <- required_outputs[!file.exists(file.path(OUTPUTS_DIR, required_outputs))]

if (length(missing_outputs) > 0) {
  message("------------------------------------------------------------")
  message("WARNING: Required model/validation files are missing from:")
  message("  ", OUTPUTS_DIR)
  message("")
  message("Missing: ", paste(basename(missing_outputs), collapse = ", "))
  message("")
  message("These files are generated by running the modelling script:")
  message("  source('", file.path(SCRIPTS_DIR, "modelling_&_mapping.R"), "')")
  message("------------------------------------------------------------")
}

# Set seed for approximate reproducibility
set.seed(2024)
```

## Part I: Theoretical Foundations {-}

------------------------------------------------------------------------

## Fundamentals of Digital Soil Mapping

### Introduction to Digital Soil Mapping

Digital soil mapping (DSM) is a modern approach that uses quantitative methods to create soil maps. It combines field observations, laboratory data, and environmental information within a spatial framework [@Lagacherie2008]. DSM represents a significant change from traditional soil survey, where maps were created based mainly on expert knowledge and manual interpretation of landscapes.

Traditional soil survey relied on a soil surveyor's conceptual model of the landscape. Surveyors used aerial photographs, satellite images, and field observations to identify soil patterns. However, this approach had important limitations: maps were subjective, difficult to update, and lacked quantitative measures of uncertainty [@Minasny2016].

The evolution towards digital methods began in the 1970s when researchers started using computers to store and analyse soil data. @Webster1979 demonstrated early digital soil cartography in England, while soil information systems were developed to manage soil databases. During the 1980s, the development of geographic information systems (GIS) and geostatistics provided new tools for spatial analysis. The 1990s brought advances in digital terrain modelling, data mining, and machine learning methods such as classification trees and neural networks.

The success of DSM in the early 2000s resulted from several factors: increased availability of digital elevation models and satellite imagery, greater computing power, development of data mining tools, and a growing global demand for soil information with uncertainty estimates [@Minasny2016]. Universities and research centres began producing soil maps that were previously only made by national survey agencies.

Today, DSM is defined as the creation of spatial soil information systems using field and laboratory observations combined with spatial inference methods [@Lagacherie2006]. The theoretical framework for DSM was formalised in 2003, establishing the conceptual basis that guides current practice.

### The SCORPAN Framework

The conceptual and methodological foundations of DSM were formalised by the *scorpan* framework [@McBratney2003]. This model represents a quantitative expansion of the classical factors of soil formation originally proposed by Dokuchaev in 1883 and later refined by @Jenny1941. While Jenny's *clorpt* model (climate, organisms, relief, parent material, time) was developed to explain soil genesis, the *scorpan* framework transforms this explanatory approach into an empirical predictive tool for spatial mapping.

The fundamental distinction between the two frameworks lies in their purpose. Jenny's model describes how soils form through the interaction of environmental factors over time—it is a qualitative, theoretical framework. The *scorpan* approach, in contrast, does not attempt to explain soil formation mechanistically but rather exploits empirical relationships between soil and environmental variables for prediction [@McBratney2003]. As noted by the authors, the direction of causality is not considered: where evidence of a relationship exists, it can be used for prediction regardless of whether the factor causes or is caused by soil properties.

The *scorpan* acronym identifies seven environmental factors that influence soil distribution:

-   **s**: soil—other properties of the soil at a point, including information from prior maps, proximal or remote sensing, or expert knowledge
-   **c**: climate—properties of the environment such as precipitation, temperature, and evapotranspiration
-   **o**: organisms—vegetation, fauna, or human activity, often represented by land cover or spectral indices
-   **r**: relief—topographic attributes derived from digital elevation models
-   **p**: parent material—lithology and geological substrate
-   **a**: age—the time factor representing soil development duration or geomorphic surface age
-   **n**: space—spatial position expressed as geographic coordinates

Two additional factors distinguish *scorpan* from Jenny's original formulation. First, soil itself (s) is included as a predictor because soil properties can be predicted from other soil attributes measured at the same location or from existing soil maps. Second, the spatial factor (n) explicitly incorporates geographic position, which captures spatial trends and autocorrelation not explained by the other environmental factors [@McBratney2003]. The spatial coordinates can be used directly or transformed into derived variables such as distance to coast or distance from discharge areas.

The general pedometric model is formulated as:

$$S_c = f(s, c, o, r, p, a, n) + \varepsilon$$

or for soil attributes:

$$S_a = f(s, c, o, r, p, a, n) + \varepsilon$$

This can be expressed more compactly as:

$$S = f(Q) + \varepsilon$$

where $S$ denotes a soil class ($S_c$) or attribute ($S_a$) at a specific location, $f(Q)$ represents a deterministic function of the *scorpan* covariates, and $\varepsilon$ represents the spatially autocorrelated residual. The complete methodological approach is termed *scorpan-SSPFe* (Soil Spatial Prediction Function with spatially autocorrelated errors), where deterministic predictions from $f(Q)$ are complemented by geostatistical modelling of residuals to account for spatial structure not captured by the covariates [@McBratney2003].

Each *scorpan* factor is represented by one or more environmental covariates. For example, climate (c) may be represented by mean annual precipitation and temperature; relief (r) by slope, aspect, curvature, and terrain wetness index; and organisms (o) by satellite-derived vegetation indices such as NDVI. The selection of covariates depends on data availability, mapping scale, and the soil property being predicted.

The *scorpan-SSPFe* approach represents a paradigm shift in soil mapping [@McBratney2003]. While the conventional Jenny model follows a deductive-nomological framework, the *scorpan* approach follows an inductive-statistical model of explanation. Both frameworks share the same ontological basis—soil as a function of environment—but differ fundamentally in methodology and apparatus. The *scorpan* approach requires digital data, computing resources, and statistical methods for fitting $f()$, whereas traditional mapping relies primarily on expert mental models and field interpretation.

### Statistical Theory for Predictive Soil Mapping

The spatial heterogeneity of soil properties results from complex interactions among soil-forming factors that are often only partially characterised. While the deterministic influences of climate, organisms, relief, parent material, and time are well-recognised, their synergistic interaction over pedogenic timescales presents significant challenges for mechanistic modelling [@HeuvelinkWebster2001]. Soil varies continuously in space and time, and any description of this variation is inevitably incomplete. Soil scientists must therefore represent this variation using models that combine deterministic and stochastic components [@HeuvelinkWebster2001].

#### Mechanistic versus Empirical Approaches

Mechanistic modelling (also referred to as process-based or white-box modelling) involves the mathematical representation of soil systems based on known physical, chemical, and biological processes. Unlike empirical models that rely on statistical correlations, mechanistic models attempt to simulate the actual mechanisms of soil formation and function—such as water movement, solute transport, organic matter decomposition, and soil erosion—over time and space [@Wadoux2021]. These models typically use systems of differential equations to describe the state of the soil system and its evolution.

@Wadoux2021 distinguish between mechanistic and functional (empirical) models for soil mapping. Mechanistic models have structures based on mechanisms derived from knowledge of physical processes and soil chemical and biological reactions. Examples include soil erosion and deposition models, soil-landscape evolution models, and soil carbon dynamics models. However, the main problems with mechanistic approaches are that they require adequate mechanistic understanding of major soil processes, need extensive input data that are often unavailable, have many parameters that are difficult to infer, are computationally challenging, and typically do not quantify prediction uncertainty.

Although advancements have been made in modelling vertical soil variation through process-based approaches, these methods remain primarily experimental and are not yet suitable for large-scale operational soil mapping [@Zhang2024]. Consequently, most operational DSM follows an **empirical approach**. Instead of simulating every physical process, empirical models use statistical relationships between soil properties and environmental covariates to make predictions [@McBratney2003].

Recent research has explored hybrid approaches that integrate process-oriented (PO) and machine learning (ML) models. @Zhang2024 proposed a framework where PO model outputs serve as additional covariates for ML models, combining the ability of PO models to capture temporal dynamics with the spatial prediction accuracy of ML models. This integration represents a promising avenue for dynamic soil mapping that addresses one of the ten challenges for the future of pedometrics [@Wadoux2021].

#### Sources of Residual Variance

Regression models frequently account for only a portion of the total soil variance. This limitation arises from several factors [@Hengl2007; @Wadoux2021]:

1. **Model structure limitations**: The deterministic model structure may fail to represent actual mechanistic pedogenic processes adequately.
2. **Missing causal factors**: Models often exclude significant causal factors that influence soil variation.
3. **Covariate limitations**: Environmental covariates serve as incomplete proxies for true soil-forming factors (see section below).
4. **Measurement errors**: Covariates contain inherent measurement errors or suffer from scale (support) mismatches relative to soil observations.
5. **Spatial scale effects**: Soil properties vary at spatial scales from the atomic to the global, and the factors causing variation at one scale may differ from those at other scales [@Wadoux2021].

Given these constraints, soil spatial models often exhibit substantial residual variance. When these residuals demonstrate spatial autocorrelation—quantifiable through variogram analysis—the application of kriging techniques to the residuals can significantly improve prediction accuracy [@McBratney2003; @Hengl2007].

#### Covariates as Proxies of Soil-Forming Factors

In DSM, we use the terms **proxy** and **covariate** when referring to environmental data layers used to predict soil properties. A proxy is an indirect measurement of a variable that is difficult to measure directly. Since we cannot measure the exact historical climate or the precise biological activity that has occurred at every location over millennia, we use available data that correlates with these processes [@McBratney2003].

For example, a **Normalized Difference Vegetation Index (NDVI)** map derived from satellite imagery is not the "organisms" factor itself but serves as a proxy for vegetation biomass and productivity, which influences organic matter inputs to the soil.

While the *scorpan* framework is based on soil-forming factors, the digital layers we input into models are mathematical representations rather than the physical factors themselves. There are four primary reasons for this distinction [@Lagacherie2008; @Kempen2009]:

1. **Temporal mismatch**: Soil formation occurs over centuries or millennia, but most climate covariates are based on the last 30 to 50 years of data. These represent proxies for the long-term climate that actually formed the soil.
2. **Information loss and simplification**: A Digital Elevation Model represents relief, but it is a grid of numbers representing average heights within pixels. It misses fine-scale terrain nuances that influence water flow and erosion.
3. **Measurement error**: Every digital layer contains inherent errors. Satellite sensors have noise, and climate station data are interpolated across spatial gaps.
4. **Indirect correlation**: Often, a covariate represents multiple factors simultaneously. Elevation (relief) correlates strongly with temperature (climate), making it a statistical surrogate rather than a pure physical factor.

#### The Universal Model of Soil Variation

In DSM, we mathematically decompose soil variation into distinct components to better understand and predict soil patterns. This decomposition follows the **Universal Model of Soil Variation** [@Webster2007; @HeuvelinkWebster2001]:

$$Z(s) = m(s) + \varepsilon'(s) + \varepsilon''(s)$$

Where:

- $Z(s)$: The value of a soil property at a specific location ($s$)
- $m(s)$: The **deterministic component** (trend)—the part of soil variation explained using environmental covariates
- $\varepsilon'(s)$: The **spatially correlated stochastic component**—variation that follows a spatial pattern but is not captured by covariates
- $\varepsilon''(s)$: The **pure noise**—measurement errors and fine-scale variation

This decomposition provides the theoretical foundation for hybrid interpolation methods such as regression-kriging, where the trend $m(s)$ is modelled as a function of environmental covariates and the spatially correlated residual $\varepsilon'(s)$ is interpolated using kriging [@Hengl2007; @Odeh1995].

@Hengl2007 demonstrated that regression-kriging explicitly separates trend estimation from residual interpolation, allowing the use of arbitrarily complex regression forms. The method can be expressed as:

$$\hat{Z}(s_0) = \hat{m}(s_0) + \hat{\varepsilon}'(s_0)$$

where $\hat{m}(s_0)$ is predicted from the regression model and $\hat{\varepsilon}'(s_0)$ is obtained by kriging the regression residuals.

#### Extending the Model: Space, Depth, and Time (3D+T)

While a 2D map describes soil variation at a single depth layer, soils are three-dimensional bodies that change over time. The Universal Model can be generalised to include depth ($d$) and time ($t$) [@HeuvelinkWebster2001]:

$$Z(s, d, t) = m(s, d, t) + \varepsilon'(s, d, t) + \varepsilon''(s, d, t)$$

This **3D+T (spatio-temporal)** model allows tracking how soil properties change with depth and evolve over time. @Heuvelink2016 noted that 3D kriging, where depth is treated as a third dimension, has important advantages because predictions at any depth interval can be made. However, modelling vertical variation realistically is challenging due to zonal and geometric anisotropies and discontinuities at horizon boundaries. Soil observations are typically averages over depth intervals and cannot be treated as vertical points. Mass-preserving splines have been developed to address this issue [@Bishop1999; @Orton2014], though they introduce additional uncertainties.

::: Highlights
**Example: Weekly Soil Moisture Mapping**
A classic example of the need for spatio-temporal modelling is **soil moisture**. Unlike soil texture (which changes very slowly over decades), soil moisture fluctuates daily or weekly based on rainfall and evaporation. Creating weekly maps of soil moisture requires a space-time framework considering location ($s$), time ($t$), and depth ($d$) simultaneously.
:::

However, moving from 2D to 3D+T significantly increases analytical complexity. Each additional dimension requires more model parameters to be estimated, and the data requirements increase substantially.

> **2D+T and 3D+T models**: Because data requirements for 2D+T and 3D+T models are high, they remain experimental in many regions. For most projects, focusing on high-quality 2D or 3D (depth-only) mapping is the standard starting point. Recent developments in integrating process-oriented models with machine learning offer promising approaches for capturing temporal dynamics while maintaining spatial prediction accuracy [@Zhang2024].

### Types of Soil Variables

Soil variables are classified into two fundamental categories based on their mathematical nature, and this distinction has important implications for the choice of prediction methods [@McBratney2003]:

**Continuous Variables (Soil Properties):** These are attributes measured on a numerical scale that can take any value within a range. Common examples include clay content (%), soil organic carbon concentration (g kg⁻¹), pH, bulk density (g cm⁻³), and cation exchange capacity (cmol kg⁻¹). From a pedological perspective, these properties typically change gradually across space, reflecting the continuous nature of soil variation [@HeuvelinkWebster2001]. However, some properties may exhibit abrupt changes at lithological boundaries or landscape discontinuities.

**Categorical Variables (Soil Classes):** These represent qualitative groups or discrete soil attributes. Examples include soil taxonomic units (e.g., Mollisol, Alfisol), drainage classes, or horizon designations. Traditional soil classification follows a "top-down" model where soils are divided into mutually exclusive classes with sharp conceptual boundaries [@Odgers2011]. However, the intrinsic variability of soil means that soil usually does not exist as discrete bodies with sharp boundaries between them—either physical boundaries in the natural landscape or conceptual boundaries in the feature space [@HeuvelinkWebster2001; @Odgers2011].

**The Soil Continuum Problem:** In reality, soil grades more or less gradually from one class to another, both in the landscape and in feature space [@Odgers2011]. This has led to the development of continuous classification approaches using fuzzy set theory, where objects can belong to multiple classes with membership values that sum to one [@HeuvelinkWebster2001]. Fuzzy classification provides a more realistic representation of soil variation by quantifying the degree of similarity between soil profiles and class centroids.

### Prediction Methods

The mathematical nature of soil variables determines the type of statistical method used to create predictive maps [@McBratney2003]:

**Regression for Continuous Properties:** For continuous variables, regression algorithms establish numerical relationships between soil properties and environmental covariates. Methods range from linear approaches (ordinary least squares, generalised linear models) to non-linear techniques (generalised additive models, regression trees, neural networks, and ensemble methods such as Random Forest). The advantage of tree-based methods over linear models is their ability to handle nonlinearity and non-additive behaviour without requiring interactions to be pre-specified [@Breiman1984; @McBratney2003].

**Classification for Soil Classes:** For categorical variables, classification algorithms calculate the probability of a location belonging to specific soil classes. Methods include discriminant analysis, logistic regression, classification trees, and machine learning classifiers. Rather than producing a single "hard" classification, modern approaches often output class membership probabilities for each location, providing a richer representation of prediction uncertainty [@McBratney2003; @Kempen2009].

**Hybrid Approaches:** Some methods can handle both continuous and categorical data simultaneously. Classification and regression trees (CART) exemplify this flexibility, automatically determining splitting variables and points based on the data structure [@Breiman1984]. Tree-based models are particularly valued for their interpretability compared to methods like neural networks or generalised additive models [@McBratney2003].

### Nonlinearity and Uncertainty

A fundamental characteristic of soil-environment relationships is that they are often **non-linear and complex** [@Minasny2016]. Environmental factors do not always influence soil properties in simple, linear ways. Terrain attributes may have threshold effects, climate interactions may be multiplicative, and biological processes introduce additional complexity. Because predictive models are simplified representations of natural processes, they are never fully accurate.

Unlike traditional soil maps that provide a single deterministic view, digital soil mapping enables the quantification of **prediction uncertainty**. This means that for every prediction, we can provide measures of confidence indicating where the model is reliable and where additional data might be needed [@Padarian2023; @Styc2021]. Uncertainty can be expressed through:

- **Prediction intervals**: Upper and lower bounds within which the true value is expected to fall with a specified probability (e.g., 90% confidence interval)
- **Standard deviation or variance**: Measures of prediction spread around the mean estimate
- **Prediction interval coverage probability (PICP)**: Assessment of whether stated confidence intervals actually contain the expected proportion of true values [@Styc2021]

However, @Padarian2023 noted that uncertainty estimates remain underutilised in practice. Around 50% of DSM studies still do not report uncertainty assessments, and end users often find uncertainty maps difficult to interpret alongside target variable maps. This challenge has motivated new approaches for communicating uncertainty, including variable-resolution maps where pixel size encodes prediction confidence.

------------------------------------------------------------------------

## Part II: Practical Application {-}

------------------------------------------------------------------------

## Workflow Overview

This section guides you through the interpretation of results from the Digital Soil Mapping workflow. We apply the theoretical concepts from Part I to predict three types of soil variables in Kansas, USA, each requiring a different modelling approach:

| Variable type | Example | Approach | What the map shows |
|---------------|---------|----------|-------------------|
| **Continuous** | pH (numeric scale) | Quantile Regression Forest | Predicted value + uncertainty at each pixel |
| **Nominal** | Clay_pH_Class (unordered categories) | Classification Random Forest | Most likely class + class probabilities |
| **Ordinal** | pH_Class (ordered categories) | Integer-encoded Regression | Predicted class + instability near boundaries |

The type of soil variable determines model choice, validation metrics, and how we interpret uncertainty. The workflow also demonstrates the **Area of Applicability (AOA)** analysis to identify where predictions can be trusted.

```{r load-packages-ch5}
# ── Check and load required packages ──
required_pkgs <- c("tidyverse", "caret", "knitr")
missing_pkgs  <- required_pkgs[!sapply(required_pkgs, requireNamespace, quietly = TRUE)]

if (length(missing_pkgs) > 0) {
  message("Installing missing packages: ", paste(missing_pkgs, collapse = ", "))
  install.packages(missing_pkgs, repos = "https://cloud.r-project.org")
}

library(tidyverse)
library(caret)
library(knitr)
```

------------------------------------------------------------------------

## Continuous Modelling (pH)

### Nature of the Variable

Soil pH is a **continuous variable**: it takes numeric values on a scale (typically 4–9 in mineral soils). We predict the actual pH value at each location rather than placing observations into categories.

Following the *scorpan* framework, we model pH as a function of environmental covariates that represent climate, organisms, relief, and other soil-forming factors. The relationship is empirical—we do not simulate the chemical processes that determine pH, but rather exploit correlations between pH and measurable environmental variables.

**What this approach produces:**

-   A map of predicted pH values across the study area
-   An uncertainty map (standard deviation) showing where predictions are more or less reliable
-   Validation metrics that measure prediction error in pH units

**What this approach cannot do:**

-   It cannot tell you *why* pH varies (only which covariates correlate with it)
-   It does not guarantee accurate predictions in areas unlike the training data
-   It does not account for temporal changes in pH

### Data and Covariates

The workflow begins by loading environmental covariates representing the *scorpan* factors: terrain attributes derived from digital elevation models (r), climate variables (c), and remote sensing indices representing vegetation (o).

```{r example-covariate, echo=FALSE, fig.cap="Example environmental covariate layer from the 250m resolution stack."}
knitr::include_graphics(file.path(FIGURES_DIR,"example_covariate.png"))
```

Each covariate layer represents one environmental variable at 100m resolution. These are **proxies** for the true soil-forming factors—they correlate with soil pH but do not directly represent the chemical processes involved. The model learns which covariates best explain pH variation from the training points, then applies that relationship to predict pH across the entire area.

### Feature Selection (Boruta)

Not all covariates contribute equally to prediction. Some may be irrelevant, redundant, or introduce noise. **Boruta** is an algorithm that identifies which covariates genuinely help predict pH by comparing each variable's importance against randomly shuffled "shadow" versions of itself [@Kursa2010].

```{r boruta-features}
selected_features <- readRDS(file.path(OUTPUTS_DIR, "models", "selected_features_pH.rds"))

print(paste("Number of selected features:", length(selected_features)))
print("Selected features:")
print(selected_features)
```

```{r boruta-plot, echo=FALSE, fig.cap="Boruta feature selection results. Green = confirmed important, red = rejected, yellow = tentative."}
knitr::include_graphics(file.path(FIGURES_DIR,"boruta_pH.png"))
```

**How to read this plot:**

-   **Green boxes**: Variables confirmed as important. Their importance exceeds random noise.
-   **Red boxes**: Variables rejected. They do not help more than randomly shuffled data.
-   **Yellow boxes**: Tentative (borderline). We include them to avoid discarding potentially useful information.

Check if the selected features make pedological sense. For pH, you might expect climate (precipitation, temperature) and terrain (slope position, wetness) to be important because they influence weathering and drainage.

### Model Training

We use **Random Forest** to predict pH. Random Forest is an ensemble method that builds many decision trees, each trained on a random subset of the data, and averages their predictions [@Breiman2001]. This approach aligns with the empirical tradition in DSM—we fit the function $f(Q)$ from the *scorpan* model using data-driven methods rather than process-based equations.

**Why Random Forest for DSM:**

-   It handles nonlinear relationships between covariates and soil properties automatically
-   It captures interactions between covariates without requiring explicit specification
-   It does not require the analyst to specify the functional form of relationships
-   It provides measures of variable importance for interpretation

**What Random Forest does NOT solve:**

-   It cannot extrapolate beyond the range of covariate values in training data
-   It does not account for spatial autocorrelation in model fitting (the $\varepsilon'(s)$ term from the Universal Model)
-   It may underperform if training data are spatially clustered or biased toward certain environments

```{r model-summary}
model_cont <- readRDS(file.path(OUTPUTS_DIR, "models", "ranger_model_pH.rds"))

# Show model summary
print(model_cont)

# Best hyperparameters
print("Best hyperparameters:")
print(model_cont$bestTune)
```

The model was trained using **10-fold cross-validation repeated 5 times**. The data were split into 10 parts; the model was trained on 9 parts and tested on the held-out part, rotating through all combinations. This process was repeated 5 times with different random splits.

**What cross-validation evaluates:** How well the model predicts pH for observations not used in training.

**What cross-validation does NOT evaluate:** Whether predictions are reliable in geographic areas far from training points. This is a fundamental limitation—cross-validation assumes that held-out observations are representative of prediction locations. If your training points are spatially clustered, the model may perform worse in unsampled regions than cross-validation suggests [@Roberts2017].

### Variable Importance

```{r varimp-plot, echo=FALSE, fig.cap="Variable importance from the trained Random Forest model."}
knitr::include_graphics(file.path(FIGURES_DIR,"varimp_pH.png"))
```

This plot ranks covariates by how much they contribute to predictions. Variables at the top reduce prediction error the most when used in tree splits.

Variable importance helps link empirical predictions back to pedological understanding. If a climate variable ranks highest, soil pH variation may be driven primarily by regional precipitation or temperature gradients. If a terrain variable ranks highest, local topographic position and drainage may dominate. However, importance does not imply causation—it reflects statistical association, not mechanistic influence.

### Validation Metrics

```{r validation-metrics}
metrics <- read_csv(file.path(OUTPUTS_DIR, "validation", "metrics_pH.csv"), show_col_types = FALSE)
kable(metrics, digits = 3, caption = "Cross-validation metrics for pH prediction")
```

**What each metric tells you:**

| Metric | Meaning | Interpretation |
|--------|---------|----------------|
| **ME (Mean Error)** | Average difference between predicted and observed. | Values near 0 indicate no systematic bias. Positive ME = model overpredicts on average. |
| **RMSE** | Root Mean Square Error—typical size of prediction errors in pH units. | Compare to the range of pH in your data. RMSE of 0.5 in data ranging 5–8 is different from RMSE of 0.5 in data ranging 6–7. |
| **R²** | Proportion of variance explained by the model. | R² = 0.32 means the model captures about one-third of pH variation. The remainder is unexplained variance ($\varepsilon'$ and $\varepsilon''$ from the Universal Model). |
| **NSE** | Nash-Sutcliffe Efficiency. | Values > 0 mean the model is better than simply predicting the mean everywhere. |

An R² of ~0.32 is modest but not unusual in DSM. This reflects the sources of residual variance discussed in Part I: measurement error, missing covariates, scale mismatches, and fine-scale soil variation that cannot be captured at 100m resolution.

```{r scatterplot, echo=FALSE, fig.cap="Observed vs Predicted pH from cross-validation. The red line shows perfect 1:1 prediction."}
knitr::include_graphics(file.path(FIGURES_DIR,"scatterplot_pH.png"))
```

**How to read this scatterplot:**

-   Points on the red line are perfectly predicted.
-   Vertical spread around the line shows prediction uncertainty.
-   If points curve away from the line at high or low values, the model may be compressing extreme predictions toward the mean. This is a common Random Forest behaviour because predictions are averages of tree outputs and cannot exceed the range of training data.

### Prediction Maps

#### Mean pH Map

```{r map-mean, echo=FALSE, fig.cap="Predicted mean pH across the study area."}
knitr::include_graphics(file.path(FIGURES_DIR,"map_mean_pH.png"))
```

This map represents the deterministic component $\hat{m}(s)$ from the Universal Model—the predicted pH value at each 100m pixel based on environmental covariates.

Look for patterns that match your understanding of soil-landscape relationships. Higher pH in flat, poorly-drained areas may indicate carbonate accumulation. Lower pH on steep slopes may reflect greater leaching. These patterns should be consistent with the *scorpan* logic: relief influences drainage, which affects weathering and pH.

**Warning signs of artefacts:**

-   Abrupt straight-line boundaries may indicate covariate artefacts (e.g., satellite image edges, DEM tile boundaries)
-   Uniform areas with no variation may indicate limited covariate information in those regions

#### Uncertainty Map (Standard Deviation)

```{r map-sd, echo=FALSE, fig.cap="Prediction uncertainty (SD) - higher values indicate more uncertain predictions."}
knitr::include_graphics(file.path(FIGURES_DIR,"map_sd_pH.png"))
```

For continuous predictions, we express uncertainty as a **standard deviation (SD)**. This comes from Quantile Regression Forest: each of the trees in the forest makes its own prediction. Where trees agree, SD is low. Where trees disagree, SD is high.

This SD map provides spatially explicit uncertainty—a key advantage of DSM over traditional mapping. High SD areas are where the model is uncertain, often occurring where:

-   Covariate combinations are unlike the training data
-   Soil patterns are complex or highly variable
-   Training data are sparse

Use this map to prioritise where additional sampling would improve predictions and to communicate confidence to map users.

#### Coefficient of Variation Map

```{r map-cv, echo=FALSE, fig.cap="Coefficient of Variation (%) - relative uncertainty."}
knitr::include_graphics(file.path(FIGURES_DIR,"map_cv_pH.png"))
```

CV expresses uncertainty relative to the predicted value: CV = (SD / Mean) × 100.

**Caution:** For pH, CV is meaningful because pH values are never near zero. For variables like soil organic carbon that can approach zero, CV becomes artificially inflated where predicted values are low and should be interpreted carefully.

------------------------------------------------------------------------

## Nominal Classification (Clay_pH_Class)

### Nature of the Variable

**Clay_pH_Class** is a **nominal variable**: it consists of unordered categories created by combining clay content classes (low, medium, high) with pH classes (acid, neutral, alkaline). The result is up to 9 combinations like "low_clay_acid" or "high_clay_alkaline".

These categories have no inherent order. "low_clay_acid" is not "less than" "medium_clay_neutral" in any meaningful sense. This distinguishes nominal from ordinal variables and affects both the model and how we evaluate predictions.

**What this approach produces:**

-   A map showing the most likely class at each pixel
-   Probability maps for each class
-   An uncertainty map based on classification confidence

**What this approach cannot do:**

-   It cannot capture gradual transitions (a pixel is assigned to exactly one class)
-   It struggles when some classes have few training samples
-   Accuracy metrics treat all misclassifications equally, whether the error is "neighbouring" or distant in attribute space

**Important:** Classifying variables that are naturally continuous (like clay content and pH) introduces artificial boundaries. Observations near class thresholds could easily fall into adjacent classes with slightly different measurements—this is the "soil continuum problem" discussed in Part I.

### Feature Selection

```{r boruta-nominal}
selected_features_nom <- readRDS(file.path(OUTPUTS_DIR, "models", "selected_features_Clay_pH_Class.rds"))

print(paste("Number of selected features:", length(selected_features_nom)))
print("Selected features:")
print(selected_features_nom)
```

The same Boruta procedure is applied, but now selecting features that help distinguish between soil classes rather than predicting a numeric value.

### Model Training

```{r model-nominal}
model_nom <- readRDS(file.path(OUTPUTS_DIR, "models", "ranger_model_Clay_pH_Class.rds"))

print(model_nom)
print("Best hyperparameters:")
print(model_nom$bestTune)
```

For classification, Random Forest assigns each observation to the class that receives the most "votes" from individual trees. Each tree classifies the observation, and the forest aggregates these votes into class probabilities. This provides a form of fuzzy classification—rather than a single hard assignment, we obtain probabilities for each class [@McBratney2003].

### Validation

```{r validation-nominal}
metrics_nom <- read_csv(file.path(OUTPUTS_DIR, "validation", "metrics_Clay_pH_Class.csv"), show_col_types = FALSE)
kable(metrics_nom, digits = 3, caption = "Classification metrics for Clay_pH_Class")
```

**What each metric tells you:**

| Metric | Meaning | Interpretation |
|--------|---------|----------------|
| **Accuracy** | Proportion of observations correctly classified. | With 9 classes, random guessing gives ~11% accuracy. Values above this indicate the model learned meaningful patterns. |
| **Kappa** | Agreement beyond chance, accounting for class frequencies. | Kappa 0.2–0.4 is "fair" agreement; 0.4–0.6 is "moderate". |

Accuracy of ~39% is better than random but limited. This is typical when classifying continuous variables into categories, especially with class imbalance (some classes have many more observations than others).

```{r confusion-matrix, echo=FALSE, fig.cap="Confusion matrix showing predicted vs observed classes. Diagonal cells are correct predictions."}
knitr::include_graphics(file.path(FIGURES_DIR,"confusion_matrix_Clay_pH_Class.png"))
```

**How to read the confusion matrix:**

-   **Diagonal cells** (top-left to bottom-right): Correct predictions. Higher numbers = better.
-   **Off-diagonal cells**: Misclassifications. Look for patterns—which classes are confused with each other?
-   **Empty rows or columns**: Classes with no correct predictions. This indicates the model ignores minority classes, a common problem with imbalanced data.

If "high_clay_acid" is often misclassified as "medium_clay_acid", the model struggles to distinguish clay content but captures pH better. This suggests clay-related covariates may be weaker predictors than pH-related covariates.

### Prediction Maps

#### Predicted Class Map

```{r map-class-max, echo=FALSE, fig.cap="Predicted class based on maximum probability."}
knitr::include_graphics(file.path(FIGURES_DIR,"map_class_max_prob.png"))
```

Each pixel is assigned to the class with the highest predicted probability. Check if spatial patterns match landscape expectations: high clay in floodplains? Acid soils in high-rainfall uplands?

#### Maximum Probability Map

```{r map-prob-max, echo=FALSE, fig.cap="Maximum class probability - higher values indicate more confident predictions."}
knitr::include_graphics(file.path(FIGURES_DIR,"map_max_prob_value.png"))
```

**Why probability is uncertainty for nominal variables:**

For classification, the model outputs a probability for each class. These probabilities reflect the votes from individual trees. If the maximum probability is 0.9, the model is confident—most trees agree. If it is 0.2 (close to 1/9 = 0.11 for random guessing with 9 classes), the model is uncertain—trees disagree about which class applies.

Low probability areas are "transition zones" where the model cannot confidently assign a single class. These often correspond to gradual soil transitions in the landscape—precisely the areas where the categorical representation is most problematic.

#### Uncertainty Map

```{r map-uncertainty-nom, echo=FALSE, fig.cap="Classification uncertainty (1 - max probability)."}
knitr::include_graphics(file.path(FIGURES_DIR,"map_uncertainty_nominal.png"))
```

This map shows 1 minus the maximum probability. High values mean the model is uncertain about class assignment.

#### Confident Predictions Map

```{r map-confident, echo=FALSE, fig.cap="Only predictions where max probability >= 0.4 are shown."}
knitr::include_graphics(file.path(FIGURES_DIR,"map_class_confident.png"))
```

This map applies a confidence threshold: predictions below 40% probability are masked. Areas in white/NA are where the model is not confident enough to assign a class.

When delivering classification maps to users, consider providing probability maps alongside class maps, or masking uncertain areas to avoid false precision.

------------------------------------------------------------------------

## Ordinal Modelling (pH_Class)

### Nature of the Variable

**pH_Class** is an **ordinal variable**: categories with a natural order. The levels are: very_acid < acid < neutral < alkaline < very_alkaline.

Unlike nominal classes, order matters. Predicting "acid" when the truth is "neutral" (one class away) is a smaller error than predicting "very_acid" (two classes away). This ordering changes how we validate predictions and interpret errors.

**What this approach produces:**

-   A map of predicted pH classes
-   An instability map showing where predictions are near class boundaries
-   Validation metrics that account for class order (distance-weighted errors)

**What this approach cannot do:**

-   It assumes equal spacing between classes (the "distance" from very_acid to acid equals acid to neutral), which may not reflect true chemical differences
-   It does not preserve the original continuous pH information

### Ordinal Encoding

```{r ordinal-encoding}
ordinal_encoding <- readRDS(file.path(OUTPUTS_DIR, "models", "ordinal_encoding.rds"))

print("Ordinal level lookup:")
print(ordinal_encoding$lookup)

print("Reclassification thresholds:")
print(ordinal_encoding$reclassify_matrix)
```

**The approach:**

1.  Convert ordered classes to integers: very_acid = 1, acid = 2, neutral = 3, alkaline = 4, very_alkaline = 5
2.  Train a regression model to predict these integers as if they were continuous
3.  Round predictions back to classes using midpoint thresholds (1.5, 2.5, 3.5, 4.5)

**Limitation:** This assumes equal spacing between classes. In reality, the chemical difference between pH 4 and 5 may not equal the difference between pH 6 and 7. The ordinal approach is a compromise between continuous prediction and nominal classification.

### Feature Selection

```{r boruta-ordinal}
selected_features_ord <- readRDS(file.path(OUTPUTS_DIR, "models", "selected_features_pH_Class.rds"))

print(paste("Number of selected features:", length(selected_features_ord)))
print("Selected features:")
print(selected_features_ord)
```

### Model Training

```{r model-ordinal}
model_ord <- readRDS(file.path(OUTPUTS_DIR, "models", "ranger_model_pH_Class_ordinal.rds"))

print(model_ord)
print("Best hyperparameters:")
print(model_ord$bestTune)
```

We use Random Forest in regression mode, treating the integer-coded classes as a continuous target. This allows the model to produce predictions like 2.7, which is then rounded to class 3 (neutral). The regression approach respects the ordering of classes—predicting 2.7 is closer to truth=3 than predicting 1.2.

### Validation

```{r validation-ordinal}
metrics_ord <- read_csv(file.path(OUTPUTS_DIR, "validation", "metrics_pH_Class_ordinal.csv"), show_col_types = FALSE)
kable(metrics_ord, digits = 3, caption = "Ordinal classification metrics for pH_Class")
```

**What each metric tells you:**

| Metric | Meaning | Interpretation |
|--------|---------|----------------|
| **MAE_rank** | Mean Absolute Error in class units—average distance between observed and predicted class. | MAE_rank = 0.58 means predictions are typically about half a class off. |
| **Mean_distance** | Weighted average error accounting for all misclassifications. | Lower values = predictions closer to truth on the ordinal scale. |

**Why ordinal metrics differ from nominal:**

In ordinal validation, not all errors are equal. Predicting class 3 when truth is class 4 (distance = 1) is less severe than predicting class 1 (distance = 3). The metrics weight errors by their distance on the ordinal scale.

```{r ordinal-confusion, echo=FALSE, fig.cap="Ordinal confusion matrix. Errors near the diagonal are minor; errors far from diagonal are severe."}
knitr::include_graphics(file.path(FIGURES_DIR,"ordinal_confusion_pH_Class.png"))
```

**How to read the ordinal confusion matrix:**

-   **Diagonal**: Correct predictions
-   **Adjacent cells (±1)**: Minor errors, one class off
-   **Distant cells**: Severe errors, multiple classes off
-   Look for systematic patterns: does the model consistently underpredict (errors below diagonal) or overpredict (errors above diagonal)?

### Prediction Maps

#### Latent Values Map

```{r map-latent, echo=FALSE, fig.cap="Continuous 'latent' predictions before reclassification to ordinal classes."}
knitr::include_graphics(file.path(FIGURES_DIR,"map_latent_pH_Class.png"))
```

This map shows the raw regression output before rounding to classes. Values range from ~1 to ~5. This reveals continuous spatial gradients that are lost when we assign discrete classes.

#### Reclassified Ordinal Class Map

```{r map-class-ord, echo=FALSE, fig.cap="Ordinal classes derived from latent predictions using midpoint thresholds."}
knitr::include_graphics(file.path(FIGURES_DIR,"map_class_pH_Class.png"))
```

Classes 1–5 correspond to: very_acid, acid, neutral, alkaline, very_alkaline. Compare this to the continuous pH map—patterns should be similar but expressed as discrete classes.

#### Standard Deviation Map

```{r map-sd-ord, echo=FALSE, fig.cap="Prediction uncertainty (SD) from the ordinal regression model."}
knitr::include_graphics(file.path(FIGURES_DIR,"map_sd_pH_Class.png"))
```

As with continuous predictions, SD reflects disagreement among trees. High SD means the model is uncertain about the latent value.

#### Class Instability Map

```{r map-instability, echo=FALSE, fig.cap="Class instability - high values where predictions are near class boundaries."}
knitr::include_graphics(file.path(FIGURES_DIR,"map_instability_pH_Class.png"))
```

**Why ordinal uncertainty is about instability:**

For ordinal variables, a unique source of uncertainty is **class boundary instability**. A prediction of 2.48 is very close to the 2.5 threshold between classes 2 and 3. Small changes in input data or model parameters could flip the assigned class.

Instability combines prediction uncertainty (SD) with proximity to class boundaries. High instability means: "The model predicts a value near a threshold AND is uncertain about that value."

Areas with high instability should be flagged as uncertain classifications, even if they receive a class label in the map. This is a form of uncertainty specific to ordinal classification that does not exist for continuous predictions.

------------------------------------------------------------------------

## Area of Applicability (AOA)

### Why AOA is Necessary

Cross-validation tells us how well the model predicts **where we have training data**. But we apply the model to the entire study area, including locations with covariate combinations unlike any training point.

This relates to a fundamental limitation of empirical models: they cannot extrapolate reliably beyond the range of training data. The *scorpan* model $S = f(Q) + \varepsilon$ is fitted using available observations, and predictions outside the training domain are unreliable.

The **Area of Applicability (AOA)** identifies which parts of the study area have covariate combinations similar to the training data [@Meyer2021]. Outside the AOA, the model is extrapolating, and predictions may be unreliable regardless of cross-validation performance.

### Dissimilarity Index (DI)

```{r map-di, echo=FALSE, fig.cap="Dissimilarity Index - how different each location is from training data in covariate space."}
knitr::include_graphics(file.path(FIGURES_DIR,"map_DI_pH.png"))
```

DI measures how different each pixel's covariate values are from the nearest training point in multivariate covariate space. It is calculated using weighted Euclidean distance, where weights reflect variable importance from the model.

DI near 0 means the pixel is similar to training data. High DI means the covariate combination was not well represented in training—the model is extrapolating.

### AOA Binary Mask

```{r map-aoa-mask, echo=FALSE, fig.cap="Area of Applicability mask. Green = within AOA (trustworthy), Grey = outside AOA (unreliable)."}
knitr::include_graphics(file.path(FIGURES_DIR,"map_AOA_pH.png"))
```

Pixels are classified as inside (1) or outside (0) the AOA based on a DI threshold derived from cross-validation.

-   **Green areas**: Model has seen similar covariate combinations. Predictions are supported by training data.
-   **Grey areas**: Covariate combinations not represented in training. Predictions are extrapolations and should not be trusted.

```{r aoa-coverage, echo=FALSE}
# AOA coverage statistic (pre-computed by generate_chapter_figures.R)
aoa_cov_file <- file.path(FIGURES_DIR,"aoa_coverage.txt")
if (file.exists(aoa_cov_file)) cat(readLines(aoa_cov_file), "\n")
```

### Local Point Density (LPD)

```{r map-lpd, echo=FALSE, fig.cap="Local Point Density - number of training points supporting predictions at each location."}
knitr::include_graphics(file.path(FIGURES_DIR,"map_LPD_pH.png"))
```

LPD counts how many training points are "nearby" in covariate space. Even within the AOA, some areas have more training support than others.

LPD = 0 means no training points are nearby—a strong warning. Higher LPD means multiple training observations support the prediction, increasing confidence.

### Predictions Masked to AOA

```{r map-masked-aoa, echo=FALSE, fig.cap="pH predictions only shown within the Area of Applicability."}
knitr::include_graphics(file.path(FIGURES_DIR,"map_mean_pH_masked_AOA.png"))
```

This map shows predictions only where the model can be trusted. White/NA areas are outside the AOA.

**Practical recommendations:**

1.  Always report AOA coverage alongside prediction maps
2.  Mask or flag predictions outside AOA when delivering products to users
3.  Use areas outside AOA to prioritise future sampling—these are gaps in training data coverage

------------------------------------------------------------------------

## Summary

| Session | Variable Type | Approach | Key Metric | Uncertainty Type |
|---------|---------------|----------|------------|------------------|
| 1 | Continuous (pH) | QRF regression | R², RMSE | Standard deviation from tree disagreement |
| 2 | Nominal (Clay_pH_Class) | Classification RF | Accuracy, Kappa | Probability of assigned class |
| 3 | Ordinal (pH_Class) | Integer regression | MAE_rank | Instability near class boundaries |
| 4 | AOA | Dissimilarity analysis | Coverage % | Inside/outside applicability |

**Key points:**

1.  **Match the model to the variable type.** Continuous, nominal, and ordinal variables require different approaches and produce different outputs. The choice follows from the mathematical nature of the variable and how errors should be evaluated.

2.  **Models are empirical, not mechanistic.** We fit statistical relationships between soil and covariates ($S = f(Q) + \varepsilon$) rather than simulating physical processes. This is powerful but limited—models cannot explain causation or extrapolate reliably.

3.  **Cross-validation has limits.** It evaluates prediction at training locations, not in unsampled areas. Spatial clustering of training data can lead to overly optimistic error estimates.

4.  **Always report uncertainty.** The form of uncertainty depends on variable type: standard deviation for continuous variables, class probabilities for nominal variables, and instability for ordinal variables.

5.  **Use AOA to identify reliable predictions.** Do not trust predictions in areas with covariate combinations unlike the training data.

------------------------------------------------------------------------

## References {-}
