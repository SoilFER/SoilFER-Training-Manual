[["index.html", "Guided R tutorials covering the soil information and data lifecycle, from soil sampling design and mapping to data-driven decision-making. Licence", " Integrated soil information for decision-making Guided R tutorials covering the soil information and data lifecycle, from soil sampling design and mapping to data-driven decision-making. Angelini, M.E, Rodriguez Lado, L.,de Sousa Mendes, W., Ribeiro, E., Luotto, I. Licence The Technical Manual is made available under the Creative Commons Attribution-NonCommercial-ShareAlike 3.0 IGO licence CC BY-NC-SA 3.0 IGO. "],["abbreviations-and-acronyms.html", "Abbreviations and acronyms", " Abbreviations and acronyms BD Bulk density CEC Cation exchange capacity CRAN Comprehensive R archive network DSM Digital soil mapping GEE Google Earth Engine GSP Global Soil Partnership INSII International Network of Soil Information Institutions ITPS Intergovernmental Technical Panel on Soils ME Mean error MAE Mean absolute error MEC Modelling efficiency coefficient NDVI Normalized difference in vegetation index QA/QC Quality assurance/quality check RMSE Root mean squared error SOC Soil organic carbon SOM Soil organic matter "],["contributors-and-reviewers.html", "Contributors and reviewers", " Contributors and reviewers "],["presentation-and-basics.html", "Chapter 1 Presentation and basics 1.1 Background and Objectives 1.2 How to use this book 1.3 Training material 1.4 Setting up the software environment 1.5 Resources to get started 1.6 Basic concepts", " Chapter 1 Presentation and basics 1.1 Background and Objectives Healthy and productive soils are fundamental to resilient agrifood systems, sustainable land management, and climate adaptation. Yet, in many regions, soil information remains fragmented, outdated, or inaccessible. The Soil Mapping for Resilient Agrifood Systems (SoilFER) programme responds to this challenge by building comprehensive soil information systems that integrate sampling design, laboratory analysis, soil spectroscopy, digital modeling, and decision support tools. These systems aim to empower governments, researchers, and farmers with actionable knowledge for crop selection, fertilizer recommendations, and soil health management. This manual, Integrated Soil Information for Decision-Making: From Soil Sampling Design and Spectroscopy to Decisions, provides a step-by-step guide along the entire soil data value chain. It is designed as both a technical reference and a practical training resource, bridging the gap between raw soil data and its functional use in agricultural and environmental decision-making. The objectives of this manual are to: Present harmonized approaches for soil sampling design, ensuring representative and high-quality soil data collection. Provide guidance on soil laboratory analyses, with a focus soil spectroscopy, including calibration, preprocessing, modeling, and validation. Introduce best practices in soil data preparation and management, aligned with the Global Soil Information System (GloSIS) data model. Demonstrate methods for digital soil modeling and mapping, covering classical statistics, machine learning, and hybrid inference for both continuous and categorical soil properties. Explain how to generate functional soil information, including soil nutrient budgets, uncertainty maps, and probability surfaces, to support evidence-based decision-making. Facilitate data sharing and dissemination, promoting open standards, metadata documentation, and web-based services. By integrating these components, the manual equips users to move from raw samples to reliable soil information products that inform policy, guide sustainable soil management, and strengthen food and nutrition security. 1.2 How to use this book The present document is a technical manual on the phase I of the GSNmap initiative. It provides the scientific background on the importance of soil nutrients and guidance on the digital soil mapping techniques to map nutrients and soil properties that govern nutrient availability. It also comprises a compendium with all necessary scripts to generate national GSNmaps. These scripts are described step-by-step in 4 steps that cover soil data preparation (Step 1), covariate download (Step 2), the mapping process itself (Step 3), and the automatic generation of national reports (Step 4). The general workflow is shown in Figure 1.1. Figure 1.1: Overview of the steps to follow for the GSNmap generation. The chapters are structured as following: Chapter 1 Lorem ipsum Chapter 2 Lorem ipsum Chapter 3 Lorem ipsum Chapter 5 Lorem ipsum Chapter 6 Lorem ipsum Annex I serves as a repository for the complete scripts compendium Annex II provides alternative step-by-step instructions for the special case of soil data without point coordinates. 1.3 Training material The train material of this book is located in the Link . 1.4 Setting up the software environment 1.5 Resources to get started 1.4.1. Introduction to R 1.4.2. Data Visualization in R 1.4.3. Basic statistics 1.4.4. Working with Spatial Data 1.6 Basic concepts 1.6.1 Digital Soil Mapping 1.6.2 Main concepts: SCORPAN model "],["sampling-design-for-soil-surveys.html", "Chapter 2 Sampling design for soil surveys 2.1 Introduction 2.2 Sampling methodologies for soil spatial survey 2.3 SoilFER Sampling Design 2.4 Tutorial Using R 2.5 Principal Component Analysis (PCA) 2.6 Organize the Sampling Universe 2.7 Create Primary Sampling Units (PSU Grid) 2.8 Select PSUs with Sufficient Land Use Coverage 2.9 Rasterize PSUs for Covariate Space Coverage 2.10 Compute Optimal Sample Size 2.11 Covariate Space Coverage - Computing PSUs 2.12 Compute SSUs and TSUs 2.13 Export Sampling Units 2.14 Compute Alternative PSUs", " Chapter 2 Sampling design for soil surveys 2.1 Introduction Soil surveys play a critical role in environmental monitoring, evaluating soil degradation, and formulating sustainable land use strategies (bui2020?). They offer essential information on soil characteristics, including texture, structure, pH levels, organic matter content, nutrient availability, and other physicochemical attributes of interest. These data are essential for making informed choices about crop selection, fertilizer use, irrigation planning, and soil conservation and management strategies. Therefore, soil information and data are key components in any comprehensive soil survey that supplies necessary information for soil management decisions at local to global scales. In this sense, the Soil Mapping for Resilient Agrifood Systems (SoilFER) framework aims to provide open-access soil information system to support the formulation of digital soil maps, fertilizer recommendations, establish crop suitability, determine soil quality indicators, assess soil health, implement soil conservation practices, and create a national soil spectral library. State-of-the-art soil sampling protocols incorporate a systematic approach that considers factors such as soil variability, sampling depth, current use of the land, farming activities, and sampling density (brevik2016?; clay2018?; morton2000?; norris2020?). The SoilFER Sampling Design Technical Manual provides a general guidance on soil sampling design methodologies to ensure precision, reproducibility, and reliability in the proper collection of soil samples for digital soil mapping and monitoring. This manual presents a three-stage hierarchical hybrid method incorporating both probability and non-probability sampling for soil mapping and monitoring within the SoilFER framework. The first and second hierarchical levels select respectively the primary and secondary sampling units using a covariate space coverage sampling method on environmental covariates (model-based), while the hierarchical third-level sampling selects the tertiary sampling units using simple random sampling, without replacement (design-based). This implementation standardizes soil sampling methodologies across the various countries involved in the project. 2.2 Sampling methodologies for soil spatial survey Soil monitoring and mapping are essential to understand soil variability and manage soil resources efficiently. In this context, it is important to understand the basics of choosing a sampling methodology that best fits the project’s objectives, whether focused on mapping, monitoring, or a combination of both. The aim of this section is to provide an overview of sampling design concepts, making them understandable and accessible for both academic and non-academic audiences. 2.2.1 Selection of the sampling methodology Purposive (also known as traditional or expert) and probability sampling are the most used methods for selecting sampling units in soil surveys. Both methods differ basically in their objectives. In simple terms, expert sampling involves selecting specific points along a slope or landscape feature, either at individual locations or along transects, such as a catena or toposequence. This approach aims to capture the relationships between the variation in soil properties and the different geographical features that make up the landscape. This concept has been widely used as stated in the literature for soil mapping (borden2020?; gobin2000?; milne1936?). This selection method effectively captures local variations in soil properties and is useful for understanding how these properties change in relation to terrain and hydrological characteristics. However, this sampling method may not always reflect the overall variability of soil properties across the entire study area. Moreover, implementing this approach generally requires significant effort, time, and funding, particularly for large study areas. On the other hand, probability sampling assigns a known probability of selection to each sampling unit, ensuring that each sampling unit has a chance of being included in the sample. This approach focuses on obtaining representative samples to make unbiased inferences about the entire population under study (degruijter2006?). This method requires careful planning and implementation to ensure that the sampling design is both unbiased and representative of the population. In addition, its implementation can be more complex compared to the expert sampling method. However, its main advantage is that every sampling unit has a chance of being included in the sample, resulting in a more representative sample of the variability in the study area. Additionally, it allows statistical inferences about the entire population from the sample. This last aspect, if not the most important, allows soil surveyors to monitor and better report the current and future status of soil properties. In general, traditional/expert (non-probabilistic) sampling is less flexible than probabilistic sampling since the latter allows for both types of inference methods, as we will see in the next subsection. 2.2.2 Inference Methods Two primary statistical inference methods are used in soil survey: design- and model-based inference approaches (degruijter2006?). Design-based inference utilizes the sample design to make inferences about the entire population including probabilities of the sampling units. This approach is suitable for estimating global quantities, such as means and totals, or local quantities for large domains based on the design of the sampling scheme. Briefly, global quantities encompass the spatial cumulative distribution function of the response variable in the sampling universe, or quantities derived from it, such as quantiles, median, mean, and standard deviation. Local quantities, on the other hand, pertain to individual estimates within sub-areas of the sampling universe, also known as domains. Model-based inference uses statistical models to make inferences about unsampled locations based on the relationships observed in the sampled data. This approach is valuable for predicting soil properties at unsampled locations, especially in areas where sampling is limited. Further information about design- and model-based approaches can be found in (brus2021?). 2.2.3 Purpose of Sampling Bounding together selection and inference methods, one can better determine which one of those combinations works best for their project goals. The choice of sampling method in soil survey and monitoring depends on the survey’s purpose (brus2022?). For monitoring soil properties over time, design-based inference and probability sampling selection are suitable. These methods allow for the unbiased estimation of changes in soil properties at specific locations over time. For mapping soil properties across a landscape, model-based inference and non-probabilistic sampling are more appropriate. These methods allow for the spatial prediction of soil properties at unsampled locations based on the relationships observed in the sampled data. Monitoring differs from estimating soil parameters of domains at a given point in time. However, similar to monitoring, design-based inference and probability sampling are the most suitable methods for estimating soil properties within a certain domain. These approaches ensure that the estimates of soil parameters (e.g., mean, variance, mode, median) are unbiased and representative of the study area. 2.2.4 Sampling Design Types The sequence of decision-making for a soil survey or monitoring project should follow a logical order to ensure that the objectives of the project are met effectively. Begin by defining the purpose of the project. Determine whether the goal is to monitor changes in soil properties over time or to map them across landscape. Based on the purpose, one can choose an appropriate selection method. After that, one can select the inference method which best suits the project. For instance, if the goal is monitoring, consider probability sampling. For mapping, consider either non-probability or probability methods. Once the purpose, selection and inference methods are defined, one can select the appropriate sampling design. Before diving into the types of sampling design, it is important to take into consideration the stratification of the sampling universe. Stratification involves dividing the sampling universe into strata or sub-areas based on certain criteria, such as soil-climate types or land-use. This approach can increase the efficiency of the sampling design by obtaining separate estimates for each stratum, leading to more precise estimates for specific regions of interest (ROI). For example, if the ROI has distinct soil types, stratifying the sample by soil type can ensure that each soil type is adequately represented in the sample. Another example is using explanatory variables from remote sensing as a means of spatial coverage and stratification. We divided the sampling design types into monitoring and mapping (Table 2.1). For monitoring, one can certainly choose among simple random sampling (SRS), stratified SRS, or two-stage cluster random sampling. In SRS, each sampling unit in the ROI has equal probability of being selected. This approach is straightforward and easy to implement but may not account for spatial variability in soil properties. Stratified simple random sampling divides the ROI into strata based on certain criteria, such as soil-climate types or land-use. Then, SRS is conducted within each stratum. This method ensures that each stratum is adequately represented in the sample, leading to more precise estimates for specific ROI. Finally, two-stage cluster random sampling, the sampling units are grouped into clusters, and then clusters are randomly selected for sampling. This method is useful for large study areas where it may be impractical to sample every unit individually. One interesting method, which is not specifically categorized as simple, stratified, or two-stage cluster random sampling, is called balanced acceptance sampling (robertson2013?). This methodological approach can be applied within any of these sampling design types to ensure a balanced representation of different strata in the sample. For example, in a simple random sampling design, balanced acceptance sampling (BAS) would involve ensuring that each stratum is adequately represented in the random sample. In a stratified sampling design, BAS would aim to balance the number of sampling points across strata. In a two-stage cluster random sampling design, BAS would involve ensuring a balanced selection of clusters from each stratum. In essence, BAS is a principle or strategy that can be incorporated into various sampling design types to reduce bias in the estimation of soil properties. If the main goal of a sampling design is to map soil properties, non-probability methods such as conditioned Latin Hypercube sampling (minasny2006?), catena or topo-sequence sampling (milne1936?), grid sampling, or covariate space coverage sampling (brus2018?; degruijter2006?) are the most suitable choices. Conditioned Latin Hypercube sampling (CLHS) selects sampling points that ensure a representative distribution of soil properties across the study area. This approach is particularly useful for capturing spatial variability within the explanatory variables that describe the pattern of the response variable, leading to more accurate soil property maps. Catena or topo-sequence sampling was already discussed in section 2.2.1. Covariate space coverage sampling (CSCS) selects sampling points to provide complete spatial coverage of the ROI. (ma2020?) compared the efficiency of SRS, CLHS, and CSCS, being the latter the most efficient. In addition to the aforementioned methods, there are sampling approaches designed to produce samples that are evenly distributed across the study area, known as spatially balanced designs. These methods ensure that all areas are considered for inclusion in fieldwork, often requiring fewer samples compared to simple random sampling (SRS). Generalized Random Tessellation Stratified (GRTS) sampling (stevens1999?) and BAS are two popular approaches within the balanced sampling framework. The rationale behind these methods is that for estimating overall properties over a landscape, such as mean soil carbon, the estimate is most reliable when samples cover all areas of interest. While SRS, GRTS, and BAS are among the many sampling methods available, each with its own set of advantages and disadvantages, selecting the most suitable method can be daunting for field scientists. They often need to choose sites for fieldwork while adhering to constraints such as budget and time. For instance, the GRTS method can be used to provide a spatially balanced, probability sample with design-based, unbiased variance estimators with a minimum sample size. Table 2.1: Short description of pros and cons within each soil sampling design type Sampling.Design Purpose Pros Cons Simple Random (SR) Monitoring Simple, unbiased May miss rare features Stratified Simple Random (SSR) Monitoring Better precision per stratum Requires prior stratification Two-Stage Cluster Random (TSCR) Monitoring Cost-effective for large areas Higher variance Conditioned Latin Hypercube (CLH) Mapping Environmental representativeness Complex implementation Catena/Toposequence (CT) Mapping Captures terrain relationships Subjective, labor-intensive Covariate Space Coverage (CSC) Mapping Most efficient for mapping Computationally intensive Generalized Tessellation Stratified (GRT) Both Spatially balanced Complex variance estimation Balanced Acceptance (BA) Both Reduces bias Requires careful planning 2.2.4.1 Example: Soils4Africa Sampling Design The EU-funded project Soils4Africa aimed to provide open-access information about the condition and spatio-temporal dynamics of African soils, accompanied by a methodology for repeated soil monitoring and mapping across the African continent to support sustainable agriculture in Africa. The project’s purpose was to monitor and map African Soils. Given this purpose, probability sampling was identified as the most suitable selection method. This method allowed both inference methods, design- and model-based mapping. Once the purpose, selection and inference methods are defined, the final step was to choose the appropriate sampling design, which was a three-stage random sampling. The sampling design was based in a hierarchical sampling method with involves the selection of sampling sites at three scale levels: Primary Sampling Units (PSUs): PSUs constitute the broadest spatial units within the study area, typically defined based on geographical, administrative, ecological boundaries, or soil management types. The size of each PSU is 4 square kilometers (2 × 2 km grid cells). The total number of PSUs selected depends on the sample size and ideally should be sufficient to cover the range of environmental gradients present in the study area. Secondary Sampling Units (SSUs): Within each PSU, a predefined number of SSUs are selected for detailed sampling. SSUs offer a finer resolution level of sampling within each PSU. The size of an SSU is determined based on the need to capture localized environmental conditions and variability. In this case the size of the SSUs is 1 hectare (100 × 100 m grid cells). This approach involves a random selection of 7 SSUs per PSU to ensure a comprehensive coverage of the PSU’s environmental diversity. Four of these SSU are target SSUs and 3 are alternative SSUs to be used as replacement areas due to potential sampling challenges encountered in the field. Tertiary Sampling Units (TSUs): TSUs represent the most detailed level of sampling, focusing on specific points within SSUs. The number of TSUs within each SSU is 3, one which acts as the primary target point and 2 as replacement points for the primary point. The sampling design in Soils4Africa proceeds as follows: (i) A first selection of PSUs is made using a stratified random sampling on farming systems strata. The farming system classification data comes from the FAO farming systems map for Africa which provides information for 46 different farming classes or stratum. (ii) Within each of the selected PSUs, a second sampling level is done upon a regular division of the PSU into cells of 1 ha, the SSU. For each PSU, 4 random SSUs are selected for sampling and 3 additional cells are stored as alternative cells. These cells are used as replacements in case some of the target SSUs cannot be sampled for some reason. The selection of SSUs is done by simple random sampling within the PSU. (iii) Within each SSU, 3 simple random sampling points are located. The first point serves as the target point to be sampled and the remaining 2 are stored as replacement points to be used as in the previous step. This Hierarchical Sampling design helps reduce bias in the data, allowing for accurate assessments of soil constraints, quantification of risk factors, and reliable evaluations of changes in soil health/quality. 2.3 SoilFER Sampling Design The SoilFER soil spatial survey aims to estimate means and totals for a set of target soil properties and quality indicators within three domains: croplands (85%), grasslands (10%), and forests (5%) in a country. The percentages for each domain are relative to the total sampling points, considering the sampling density required for high-resolution digital soil maps. Once the purpose was defined, the most suitable selection method identified was probability sampling, which allows for both design- and model-based inference methods. Given the purpose, selection and inference methods, the final step was to choose the appropriate sampling design. Like the Soils4Africa project, the SoilFER project employs three sampling units, which are discrete entities in space, and thus a three-stage hierarchical hybrid method incorporating both probability and non-probability sampling was selected for mapping and monitoring within the SoilFER sampling design. In the literature, covariate space coverage (CSC) sampling has proven to be more efficient than other approaches for mapping a continuous soil property or class (brus2018?; degruijter2006?; ma2020?; schmidinger2024?). Therefore, in the SoilFER sampling design, the first stage or first hierarchical level selects the primary sampling units (PSUs) (i.e., 2km × 2km) using a CSC sampling method, or k-means sampling if you will (i.e., model-based). K-means is an unsupervised clustering method used when no response variable is available, partitioning data into k groups by maximizing similarity within groups and differences between them. Like the selection of PSUs, the second hierarchical level selects the secondary sampling units (SSUs) (i.e., 100m × 100m) using CSC sampling and a second set of environmental covariates (i.e., model-based). Lastly, the third hierarchical levels select the tertiary sampling units using simple random sampling without replacement (i.e., design-based). This multi-stage sampling framework enhances both spatial representativeness and statistical reliability. 2.3.1 Soil Properties Understanding and assessing soil properties are critical for evaluating soil health, guiding fertilizer recommendations, and informing land management decisions. In the SoilFER project, both chemical and physical soil attributes are measured to provide a comprehensive evaluation of soil quality and health (Table 2.2). Some key chemical properties include soil organic carbon, pH, cation exchange capacity, and nutrient levels such as nitrogen, phosphorus, and potassium. On the physical side, attributes like soil texture, soil depth, rockiness of surface soil, drainage, surface cracks, soil color, soil erosion and gypsum content are analyzed to understand soil structure, compaction, and water infiltration rates. Collectively, these attributes offer valuable insights into the soil’s ability to support plant growth, resist erosion, and store carbon, making them indispensable for both mapping soil health and tailoring site-specific fertilizer recommendations. Table 2.2: Some of the chemical and physical soil properties to be quantified in the SoilFER project Category Property Unit Chemical Soil Organic Carbon % Chemical pH pH units Chemical Cation Exchange Capacity cmol(+)/kg Chemical Total Nitrogen % Chemical Available Phosphorus mg/kg Chemical Exchangeable Potassium cmol(+)/kg Chemical Micronutrients mg/kg Chemical Electrical Conductivity dS/m Physical Soil Texture % sand/silt/clay Physical Soil Depth cm Physical Rockiness % coverage Physical Drainage class Physical Surface Cracks present/absent Physical Soil Color Munsell Physical Erosion severity class Physical Gypsum Content % Note: Both wet and dry chemistry (i.e., infrared spectroscopy) will be conducted in the laboratory for soil analysis as part of the SoilFER project. These methods are not listed here, as a standard operating procedure exists for each one of them. 2.3.2 Environmental Covariates Environmental covariates, also referred to as explanatory or environmental variables or simply covariates, are essential for understanding the spatial variability of soil properties and landscape processes. The SoilFER sampling design incorporates 85 environmental covariates (Table 2.3) to optimise the identification and allocation of PSUs (2km × 2km) across diverse landscapes at a national scale. As SoilFER targets soil mapping at national scale, the first set of environmental covariates have pixel resolution varying from 250 m to 1 km. To align with the PSU size (2km × 2km), these covariates were upscaled to ensure that each PSU is represented by a single aggregated value per covariate. This resolution provides a balance between sufficient spatial detail and computational efficiency, ensuring accurate large-scale mapping while manageable computational processing requirements, ensuring efficient data handling and analysis. Moreover, this comprehensive set of covariates ensures that the selected PSUs capture the full range of environmental conditions and soil diversity within a country. Additionally, a second set of 24 environmental covariates (Table 2.4) is used to refine site selection at farm level. This high-resolution dataset (e.g., ≤ 90 m pixel resolution) was upscaled to match the SSU size (100m × 100m), ensuring that each SSU is characterized by a single representative value for each covariate. This significantly enhances the precision of SSU selection by capturing finer spatial variability. If additional high-resolution environmental covariates are available for a given country, it is highly recommended to incorporate them into the analysis to further improve sampling precision. These two sets of environmental covariates were carefully chosen based on their relevance to soil formation processes, spatial variability, and their role as proxies for key soil properties. Further information on how to retrieve these environmental covariates can be found in the SoilFER GitHub repository. Table 2.3: Some of the chemical and physical soil properties to be quantified in the SoilFER project Category Property Unit Chemical Soil Organic Carbon % Chemical pH pH units Chemical Cation Exchange Capacity cmol(+)/kg Chemical Total Nitrogen % Chemical Available Phosphorus mg/kg Chemical Exchangeable Potassium cmol(+)/kg Chemical Micronutrients mg/kg Chemical Electrical Conductivity dS/m Physical Soil Texture % sand/silt/clay Physical Soil Depth cm Physical Rockiness % coverage Physical Drainage class Physical Surface Cracks present/absent Physical Soil Color Munsell Physical Erosion severity class Physical Gypsum Content % Note: NSM, Normalized Soil Moisture; SWIR, Shortwave Infrared. Full list contains 85 variables. Table 2.4: List of environmental covariates used to delineate the secondary sampling units Source Variable Resolution SRTM Terrain Elevation 90-100m SRTM Terrain Slope 90-100m SRTM Terrain Aspect 90-100m SRTM Terrain TPI 90-100m SRTM Terrain Hillshade 90-100m Sentinel-2 Spectral B2 (Blue) 10-20m Sentinel-2 Spectral B3 (Green) 10-20m Sentinel-2 Spectral B4 (Red) 10-20m Sentinel-2 Spectral B5 (Red Edge 1) 10-20m Sentinel-2 Spectral B6 (Red Edge 2) 10-20m Sentinel-2 Spectral B7 (Red Edge 3) 10-20m Sentinel-2 Spectral B8 (NIR) 10-20m Sentinel-2 Spectral B8A (NIR Narrow) 10-20m Sentinel-2 Spectral B11 (SWIR 1) 10-20m Sentinel-2 Spectral B12 (SWIR 2) 10-20m Sentinel-2 Indices NDVI 10-20m Sentinel-2 Indices EVI 10-20m Sentinel-2 Indices NDBSI 10-20m Sentinel-2 Indices Brightness Index 10-20m Sentinel-2 Indices Redness Index 10-20m Sentinel-2 Indices VNSIR 10-20m Geomorphology Geomorphon Class 1-9 90m Geomorphology … 90m Geomorphology … 90m 2.3.3 Understanding the Methodology The SoilFER sampling design methodology follows a hierarchical sampling approach with three levels of increasing spatial resolution: Primary Sampling Units (PSUs), Secondary Sampling Units (SSUs), and Tertiary Sampling Units (TSUs) (Figure ??). This design is similar to the Soils4Africa project but differs in two key aspects. First, we use covariate space coverage (CSC) to select PSUs (brus2022?), whereas Soils4Africa employed stratified random sampling at this stage. At the first hierarchical level, CSC is used to select PSUs (2km × 2km areas), ensuring a well-distributed spread across the covariate space to maximise environmental variation and represent soil diversity across the region at national scale. Second key aspect, we select SSUs using CSC rather than simple random sampling, as done in Soils4Africa. At this second hierarchical level, CSC is applied to select SSUs (100m × 100m) based on a refined set of environmental covariates at farm scale. It is of note that each project adopted a methodology tailored to its specific objectives/purposes, ensuring that the sampling design aligns with its intended goals. Each PSU contains multiple SSUs, each measuring 100m × 100m, and each SSU consists of three TSUs, each measuring 20m × 20m. This structured yet flexible sampling design captures soil variability across different spatial scales (Figure ??). 2.3.3.1 PSU Selection Process The process of selecting the PSUs is summarized as follows: Step 1: Create 2km × 2km Grid (Figure ??) - A 2km × 2km grid covering the entire region of interest (ROI) is created - This grid serves as the foundational structure for stratification and spatial analysis Step 2: Overlay Land Use and Protected Areas (Figure ??) - Land-use and land cover (LULC) mask layer identifying cropland, forest, or grassland is overlaid - Protected area boundaries are incorporated - Grid cells where cropland, forest, or grassland cover more than 10% (or specified percentage) are retained - This ensures that only PSUs containing target land uses are selected Step 3: Apply Covariate Space Coverage (Figure ??) - PSUs are selected through CSC sampling using 85 environmental covariates - Covariates are resampled to 250m resolution - Principal Component Analysis (PCA) transforms covariates into PC layers - PC layers accounting for up to 99% of variance are retained - PC layers are upscaled to 2km × 2km resolution Step 4: Determine Optimal Sample Size - Optimal number of PSUs determined using divergence metrics - Kullback-Leibler Divergence (KLD) quantifies how well sampled data represent full environmental covariate space - Unlike Jensen-Shannon methods, KLD does not overestimate optimal sample size (saurette2023?) Step 5: Perform K-means Clustering - K-means clustering creates homogeneous clusters (strata) - Number of clusters equals total number of PSUs to be sampled - Legacy point data can be incorporated (clusters fixed at legacy locations) - Mean Squared Shortest Distance (MSSD) in covariate space is calculated - Process repeated 10 times, retaining trial with minimum MSSD Step 6: Select Target and Replacement PSUs (Figure ??) - PSUs allocated by minimizing MSSD within environmental covariate space - Output: N PSUs where N equals total number of samples - Replacement PSUs calculated by randomly selecting grid cells within same cluster class 2.3.3.2 SSU and TSU Selection Process Stage 2: Secondary Sampling Units (SSUs) The second stage involves selecting SSUs using Covariate Space Coverage Sampling (CSCS) based on 24 high-resolution environmental covariates. Each SSU represents a 100m × 100m (1 hectare) area divided into 25 regular sampling plots of 20m × 20m (Figure ??). Selection scheme: - 8 SSUs selected from each PSU - 4 target SSUs (1-4): where samples will be collected - 4 replacement SSUs (5-8): pre-identified substitutes - Replacement SSUs directly linked to targets (one-to-one basis): - SSU 5 replaces SSU 1 - SSU 6 replaces SSU 2 - SSU 7 replaces SSU 3 - SSU 8 replaces SSU 4 If all SSUs within a target PSU are exhausted, missing SSUs must be selected from the designated PSU replacement. The designated PSU replacements were selected with the same spatial variability as their PSU targets. Stage 3: Tertiary Sampling Units (TSUs) The third and final stage involves selecting TSUs using simple random sampling without replacement. Each SSU contains 25 possible TSUs of 20m × 20m, corresponding to the pixel size of the crop raster layer. Selection scheme: - 3 TSUs per SSU: - 1 target TSU (primary sampling location) - 2 replacement TSUs (alternatives) - If a TSU is unsuitable, any replacement can be used - If all TSUs within an SSU are unsuitable, entire SSU is rejected Soil sampling protocol: - Sampling conducted strictly at designated TSU locations - Maximum shift: 10 metres from defined coordinates 2.3.3.3 Site Identification System The final site ID is an alphanumeric identifier structured as follows (Figure ??): Format: AAA####-#-#X Where: - AAA = Three-letter country ISO code (e.g., KEN for Kenya) - #### = Primary Sampling Unit number (zero-padded, e.g., 0001) - First # = Secondary Sampling Unit number (1-8) - Second # = Tertiary Sampling Unit number (1-3) - X = Land use type code: - C = Cropland - G = Grassland - F = Forest Example: KEN0001-1-1C - Country: Kenya - PSU: 1 - SSU: 1 (target) - TSU: 1 (target) - Land use: Cropland 2.4 Tutorial Using R This tutorial is designed for users with a basic understanding of R programming and provides a comprehensive guide to implementing a soil sampling design for soil monitoring and mapping. All scripts and data are available at the SoilFER GitHub repository.  Note: Ensure you have the tinytex, knitr, rmarkdown, and rstudioapi R packages installed to successfully run and render R Markdown documents before running this script. Additionally, install tinytex by running tinytex::install_tinytex() to enable LaTeX support for PDF generation. 2.4.1 Setting Up the Environment First, set the working directory to the location of your R script. This ensures that all paths in the script are relative to the script’s location. setwd(dirname(rstudioapi::getActiveDocumentContext()$path)) setwd(&quot;../&quot;) # Move wd down to the main folder getwd() 2.4.1.1 Install Required Libraries The script uses several R libraries, each serving a specific purpose (Table 2.5). Table 2.5: Summary of required R libraries and their purposes for SoilFER sampling design Package Purpose sp Spatial data classes terra Raster data manipulation raster Legacy raster support sf Simple features (vector data) sgsR Spatial sampling entropy Entropy calculations tripack Triangulation tibble Data frames manipulate Interactive plotting dplyr Data manipulation synoptReg PCA for rasters doSNOW Parallel processing Rfast Fast R functions fields Distance calculations ggplot2 Visualization rassta Terrain analysis Some libraries might need to be installed from GitHub. Uncomment and run the lines below if needed: # Install synoptReg package from GitHub # install.packages(&quot;remotes&quot;) # Install remotes if not installed # remotes::install_github(&quot;lemuscanovas/synoptReg&quot;) packages &lt;- c(&quot;sp&quot;, &quot;terra&quot;, &quot;raster&quot;, &quot;sf&quot;, &quot;sgsR&quot;, &quot;entropy&quot;, &quot;tripack&quot;, &quot;tibble&quot;, &quot;manipulate&quot;, &quot;dplyr&quot;, &quot;synoptReg&quot;, &quot;doSNOW&quot;, &quot;Rfast&quot;, &quot;fields&quot;, &quot;ggplot2&quot;, &quot;rassta&quot;) invisible(lapply(packages, library, character.only = TRUE)) rm(packages) Alternative method: In RStudio, use the Packages tab in the bottom-right pane → click Install → type package name → ensure “Install dependencies” is checked → click Install. 2.4.2 Define Variables and Parameters Several variables and parameters must be defined to establish a consistent framework for the script. 2.4.3 Country ISO Code ISO.code &lt;- &quot;ZMB&quot; # Zambia example  Note: Use the Alpha-3 ISO code for your country of interest. 2.4.3.1 Land Use Type The SoilFER project focuses on three primary land-use types: croplands (80%), grasslands (15%), and forests (5%). landuse &lt;- &quot;crops&quot; # Options: &quot;crops&quot;, &quot;grassland&quot;, &quot;forest&quot; 2.4.3.2 File Paths # Path to data folders raster.path &lt;- &quot;data/rasters/&quot; shp.path &lt;- &quot;data/shapes/&quot; other.path &lt;- &quot;data/other/&quot; landuse_dir &lt;- paste0(&quot;data/results/&quot;, landuse, &quot;/&quot;) # Check if landuse directory exists; if not, create it if (!file.exists(landuse_dir)) { dir.create(landuse_dir) } results.path &lt;- landuse_dir 2.4.3.3 Coordinate Reference System epsg &lt;- &quot;EPSG:3857&quot; # WGS 84 / Pseudo-Mercator # Find your CRS at: https://epsg.io/ 2.4.3.4 Sample Size Calculation nsamples &lt;- 300 # Total sampling sites share &lt;- 0.80 # 80% for this land use (croplands) nsites &lt;- nsamples * share # Final number of sites # However, we will calculate the optimal sample size statistically later 2.4.3.5 Sampling Unit Definitions # Define the number of PSUs to sample n.psu &lt;- round(nsamples/8) # Will be optimized later # Define PSU and SSU sizes psu_size &lt;- 2000 # 2km × 2 km ssu_size &lt;- 100 # 100m × 100m = 1 ha # Define number of target and alternative SSUs at each PSU num_primary_ssus &lt;- 4 num_alternative_ssus &lt;- 4 # Define number of TSUs at each SSU number_TSUs &lt;- 3 2.4.3.6 Algorithm Parameters # Number of iterations for clustering algorithm iterations &lt;- 10 # 10 is sufficient # Minimum crop percentage in selected PSUs percent_crop &lt;- 10 # At least 10% crop coverage 2.4.4 Custom Functions 2.4.4.1 Covariate Space Coverage Function (CSIS) The CSIS function is the backbone of clustering sampling units in covariate space. It ensures optimal distribution while considering any fixed legacy data. How it works: Input Parameters: fixed: Preselected (legacy) sampling points nsup: Number of additional (new) sampling points to select nstarts: Number of random starting points for clustering mygrd: Grid of covariate data for ROI Workflow: Extract fixed points and exclude from grid Randomly initialize new sampling points Combine fixed and new centers Compute distances and assign points to nearest cluster Update cluster centers iteratively until convergence Calculate Mean Squared Shortest Distance (MSSSD) Save best clustering result # Clustering CSC function with fixed legacy data CSIS &lt;- function(fixed, nsup, nstarts, mygrd) { n_fix &lt;- nrow(fixed) p &lt;- ncol(mygrd) units &lt;- fixed$units mygrd_minfx &lt;- mygrd[-units, ] MSSSD_cur &lt;- NA for (s in 1:nstarts) { units &lt;- sample(nrow(mygrd_minfx), nsup) centers_sup &lt;- mygrd_minfx[units, ] centers &lt;- rbind(fixed[, names(mygrd)], centers_sup) repeat { D &lt;- rdist(x1 = centers, x2 = mygrd) cluster &lt;- apply(X = D, MARGIN = 2, FUN = which.min) %&gt;% as.factor(.) centers_cur &lt;- centers for (i in 1:p) { centers[, i] &lt;- tapply(mygrd[, i], INDEX = cluster, FUN = mean) } # Restore fixed centers centers[1:n_fix, ] &lt;- centers_cur[1:n_fix, ] # Check convergence sumd &lt;- diag(rdist(x1 = centers, x2 = centers_cur)) %&gt;% sum(.) if (sumd &lt; 1E-12) { D &lt;- rdist(x1 = centers, x2 = mygrd) Dmin &lt;- apply(X = D, MARGIN = 2, FUN = min) MSSSD &lt;- mean(Dmin^2) if (s == 1 | MSSSD &lt; MSSSD_cur) { centers_best &lt;- centers clusters_best &lt;- cluster MSSSD_cur &lt;- MSSSD } break } } print(paste0(s, &quot; out of &quot;, nstarts)) } list(centers = centers_best, cluster = clusters_best) }  Note: This function creates spatially balanced sampling designs by considering existing sampling points and covariate distributions. 2.4.4.2 TSU Generation Function This function generates TSUs within SSUs, ensuring random distribution while respecting land-use constraints. generate_tsu_points_within_ssu &lt;- function(ssu, number_TSUs, index, ssu_type, crops) { # Convert SSU to SpatVector for masking ssu_vect &lt;- ssu_grid_sf[index, ] # Clip the raster to the SSU boundaries clipped_lu &lt;- crop(crops, ssu_vect) # Generate random points within the clipped raster sampled_points &lt;- sample_srs(clipped_lu, nSamp = number_TSUs) # Ensure required number of TSUs while (nrow(sampled_points) &lt; number_TSUs) { sampled_points &lt;- spatSample(clipped_lu, size = number_TSUs) } # Add metadata sampled_points$PSU_ID &lt;- selected_psu$ID sampled_points$SSU_ID &lt;- index sampled_points$TSU_ID &lt;- seq_len(nrow(sampled_points)) sampled_points$SSU_Type &lt;- ssu_type sampled_points$TSU_Name &lt;- paste0(ISO.code, sprintf(&quot;%04d&quot;, sampled_points$PSU_ID), &quot;-&quot;, sampled_points$SSU_ID, &quot;-&quot;, seq_len(nrow(sampled_points))) return(sampled_points) } 2.4.5 Load Country Boundaries and Legacy Data Load and transform country boundaries and legacy soil data to the desired CRS: # Define file locations country_boundaries &lt;- file.path(paste0(shp.path, &quot;roi_epsg_3857.shp&quot;)) legacy &lt;- file.path(paste0(shp.path, &quot;zmb_legacy_v2_clipped_epsg_3857.shp&quot;)) # Load and transform country boundaries country_boundaries &lt;- sf::st_read(country_boundaries, quiet = TRUE) if (sf::st_crs(country_boundaries)$epsg != epsg) { country_boundaries &lt;- sf::st_transform(country_boundaries, crs = epsg) } # Load legacy data (if it exists) if (file.exists(legacy)) { legacy &lt;- sf::st_read(legacy, quiet = TRUE) # Transform coordinates if (sf::st_crs(legacy)$epsg != epsg) { legacy &lt;- sf::st_transform(legacy, crs = epsg) } } else { # If legacy data does not exist, delete the object rm(legacy) }  Note: Always verify that all geospatial datasets use the same CRS before performing spatial operations. Misaligned CRSs can lead to incorrect analyses. 2.4.5.1 Visualize Boundaries and Legacy Data library(ggplot2) library(ggspatial) basemap &lt;- annotation_map_tile(&quot;osm&quot;) ggplot(data = country_boundaries) + basemap + geom_sf(fill = &quot;transparent&quot;, color = &quot;black&quot;) + geom_sf(data = legacy, aes(geometry = geometry), color = &quot;black&quot;, size = 0.3) + labs(title = &quot;Country Boundaries and Legacy Data&quot;, caption = &quot;Data source: OpenStreetMap&quot;) + theme_minimal() + theme(plot.title = element_text(hjust = 0.5)) 2.4.6 Load Environmental Covariates for PSUs 2.4.6.1 Optional Auxiliary Data Additional data layers can improve PSU selection: # Non-protected areas (shapefile or raster) npa &lt;- file.path(paste0(shp.path, &quot;zmb_national_parks_zari_clipped_epsg_3857.shp&quot;)) # High slopes (binary raster: 1 = &lt;50%, NA = &gt;=50%) slope &lt;- file.path(paste0(raster.path, &quot;zmb_clipped_slope_mask_epsg_3857.tif&quot;)) # Geology geo &lt;- file.path(paste0(shp.path, &quot;zmb_geology_clipped_epsg_3857.shp&quot;)) geo.classes &lt;- &quot;GEO&quot; # Field name for geologic classes # Geomorphology geomorph &lt;- file.path(paste0(raster.path, &quot;zmb_clipped_Geomorphon.tif&quot;)) geomorph.classes &lt;- &quot;CODR&quot; # Field name for geomorphology classes 2.4.6.2 Load and Process Auxiliary Data # Non-protected areas if (file.exists(npa)) { npa &lt;- sf::st_read(npa, quiet = FALSE) npa &lt;- sf::st_union(npa) npa &lt;- sf::st_difference(country_boundaries, npa) if (sf::st_crs(npa)$epsg != epsg) { npa &lt;- sf::st_transform(npa, crs = epsg) } } else { rm(npa) } # Slope mask if (file.exists(slope)) { slope &lt;- rast(slope) if (crs(slope) != epsg) { slope &lt;- project(slope, epsg, method = &quot;near&quot;) } slope &lt;- slope / slope # Normalize to 0/1 } else { rm(slope) } # Geology if (file.exists(geo)) { geo &lt;- sf::st_read(geo, quiet = TRUE) if (sf::st_crs(geo)$epsg != epsg) { geo &lt;- sf::st_transform(geo, crs = epsg) } geo$GEO &lt;- as.numeric(as.factor(geo[[geo.classes]])) } else { rm(geo) } # Geomorphology if (file.exists(geomorph)) { file_extension &lt;- tools::file_ext(geomorph) if (file_extension == &quot;tif&quot;) { geomorph &lt;- rast(geomorph) names(geomorph) &lt;- &#39;GEOMORPH&#39; if (crs(geomorph) != epsg) { geomorph &lt;- project(geomorph, epsg, method = &quot;near&quot;) } } else if (file_extension == &quot;shp&quot;) { geomorph &lt;- sf::st_read(geomorph, quiet = TRUE) if (sf::st_crs(geomorph)$epsg != epsg) { geomorph &lt;- sf::st_transform(geomorph, crs = epsg) } geomorph$GEOMORPH &lt;- as.numeric(as.factor(geomorph[[geomorph.classes]])) } else { warning(&quot;File format not recognized. Expected .tif or .shp.&quot;) rm(geomorph) } } else { rm(geomorph) } 2.4.7 Load Main Environmental Covariates # Load covariate stack cov.dat &lt;- list.files(raster.path, pattern = &quot;covs_zam_clipped.tif$&quot;, recursive = TRUE, full.names = TRUE) cov.dat &lt;- terra::rast(cov.dat) cat(sprintf(&quot;Loaded %d environmental covariates\\n&quot;, nlyr(cov.dat))) # Display covariate names in a table cov.dat.names &lt;- matrix(names(cov.dat), ncol = 3, byrow = TRUE) knitr::kable(cov.dat.names, format = &quot;markdown&quot;, caption = &quot;Environmental covariates&quot;)  Note: Initial number of environmental covariates: 68 2.4.7.1 Load and Process Soil Climate Data (Newhall) # Load soil climate data newhall &lt;- list.files(raster.path, pattern = &quot;newhall_zam_clipped.tif$&quot;, recursive = TRUE, full.names = TRUE) newhall &lt;- terra::rast(newhall) # Display variable names newhall.names &lt;- matrix(names(newhall), ncol = 3, byrow = TRUE) knitr::kable(newhall.names, format = &quot;markdown&quot;, caption = &quot;Soil climate variables&quot;) # Remove unnecessary subdivisions newhall$regimeSubdivision1 &lt;- c() newhall$regimeSubdivision2 &lt;- c() cat(sprintf(&quot;Soil climate variables: %d (after removing subdivisions)\\n&quot;, nlyr(newhall))) 2.4.7.2 Convert Categorical Variables to Dummy Variables # Convert temperatureRegime and moistureRegime to dummy variables temperatureRegime &lt;- dummies(ca.rast = newhall$temperatureRegime, preval = 1, absval = 0) moistureRegime &lt;- dummies(ca.rast = newhall$moistureRegime, preval = 1, absval = 0) # Remove original categorical layers newhall$temperatureRegime &lt;- c() newhall$moistureRegime &lt;- c() 2.4.7.3 Merge All Covariate Layers # Merge covariates and climate data cov.dat &lt;- c(cov.dat, newhall, temperatureRegime, moistureRegime) # Project if necessary if (crs(cov.dat) != epsg) { cov.dat &lt;- terra::project(cov.dat, epsg, method = &quot;near&quot;) } # Add geology if exists if (exists(&quot;geo&quot;)) { geo &lt;- rasterize(as(geo, &quot;SpatVector&quot;), cov.dat, field = &quot;GEO&quot;) geo &lt;- dummies(ca.rast = geo$GEO, preval = 1, absval = 0) cov.dat &lt;- c(cov.dat, geo) } # Add geomorphology if exists if (exists(&quot;geomorph&quot;)) { if (!inherits(geomorph, &quot;SpatRaster&quot;)) { geomorph &lt;- rasterize(as(geomorph, &quot;SpatVector&quot;), cov.dat, field = &quot;GEOMORPH&quot;) } geomorph &lt;- dummies(ca.rast = geomorph$GEOMORPH, preval = 1, absval = 0) # Ensure matching extent and resolution if (!identical(ext(cov.dat), ext(geomorph))) { geomorph &lt;- extend(geomorph, cov.dat) } if (!all(res(cov.dat) == res(geomorph))) { geomorph &lt;- resample(geomorph, cov.dat, method = &quot;near&quot;) } cov.dat &lt;- c(cov.dat, geomorph) } # Cleanup rm(newhall, geomorph, geo) gc() cat(sprintf(&quot;\\nFinal covariate stack: %d layers\\n&quot;, nlyr(cov.dat)))  Note: Stacking is the process of combining multiple spatial data layers (raster datasets) into a single multi-layer object. This is useful when multiple variables need to be analyzed together. 2.4.7.4 Crop and Save Covariates # Crop covariates to administrative boundary cov.dat &lt;- crop(cov.dat, country_boundaries, mask = TRUE, overwrite = TRUE) writeRaster(cov.dat, paste0(raster.path, &quot;cov_dat_stack.tif&quot;), overwrite = TRUE) # Reload if needed # cov.dat &lt;- rast(paste0(raster.path, &quot;cov_dat_stack.tif&quot;)) 2.5 Principal Component Analysis (PCA) PCA reduces dimensionality while retaining the most important information. Components explaining 99% of variance are kept. # Perform PCA pca &lt;- synoptReg::raster_pca(cov.dat) # Faster than terra::princomp # Get SpatRaster layers cov.dat &lt;- pca$PCA # Display summary summary(cov.dat) # Subset to main PCs (variance explained &gt;= 0.99) n_comps &lt;- first(which(pca$summaryPCA[3,] &gt; 0.99)) cov.dat &lt;- pca$PCA[[1:n_comps]] cat(sprintf(&quot;\\nRetained %d principal components (99%% variance)\\n&quot;, n_comps)) # Save PCA rasters writeRaster(cov.dat, paste0(results.path, &quot;PCA_projected.tif&quot;), overwrite = TRUE) # Cleanup rm(pca) # Reload if needed # cov.dat &lt;- rast(paste0(results.path, &quot;PCA_projected.tif&quot;)) 2.6 Organize the Sampling Universe 2.6.1 Load and Process Land Use Data # Define land-use raster path landuse &lt;- file.path(paste0(raster.path, &quot;cropland_clipped_zmb_v1_epsg_3857.tif&quot;)) # Load binary raster (1 = target land use, NA = other) crops &lt;- rast(landuse) crops &lt;- crops / crops # Normalize names(crops) &lt;- &quot;lu&quot; # Display raster info crops 2.6.2 Exclude Protected Areas if (exists(&quot;npa&quot;)) { # If npa is a shapefile crops &lt;- mask(crops, npa) # If npa is a raster (uncomment if applicable) # npa &lt;- resample(npa, crops, method = &quot;near&quot;) # crops &lt;- crops * npa } rm(npa)  Note: Use the correct operation for your npa dataset type (shapefile or raster). 2.6.3 Restrict to Accessible Slopes if (exists(&quot;slope&quot;)) { # Resample to match crops resolution slope &lt;- resample(slope, crops, method = &quot;near&quot;) crops &lt;- crops * slope } rm(slope) 2.6.4 Aggregate to 100m Resolution Resampling to 100m resolution significantly optimizes processing speed and reduces storage requirements. # Aggregate 20m pixels to 100m (5×5 grid using modal value) lu &lt;- aggregate(crops, 5, fun = modal, cores = 4, na.rm = TRUE) lu &lt;- lu / lu # Display new resolution lu 2.6.5 Save Processed Land Use writeRaster(lu, paste0(raster.path, &quot;cropland_zmb_100m_v1_epsg_3857.tif&quot;), overwrite = TRUE) # Reload if needed lu &lt;- rast(paste0(raster.path, &quot;cropland_zmb_100m_v1_epsg_3857.tif&quot;)) 2.6.6 Filter Legacy Data if (exists(&quot;legacy&quot;)) { # Keep only legacy points within land-use areas legacy$INSIDE &lt;- terra::extract(crops, legacy) %&gt;% dplyr::select(lu) legacy &lt;- legacy[!is.na(legacy$INSIDE), ] %&gt;% dplyr::select(-&quot;INSIDE&quot;) cat(sprintf(&quot;Legacy points within land-use areas: %d\\n&quot;, nrow(legacy))) } 2.7 Create Primary Sampling Units (PSU Grid) 2.7.1 Generate 2km × 2km Grid # Create grid psu_grid &lt;- st_make_grid(country_boundaries, cellsize = c(psu_size, psu_size), square = TRUE) psu_grid &lt;- st_sf(geometry = psu_grid) psu_grid$ID &lt;- 1:nrow(psu_grid) cat(sprintf(&quot;Generated %d PSU grid cells\\n&quot;, nrow(psu_grid))) 2.7.2 Trim to Country Boundaries # This operation is computationally intensive psu_grid &lt;- psu_grid[country_boundaries[1], ] cat(sprintf(&quot;PSUs within country boundaries: %d\\n&quot;, nrow(psu_grid))) 2.7.3 Save PSU Grid write_sf(psu_grid, paste0(results.path, &quot;../grid2k.shp&quot;), overwrite = TRUE) # Reload if needed psu_grid &lt;- sf::st_read(file.path(paste0(results.path, &quot;../grid2k.shp&quot;))) 2.8 Select PSUs with Sufficient Land Use Coverage 2.8.1 Calculate Land Use Percentage # Extract land-use values for each PSU extracted_values &lt;- terra::extract(lu, psu_grid) # Calculate percentage (400 pixels = 2km × 2km PSU at 100m resolution) crop_perc &lt;- extracted_values %&gt;% group_by(ID) %&gt;% summarize(crop_perc = sum(lu, na.rm = TRUE) * 100 / 400) rm(extracted_values) # Add to PSU grid psu_grid$crop_perc &lt;- crop_perc$crop_perc cat(sprintf(&quot;Crop percentage calculated for all PSUs\\n&quot;)) 2.8.2 Filter PSUs # Keep only PSUs with sufficient crop coverage psu_grid_filtered &lt;- psu_grid[psu_grid$crop_perc &gt; percent_crop, &quot;ID&quot;] cat(sprintf(&quot;PSUs with &gt;%d%% crop coverage: %d\\n&quot;, percent_crop, nrow(psu_grid_filtered))) # Save filtered grid write_sf(psu_grid_filtered, file.path(paste0(results.path, &quot;/psu_grid_counts.shp&quot;)), overwrite = TRUE) 2.9 Rasterize PSUs for Covariate Space Coverage # Create raster template at PSU resolution template &lt;- rast(vect(psu_grid_filtered), res = psu_size) template &lt;- rasterize(vect(psu_grid_filtered), template, field = &quot;ID&quot;) # Crop covariates to filtered PSUs cov.dat &lt;- crop(cov.dat, psu_grid_filtered, mask = TRUE, overwrite = TRUE) # Resample covariates to match PSU template PSU.r &lt;- resample(cov.dat, template) cat(sprintf(&quot;Covariates resampled to PSU resolution\\n&quot;))  Why this matters: Cropping and resampling ensure that spatial resolution and extent of covariates match the PSU grid, facilitating accurate clustering. 2.10 Compute Optimal Sample Size After rasterizing PSUs for Covariate Space Coverage, calculate the optimal sample size based on divergence metrics (saurette2023?). # Load optimization script source(&quot;scripts/opt_sample.R&quot;) # Prepare covariate data psu.r.df &lt;- data.frame(PSU.r) # Define parameters initial.n &lt;- 50 # Minimum sample size final.n &lt;- 3000 # Maximum sample size by.n &lt;- 25 # Increment step iters &lt;- 4 # Iterations per trial 2.10.1 Run Optimization # Calculate optimal sample size using normalized KL-divergence opt_N_fcs &lt;- opt_sample( alg = &quot;fcs&quot;, # Feature coverage sampling s_min = initial.n, s_max = final.n, s_step = by.n, s_reps = iters, covs = psu.r.df, cpus = NULL, # Use all available cores conf = 0.95 ) # Display results print(opt_N_fcs$optimal_sites) 2.10.2 Extract Optimal Sample Size # Use Kullback-Leibler Divergence (most reliable) optimal_N_KLD &lt;- opt_N_fcs$optimal_sites[1, 2] cat(sprintf(&quot;\\n✓ Optimal sample size: %d PSUs\\n&quot;, optimal_N_KLD)) # Set final PSU count n.psu &lt;- optimal_N_KLD # Expected total sampling sites (4 per PSU) cat(sprintf(&quot;Expected total target sites: %d\\n&quot;, n.psu * 4))  Note: The optimal sample size is based on Kullback-Leibler Divergence, which does not overestimate like Jensen-Shannon methods. 2.11 Covariate Space Coverage - Computing PSUs Covariate Space Coverage (CSC) sampling ensures sampling units are distributed effectively across the multidimensional space of environmental covariates. 2.11.1 Prepare Function Parameters # Convert raster to dataframe with coordinates PSU.df &lt;- as.data.frame(PSU.r, xy = TRUE) # Get covariate names covs &lt;- names(cov.dat) # Scale covariates (mean = 0, SD = 1) mygrd &lt;- data.frame(scale(PSU.df[, covs])) cat(sprintf(&quot;Prepared %d PSUs with %d covariates for clustering\\n&quot;, nrow(mygrd), ncol(mygrd))) 2.11.2 Perform K-means Clustering # With legacy data if (exists(&quot;legacy&quot;)) { # Filter legacy to PSU grid extent legacy &lt;- st_filter(legacy, psu_grid_filtered) legacy_df &lt;- st_coordinates(legacy) # Find nearest PSU for each legacy point units &lt;- numeric(nrow(legacy_df)) for (i in 1:nrow(legacy_df)) { distances &lt;- sqrt((PSU.df$x - legacy_df[i, &quot;X&quot;])^2 + (PSU.df$y - legacy_df[i, &quot;Y&quot;])^2) units[i] &lt;- which.min(distances) } # Create fixed centers from legacy points fixed &lt;- unique(data.frame(units, scale(PSU.df[, covs])[units, ])) cat(sprintf(&quot;Incorporating %d legacy points as fixed centers\\n&quot;, nrow(fixed))) # Run constrained clustering res &lt;- CSIS(fixed = fixed, nsup = n.psu, nstarts = iterations, mygrd = mygrd) } else { # Standard k-means without legacy data res &lt;- kmeans(mygrd, centers = n.psu, iter.max = 10000, nstart = 100) cat(sprintf(&quot;Running standard k-means with %d centers\\n&quot;, n.psu)) }  Note: Progress is printed as “X out of Y” for each clustering iteration. 2.11.3 Assign Clusters and Calculate Distances # Assign clusters to PSUs PSU.df$cluster &lt;- res$cluster # Compute distances to nearest cluster center D &lt;- rdist(x1 = res$centers, x2 = scale(PSU.df[, covs])) units &lt;- apply(D, MARGIN = 1, FUN = which.min) # Calculate Mean Squared Shortest Distance (MSSSD) dmin &lt;- apply(D, MARGIN = 2, min) MSSSD &lt;- mean(dmin^2) cat(sprintf(&quot;MSSSD: %.6f\\n&quot;, MSSSD)) 2.11.4 Extract Selected PSUs # Get selected PSU coordinates and covariates myCSCsample &lt;- PSU.df[units, c(&quot;x&quot;, &quot;y&quot;, covs)] # Label as legacy or new if (exists(&quot;legacy&quot;)) { myCSCsample$type &lt;- c(rep(&quot;legacy&quot;, nrow(fixed)), rep(&quot;new&quot;, length(units) - nrow(fixed))) } else { myCSCsample$type &lt;- &quot;new&quot; } # Convert to spatial object myCSCsample &lt;- myCSCsample %&gt;% st_as_sf(coords = c(&quot;x&quot;, &quot;y&quot;), crs = epsg) # Separate legacy and new points if (exists(&quot;legacy&quot;)) { legacy &lt;- myCSCsample[myCSCsample$type == &quot;legacy&quot;, ] } new &lt;- myCSCsample[myCSCsample$type == &quot;new&quot;, ] # Get PSU IDs PSUs &lt;- sf::st_intersection(psu_grid_filtered, new) %&gt;% dplyr::select(ID) # Extract target PSUs target.PSUs &lt;- psu_grid_filtered[psu_grid_filtered$ID %in% PSUs$ID, ] %&gt;% dplyr::select(ID) cat(sprintf(&quot;\\n✓ Selected %d target PSUs\\n&quot;, nrow(target.PSUs))) # Cleanup rm(new, dmin, MSSSD) 2.11.5 Visualize in Covariate Space 2.12 Compute SSUs and TSUs Now we generate Secondary Sampling Units (100m × 100m) within each PSU and Tertiary Sampling Units (20m × 20m points) within each SSU. 2.12.1 Load High-Resolution Covariates # Load SSU-level covariates (100m resolution) cov.dat.ssu &lt;- terra::rast(paste0(raster.path, &quot;hres_data/covs_stack_ssu_1ha_ZMB.tif&quot;)) cat(sprintf(&quot;Loaded %d high-resolution covariates\\n&quot;, nlyr(cov.dat.ssu))) # Display covariate names names(cov.dat.ssu) # Remove system-specific bands if needed cov.dat.ssu &lt;- subset(cov.dat.ssu, names(cov.dat.ssu)[!names(cov.dat.ssu) %in% c(&quot;sysisen_B2&quot;, &quot;sysisen_B3&quot;, &quot;sysisen_B4&quot;, &quot;sysisen_B5&quot;, &quot;sysisen_B6&quot;, &quot;sysisen_B7&quot;, &quot;sysisen_B8&quot;, &quot;sysisen_B8A&quot;, &quot;sysisen_B11&quot;, &quot;sysisen_B12&quot;)]) # Replace NA with 0 (for categorical variables) cov.dat.ssu[is.na(cov.dat.ssu)] &lt;- 0 cat(sprintf(&quot;Final SSU covariates: %d layers\\n&quot;, nlyr(cov.dat.ssu))) 2.12.2 Process Each PSU # Initialize storage lists selected_ssus &lt;- list() all_psus_tsus &lt;- list() # Main loop: Process each PSU for (psu_id in 1:nrow(target.PSUs)) { selected_psu &lt;- target.PSUs[psu_id, ] # Generate 100m × 100m SSU grid ssu_grid &lt;- st_make_grid(selected_psu, cellsize = c(ssu_size, ssu_size), square = TRUE) ssu_grid_sf &lt;- st_sf(geometry = ssu_grid) # Convert to SpatVector for extraction ssu_grid_vect &lt;- vect(ssu_grid_sf) # Extract land-use values to filter SSUs extracted_values &lt;- extract(crops, ssu_grid_vect, fun = table) # Calculate land-use percentage (25 pixels per SSU at 20m resolution) ssu_grid_sf$lu &lt;- (extracted_values[, 2] * 100) / 25 ssu_grid_sf &lt;- ssu_grid_sf[ssu_grid_sf$lu &gt; percent_crop, ] # Progress update if (psu_id %% 10 == 0 || psu_id == nrow(target.PSUs)) { cat(sprintf(&quot;\\rProgress: %.2f%% (%d out of %d)&quot;, (psu_id / nrow(target.PSUs)) * 100, psu_id, nrow(target.PSUs))) flush.console() } # Check minimum SSU requirement total_ssus &lt;- nrow(ssu_grid_sf) if (total_ssus &lt; (num_primary_ssus + num_alternative_ssus)) { warning(paste(&quot;PSU&quot;, psu_id, &quot;does not have enough SSUs. Skipping.&quot;)) next } # Extract covariate values for each SSU ssu_covariates &lt;- terra::extract(cov.dat.ssu, vect(ssu_grid_sf), df = TRUE) # Merge with SSU spatial data ssu_data &lt;- cbind(ssu_grid_sf, ssu_covariates[, -1]) ssu_data_values &lt;- st_drop_geometry(ssu_data) # Identify columns to exclude from scaling (categorical) exclude &lt;- grep(&quot;^geomorph_|^lu$&quot;, names(ssu_data_values), value = TRUE) # Separate scalable and non-scalable data to_scale &lt;- ssu_data_values[, !names(ssu_data_values) %in% exclude] to_keep &lt;- ssu_data_values[, names(ssu_data_values) %in% exclude, drop = FALSE] # Remove NA-only columns to_scale &lt;- to_scale[, colSums(!is.na(to_scale)) &gt; 0, drop = FALSE] # Identify zero-variance columns zero_variance_cols &lt;- sapply(to_scale, function(x) sd(x, na.rm = TRUE) == 0) zero_variance_cols[is.na(zero_variance_cols)] &lt;- TRUE # Scale only non-zero-variance columns scaled_part &lt;- to_scale if (any(!zero_variance_cols)) { scaled_part[, !zero_variance_cols] &lt;- scale(to_scale[, !zero_variance_cols]) } # Recombine mygrd_ssu &lt;- cbind(to_keep, scaled_part) # Perform k-means clustering for this PSU optimal_k &lt;- num_primary_ssus # 4 clusters kmeans_result &lt;- kmeans(mygrd_ssu[, -1], centers = optimal_k, iter.max = 10000, nstart = 10) ssu_data$cluster &lt;- as.factor(kmeans_result$cluster) # CSC Sampling: Select closest SSUs to cluster centers D &lt;- rdist(x1 = kmeans_result$centers, x2 = mygrd_ssu[, -1]) # Target SSUs (closest to centers) target_units &lt;- apply(D, MARGIN = 1, FUN = function(x) order(x)[1]) target_ssus &lt;- ssu_data[target_units, ] # Replacement SSUs (second closest to centers) replacement_units &lt;- apply(D, MARGIN = 1, FUN = function(x) order(x)[2]) replacement_ssus &lt;- ssu_data[replacement_units, ] # Add metadata target_ssus$SSU_Type &lt;- &quot;Target&quot; replacement_ssus$SSU_Type &lt;- &quot;Replacement&quot; target_ssus$SSU_ID &lt;- 1:nrow(target_ssus) replacement_ssus$SSU_ID &lt;- (nrow(target_ssus) + 1):(2 * nrow(target_ssus)) # Link replacements to targets by cluster replacement_ssus$replacement_for &lt;- sapply(replacement_ssus$cluster, function(cl) { matched &lt;- target_ssus$SSU_ID[target_ssus$cluster == cl] if (length(matched) &gt; 0) return(matched[1]) else return(NA) }) target_ssus$replacement_for &lt;- NA # Add PSU ID target_ssus$PSU_ID &lt;- selected_psu$ID replacement_ssus$PSU_ID &lt;- selected_psu$ID # Store SSUs selected_ssus[[psu_id]] &lt;- rbind(target_ssus, replacement_ssus) # Generate TSUs for target SSUs primary_tsus &lt;- lapply(rownames(target_ssus), function(index) { generate_tsu_points_within_ssu( ssu_grid_sf[rownames(ssu_grid_sf) == index, ], number_TSUs, index, &quot;Target&quot;, crops ) }) # Generate TSUs for replacement SSUs alternative_tsus &lt;- lapply(rownames(replacement_ssus), function(index) { generate_tsu_points_within_ssu( ssu_grid_sf[rownames(ssu_grid_sf) == index, ], number_TSUs, index, &quot;Replacement&quot;, crops ) }) # Combine all TSUs for this PSU all_psus_tsus[[psu_id]] &lt;- do.call(rbind, c(primary_tsus, alternative_tsus)) } cat(sprintf(&quot;\\n✓ Processing complete\\n&quot;)) cat(sprintf(&quot; Successful PSUs: %d\\n&quot;, length(selected_ssus))) 2.12.3 Combine SSUs and TSUs # Combine all SSUs all_ssus &lt;- do.call(rbind, selected_ssus) all_ssus &lt;- all_ssus %&gt;% mutate_at(vars(PSU_ID, SSU_ID), as.numeric) # Combine all TSUs all_tsus &lt;- do.call(rbind, all_psus_tsus) all_tsus &lt;- all_tsus %&gt;% mutate_at(vars(PSU_ID, SSU_ID), as.numeric) # Join TSU and SSU metadata all_tsus &lt;- st_join(all_tsus, all_ssus[c(&quot;PSU_ID&quot;, &quot;SSU_ID&quot;, &quot;SSU_Type&quot;, &quot;replacement_for&quot;)]) # Reorganize columns all_tsus &lt;- all_tsus %&gt;% select(PSU_ID = PSU_ID.x, SSU_ID = SSU_ID.y, SSU_Type = SSU_Type.y, Replacement_for = replacement_for, TSU_ID, geometry) # Label TSU types all_tsus$TSU_Type &lt;- &quot;Target&quot; all_tsus[all_tsus$TSU_ID &gt; 1, &quot;TSU_Type&quot;] &lt;- &quot;Alternative&quot; all_tsus$PSU_Type &lt;- &quot;Target&quot; # Final column selection all_tsus &lt;- all_tsus %&gt;% dplyr::select(&quot;PSU_ID&quot;, &quot;SSU_ID&quot;, &quot;SSU_Type&quot;, &quot;Replacement_for&quot;, &quot;TSU_ID&quot;, &quot;TSU_Type&quot;, &quot;geometry&quot;) cat(sprintf(&quot;\\n✓ Total SSUs: %d\\n&quot;, nrow(all_ssus))) cat(sprintf(&quot;✓ Total TSUs: %d\\n&quot;, nrow(all_tsus))) cat(sprintf(&quot;✓ Target sampling sites: %d\\n&quot;, sum(all_tsus$SSU_Type == &quot;Target&quot; &amp; all_tsus$TSU_Type == &quot;Target&quot;))) 2.13 Export Sampling Units 2.13.1 Create Cluster Raster # Convert cluster information to raster dfr &lt;- PSU.df[, c(&quot;x&quot;, &quot;y&quot;, &quot;cluster&quot;)] dfr$cluster &lt;- as.numeric(dfr$cluster) dfr &lt;- rasterFromXYZ(dfr) crs(dfr) &lt;- epsg # Associate clusters with PSUs PSU_cluster.id &lt;- unlist(extract(dfr, target.PSUs)) valid.PSU_clusters &lt;- target.PSUs %&gt;% mutate(cluster = extract(dfr, target.PSUs, fun = mean, na.rm = TRUE)) all.PSU_clusters &lt;- psu_grid_filtered %&gt;% mutate(cluster = extract(dfr, psu_grid_filtered, fun = mean, na.rm = TRUE)) all.PSU_clusters &lt;- na.omit(all.PSU_clusters) 2.13.2 Join Cluster Info to TSUs # Rename cluster column valid.PSU_clusters &lt;- valid.PSU_clusters %&gt;% rename(Replace_ID = cluster) # Join to TSUs all_tsus &lt;- st_join(all_tsus, valid.PSU_clusters) # Assign sampling order all_tsus &lt;- all_tsus %&gt;% group_by(PSU_ID) %&gt;% mutate(order = match(SSU_ID, unique(SSU_ID))) %&gt;% ungroup() 2.13.3 Create Site IDs # Generate unique site IDs all_tsus$site_id &lt;- paste0(ISO.code, sprintf(&quot;%04d&quot;, all_tsus$PSU_ID), &quot;-&quot;, all_tsus$SSU_ID, &quot;-&quot;, all_tsus$TSU_ID, &quot;C&quot;) # C = Cropland cat(sprintf(&quot;Created %d unique site IDs\\n&quot;, nrow(all_tsus))) 2.13.4 Export Shapefiles # Export target PSUs write_sf(valid.PSU_clusters, paste0(results.path, &quot;/PSUs_target.shp&quot;), overwrite = TRUE) # Export target TSUs write_sf(all_tsus, paste0(results.path, &quot;/TSUs_target.shp&quot;), overwrite = TRUE) # Export all PSU clusters write_sf(all.PSU_clusters, paste0(results.path, &quot;/PSU_pattern_cl.shp&quot;), overwrite = TRUE) # Export cluster raster writeRaster(dfr, paste0(results.path, &quot;/clusters.tif&quot;), overwrite = TRUE) cat(sprintf(&quot;\\n✓ All files exported successfully\\n&quot;)) cat(sprintf(&quot; Location: %s\\n&quot;, results.path)) 2.14 Compute Alternative PSUs Generate replacement PSUs from the same environmental clusters as targets. # Filter out already selected PSUs remaining_psus &lt;- all.PSU_clusters %&gt;% filter(!(ID %in% target.PSUs$ID)) # Get unique cluster IDs from targets unique_clusters &lt;- unique(valid.PSU_clusters$Replace_ID) # Initialize list for replacements replacement_psus &lt;- data.frame() # For each target cluster, select a replacement for (clust_id in unique_clusters) { # Find candidates in same cluster candidates &lt;- remaining_psus %&gt;% filter(cluster == clust_id) if (nrow(candidates) &gt; 0) { # Random selection from candidates replacement &lt;- candidates[sample(nrow(candidates), 1), ] replacement$replaces_PSU &lt;- valid.PSU_clusters$ID[ valid.PSU_clusters$Replace_ID == clust_id ][1] replacement_psus &lt;- rbind(replacement_psus, replacement) } else { warning(sprintf(&quot;No replacement found for cluster %d&quot;, clust_id)) } } cat(sprintf(&quot;\\n✓ Generated %d replacement PSUs\\n&quot;, nrow(replacement_psus))) # Export replacement PSUs write_sf(replacement_psus, paste0(results.path, &quot;/PSUs_replacements.shp&quot;), overwrite = TRUE) Note: Follow the same SSU and TSU generation process for replacement PSUs to create complete backup sampling locations. "],["merging-results.html", "Chapter 3 Merging Results from Multiple Land Uses 3.1 Assign Country-Wide Unique IDs 3.2 Export Unified Datasets 3.3 Summary Statistics 3.4 Create Distribution Maps", " Chapter 3 Merging Results from Multiple Land Uses After running the sampling design separately for cropland, grassland, and forest, combine all results into a unified dataset with country-wide unique IDs. library(data.table) # Define land uses and codes landuses &lt;- c(&quot;cropland&quot;, &quot;grassland&quot;, &quot;forest&quot;) lu_codes &lt;- c(&quot;C&quot;, &quot;G&quot;, &quot;F&quot;) # Initialize storage lists psus_target_list &lt;- list() tsus_target_list &lt;- list() psus_repl_list &lt;- list() tsus_repl_list &lt;- list() # Load all shapefiles for (i in seq_along(landuses)) { lu &lt;- landuses[i] code &lt;- lu_codes[i] # Target PSUs file_path &lt;- sprintf(&quot;results/%s/PSUs_target.shp&quot;, lu) if (file.exists(file_path)) { temp &lt;- st_read(file_path, quiet = TRUE) temp$lulc &lt;- code psus_target_list[[lu]] &lt;- temp cat(sprintf(&quot;✓ Loaded %s target PSUs (%d)\\n&quot;, lu, nrow(temp))) } # Target TSUs file_path &lt;- sprintf(&quot;results/%s/TSUs_target.shp&quot;, lu) if (file.exists(file_path)) { temp &lt;- st_read(file_path, quiet = TRUE) temp$lulc &lt;- code tsus_target_list[[lu]] &lt;- temp cat(sprintf(&quot;✓ Loaded %s target TSUs (%d)\\n&quot;, lu, nrow(temp))) } # Replacement PSUs and TSUs # (similar loading process) } # Merge using data.table (handles different column names) psus_target &lt;- st_as_sf(rbindlist(psus_target_list, fill = TRUE)) tsus_target &lt;- st_as_sf(rbindlist(tsus_target_list, fill = TRUE)) cat(sprintf(&quot;\\n=== MERGED DATASETS ===\\n&quot;)) cat(sprintf(&quot;Target PSUs: %d\\n&quot;, nrow(psus_target))) cat(sprintf(&quot;Target TSUs: %d\\n&quot;, nrow(tsus_target))) 3.1 Assign Country-Wide Unique IDs # Assign sequential IDs to target PSUs psus_target$PSU_ID_country &lt;- 1:nrow(psus_target) # Continue numbering for replacements start_id &lt;- nrow(psus_target) + 1 psus_repl$PSU_ID_country &lt;- start_id:(start_id + nrow(psus_repl) - 1) # Transfer country-wide IDs to TSUs tsus_target$PSU_temp_ID &lt;- paste0(tsus_target$PSU_ID, &quot;-&quot;, tsus_target$lulc) psus_target$PSU_temp_ID &lt;- paste0(psus_target$ID, &quot;-&quot;, psus_target$lulc) tsus_target$PSU_ID_country &lt;- psus_target$PSU_ID_country[ match(tsus_target$PSU_temp_ID, psus_target$PSU_temp_ID) ] # Create final site IDs with country-wide PSU numbers tsus_target &lt;- tsus_target %&gt;% mutate(site_id = sprintf(&quot;%s%04d-%d-%d%s&quot;, ISO.code, PSU_ID_country, SSU_ID, TSU_ID, lulc)) # Clean up tsus_target &lt;- tsus_target %&gt;% select(PSU_ID = PSU_ID_country, SSU_ID, SSU_Type, TSU_ID, TSU_Type, site_id, lulc, geometry) psus_target &lt;- psus_target %&gt;% select(PSU_ID = PSU_ID_country, lulc, geometry) cat(sprintf(&quot;\\n✓ Assigned country-wide unique IDs\\n&quot;)) 3.2 Export Unified Datasets # Export unified datasets write_sf(psus_target, &quot;results/all/all_psus_target.shp&quot;, overwrite = TRUE) write_sf(tsus_target, &quot;results/all/all_tsus_target.shp&quot;, overwrite = TRUE) write_sf(psus_repl, &quot;results/all/all_psus_replacements.shp&quot;, overwrite = TRUE) write_sf(tsus_repl, &quot;results/all/all_tsus_replacements.shp&quot;, overwrite = TRUE) cat(sprintf(&quot;\\n✓ All unified datasets exported to results/all/\\n&quot;)) 3.3 Summary Statistics # Count target sites by land use summary_lu &lt;- tsus_target %&gt;% filter(SSU_Type == &quot;Target&quot; &amp; TSU_Type == &quot;Target&quot;) %&gt;% st_drop_geometry() %&gt;% group_by(lulc) %&gt;% summarise( n_PSUs = n_distinct(PSU_ID), n_sites = n(), sites_per_PSU = n / n_distinct(PSU_ID) ) print(summary_lu) # Total counts cat(sprintf(&quot;\\n=== FINAL SAMPLING DESIGN ===\\n&quot;)) cat(sprintf(&quot;Total PSUs: %d\\n&quot;, n_distinct(tsus_target$PSU_ID))) cat(sprintf(&quot;Total target sampling sites: %d\\n&quot;, sum(tsus_target$SSU_Type == &quot;Target&quot; &amp; tsus_target$TSU_Type == &quot;Target&quot;))) cat(sprintf(&quot; - Cropland: %d\\n&quot;, sum(tsus_target$lulc == &quot;C&quot; &amp; tsus_target$SSU_Type == &quot;Target&quot; &amp; tsus_target$TSU_Type == &quot;Target&quot;))) cat(sprintf(&quot; - Grassland: %d\\n&quot;, sum(tsus_target$lulc == &quot;G&quot; &amp; tsus_target$SSU_Type == &quot;Target&quot; &amp; tsus_target$TSU_Type == &quot;Target&quot;))) cat(sprintf(&quot; - Forest: %d\\n&quot;, sum(tsus_target$lulc == &quot;F&quot; &amp; tsus_target$SSU_Type == &quot;Target&quot; &amp; tsus_target$TSU_Type == &quot;Target&quot;))) 3.4 Create Distribution Maps library(ggplot2) # Filter to target sites only target_sites &lt;- tsus_target %&gt;% filter(SSU_Type == &quot;Target&quot; &amp; TSU_Type == &quot;Target&quot;) # Load country boundaries country &lt;- st_read(&quot;shapes/roi_country_epsg_4326.shp&quot;) # Create distribution map ggplot() + geom_sf(data = country, fill = &quot;gray95&quot;, color = &quot;gray50&quot;) + geom_sf(data = target_sites, aes(color = lulc), size = 0.5, alpha = 0.6) + scale_color_manual( name = &quot;Land Use&quot;, values = c(&quot;C&quot; = &quot;#F096FF&quot;, &quot;G&quot; = &quot;#FFFF4C&quot;, &quot;F&quot; = &quot;#006400&quot;), labels = c(&quot;C&quot; = &quot;Cropland&quot;, &quot;G&quot; = &quot;Grassland&quot;, &quot;F&quot; = &quot;Forest&quot;) ) + labs(title = &quot;SoilFER Sampling Design - Final Site Distribution&quot;, subtitle = sprintf(&quot;%d sampling locations across %d PSUs&quot;, nrow(target_sites), n_distinct(target_sites$PSU_ID))) + theme_minimal() + theme(legend.position = &quot;right&quot;) ggsave(&quot;results/all/final_site_distribution.png&quot;, width = 12, height = 10, dpi = 300) cat(sprintf(&quot;\\n✓ Distribution map saved\\n&quot;)) "],["field-protocol.html", "Chapter 4 Field Sampling Protocol 4.1 GPS Device Preparation 4.2 Sampling Procedure 4.3 Data Recording 4.4 Quality Control", " Chapter 4 Field Sampling Protocol 4.1 GPS Device Preparation 4.1.1 Export to GPX Format # Load target TSUs tsus_field &lt;- st_read(&quot;results/all/all_tsus_target.shp&quot;) # Filter to primary targets only (1 per SSU) tsus_field &lt;- tsus_field %&gt;% filter(SSU_Type == &quot;Target&quot; &amp; TSU_Type == &quot;Target&quot;) # Transform to WGS84 (required for GPS) tsus_field_wgs84 &lt;- st_transform(tsus_field, crs = 4326) # Export to GPX st_write(tsus_field_wgs84, &quot;results/all/field_sampling_points.gpx&quot;, driver = &quot;GPX&quot;, delete_dsn = TRUE) cat(sprintf(&quot;✓ Exported %d waypoints to GPX\\n&quot;, nrow(tsus_field_wgs84))) 4.1.2 Load onto GPS Device Connect GPS device to computer Copy field_sampling_points.gpx to GPS memory Load waypoints in GPS software Waypoint names will be site_id values (e.g., KEN0001-1-1C) 4.2 Sampling Procedure For each PSU (in order of PSU_ID): Navigate to TSU 1 (SSU_ID=1, TSU_ID=1) If accessible → Sample at exact GPS location If inaccessible (physical barrier) → Use TSU 2 (Alternative 1) or TSU 3 (Alternative 2) Navigate to TSU 2 (SSU_ID=2, TSU_ID=1) Repeat accessibility check and sampling Navigate to TSU 3 (SSU_ID=3, TSU_ID=1) Navigate to TSU 4 (SSU_ID=4, TSU_ID=1) If entire PSU is inaccessible: - Use replacement_PSU_ID to find backup PSU - Navigate to replacement PSU location - Sample at replacement TSUs (SSU_ID = 5, 6, 7, 8) 4.3 Data Recording Field form should capture (Table 4.1): Table 4.1: Required field data recording Field Example Notes site_id KEN0001-1-1C CRITICAL - Unique identifier date 2025-01-15 Date of sampling team Team A Field team name coordinates_actual -1.2345, 36.7890 Actual GPS coordinates accessibility Target / Alt1 / Alt2 Which TSU was sampled land_use_observed Maize field Visual confirmation soil_depth 45 cm Depth to restrictive layer photos KEN0001-1-1C_001.jpg Geotagged photos notes Rocky, 15% slope Relevant observations 4.4 Quality Control During field work: - ✅ Verify land use matches expected type - ✅ Document if alternative TSU used (and reason) - ✅ Take geotagged photos at each site - ✅ Record actual GPS coordinates - ✅ Note any anomalies or challenges Post-field QC: - Check for duplicate site_id entries - Verify all mandatory fields completed - Cross-reference photos with field records - Flag sites with large GPS deviation (&gt;30m from waypoint) "],["references.html", "References", " References "],["appendices.html", "Appendices Appendix A: Software and Data Sources Appendix B: Troubleshooting Common Issues Appendix C: Acronyms and Abbreviations", " Appendices Appendix A: Software and Data Sources 4.4.1 Software Requirements R (version ≥ 4.0): https://www.r-project.org/ RStudio: https://posit.co/products/open-source/rstudio/ Google Earth Engine: https://earthengine.google.com/ QGIS (optional): https://qgis.org/ 4.4.2 Data Sources Global Datasets: - CHELSA Climate: https://chelsa-climate.org/ - TerraClimate: https://www.climatologylab.org/terraclimate.html - MODIS: https://lpdaac.usgs.gov/products/mod13q1v006/ - Sentinel-2: https://scihub.copernicus.eu/ - SRTM DEM: https://earthexplorer.usgs.gov/ - ESA WorldCover: https://esa-worldcover.org/ - SoilGrids: https://soilgrids.org/ - Geomorpho90m: https://www.geomorpho90m.org/ Administrative Boundaries: - GADM: https://gadm.org/ - Natural Earth: https://www.naturalearthdata.com/ Appendix B: Troubleshooting Common Issues 4.4.3 Memory Issues Problem: R runs out of memory during processing Solution: # Increase memory limit (Windows) memory.limit(size = 32000) # 32 GB # Use terra instead of raster (more efficient) # Process data in chunks # Close unnecessary applications 4.4.4 CRS Misalignment Problem: Spatial layers don’t overlap correctly Solution: # Always check CRS sf::st_crs(your_data) terra::crs(your_raster) # Reproject if needed your_data &lt;- sf::st_transform(your_data, crs = target_crs) your_raster &lt;- terra::project(your_raster, target_crs) 4.4.5 Empty Geometry Problem: SSU or TSU generation fails Solution: # Check for valid geometries st_is_valid(your_sf_object) # Fix invalid geometries your_sf_object &lt;- st_make_valid(your_sf_object) # Remove empty geometries your_sf_object &lt;- your_sf_object[!st_is_empty(your_sf_object), ] Appendix C: Acronyms and Abbreviations Table 4.2: List of acronyms and abbreviations used in this manual Acronym Definition AOI Area of Interest AWC Available Water Capacity BAS Balanced Acceptance Sampling CHELSA Climatologies at High resolution for the Earth’s Land Surface Areas CLHS Conditioned Latin Hypercube Sampling CRS Coordinate Reference System CSC Covariate Space Coverage CSCS Covariate Space Coverage Sampling DEM Digital Elevation Model DSM Digital Soil Mapping EVI Enhanced Vegetation Index FAO Food and Agriculture Organization FPAR Fraction of Photosynthetically Active Radiation GEE Google Earth Engine GRTS Generalized Random Tessellation Stratified ISO International Organization for Standardization KLD Kullback-Leibler Divergence JSD Jensen-Shannon Divergence LULC Land Use and Land Cover MAST Mean Annual Soil Temperature MODIS Moderate Resolution Imaging Spectroradiometer MSSD Mean Squared Shortest Distance MSSSD Mean Squared Shortest Scaled Distance NDVI Normalized Difference Vegetation Index NSM Normalized Soil Moisture PCA Principal Component Analysis PC Principal Component PET Potential Evapotranspiration PSU Primary Sampling Unit ROI Region of Interest SoilFER Soil Mapping for Resilient Agrifood Systems SRS Simple Random Sampling SRTM Shuttle Radar Topography Mission SSU Secondary Sampling Unit SWIR Shortwave Infrared TPI Topographic Position Index TSU Tertiary Sampling Unit TWI Topographic Wetness Index VACS Vision for Adapted Crops and Soils For questions, technical support, or to report issues, please visit the SoilFER GitHub repository or contact the SoilFER team at FAO. "],["tbd.html", "Chapter 5 TBD", " Chapter 5 TBD "],["soil-data-preparation.html", "Chapter 6 Soil Data Preparation 6.1 Introduction 6.2 Data cleaning 6.3 Soil Data Harmonization 6.4 Concepts on digital exchange of soil-related data (ISO 28258) 6.5 Plot data 6.6 Profile data 6.7 Element data 6.8 Specimen data 6.9 Definition of laboratory procedures 6.10 Data transformation 6.11 Organizing soil data according to the GloSIS database 6.12 Tools for harmonization of soil databases within GloSIS", " Chapter 6 Soil Data Preparation 6.1 Introduction 6.2 Data cleaning 6.3 Soil Data Harmonization 6.4 Concepts on digital exchange of soil-related data (ISO 28258) 6.5 Plot data 6.6 Profile data 6.7 Element data 6.8 Specimen data 6.9 Definition of laboratory procedures 6.10 Data transformation 6.11 Organizing soil data according to the GloSIS database 6.12 Tools for harmonization of soil databases within GloSIS "],["digital-soil-mapping-and-modeling.html", "Chapter 7 Digital soil mapping and modeling", " Chapter 7 Digital soil mapping and modeling "],["soil-data-sharing.html", "Chapter 8 Soil Data Sharing 8.1 Data sharing and export formats 8.2 Metadata 8.3 Web Services", " Chapter 8 Soil Data Sharing 8.1 Data sharing and export formats 8.2 Metadata 8.3 Web Services "],["references-1.html", "References", " References "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
