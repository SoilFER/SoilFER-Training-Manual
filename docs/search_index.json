[["index.html", "Guided R tutorials covering the soil information and data lifecycle, from soil sampling design and mapping to data-driven decision-making. Licence", " DRAFT - Integrated soil information for decision-making Guided R tutorials covering the soil information and data lifecycle, from soil sampling design and mapping to data-driven decision-making. Angelini, M.E, Rodriguez Lado, L.,de Sousa Mendes, W., Ribeiro, E., Luotto, I. Licence The Technical Manual is made available under the Creative Commons Attribution-NonCommercial-ShareAlike 3.0 IGO licence CC BY-NC-SA 3.0 IGO. "],["abbreviations-and-acronyms.html", "Abbreviations and acronyms", " Abbreviations and acronyms BD Bulk density CEC Cation exchange capacity CRAN Comprehensive R archive network DSM Digital soil mapping GEE Google Earth Engine GSP Global Soil Partnership INSII International Network of Soil Information Institutions ITPS Intergovernmental Technical Panel on Soils ME Mean error MAE Mean absolute error MEC Modelling efficiency coefficient NDVI Normalized difference in vegetation index QA/QC Quality assurance/quality check RMSE Root mean squared error SOC Soil organic carbon SOM Soil organic matter "],["contributors-and-reviewers.html", "Contributors and reviewers", " Contributors and reviewers "],["presentation-and-basics.html", "Presentation and basics Background and Objective What this Tutorial is not How to use this book", " Presentation and basics Background and Objective Healthy and productive soils are fundamental to resilient agrifood systems, sustainable land management, and climate adaptation. Yet, in many regions, soil information remains fragmented, outdated, or inaccessible. The Soil Mapping for Resilient Agrifood Systems (SoilFER) programme responds to this challenge by building comprehensive soil information systems that integrate sampling design, laboratory analysis, soil spectroscopy, digital modeling, and decision support tools. These systems aim to empower governments, researchers, and farmers with actionable knowledge for crop selection, fertilizer recommendations, and soil health management. This manual, provides a step-by-step guide along the entire soil data value chain. It is designed as both a technical reference and a practical training resource, bridging the gap between raw soil data and its functional use in agricultural and environmental decision-making. The objectives of this manual are to: Present harmonized approaches for soil sampling design used under the SoilFER programme Provide guidance on the integration soil spectroscopy estimated soil parameters into the digital soil mapping process Introduce best practices in soil data preparation and management, aligned with the Global Soil Information System (GloSIS) Demonstrate methods for digital soil modeling and mapping, covering classical statistics, machine learning, and hybrid inference for both continuous and categorical soil properties. Explain how to generate functional soil information to support evidence-based decision-making. Facilitate data sharing and dissemination, promoting open standards, metadata documentation, and web-based services. By integrating these components, the manual equips users to move from raw samples to reliable soil information products that inform policy, guide sustainable soil management, and strengthen food and nutrition security. What this Tutorial is not This is not a comprehensive course on R, statistics, or modeling. Instead, it focuses on the essential skills needed to work effectively with soil data. The main strength of this tutorial is that it provides a complete learning environment that combines lessons, hands-on examples, and assessments—with a strong emphasis on soil data workflows. The goal is to help you build practical skills in soil data management, soil sampling design, Digital Soil Mapping, and soil spectroscopy using R. For more information, we recommend the following dedicated resources: R for Data Science (Grolemund &amp; Wickham, 2017) : A free, beginner-friendly guide to doing data science with R, emphasizing best practices for reproducible and efficient analysis. Spatial Sampling with R (DJ Brus, 2023) : A practical guide to designing and analyzing spatial surveys in R, with examples and exercises for environmental and natural resource studies. Predictive Soil Mapping with R (Hengl &amp; MacMillan, 2019): An introduction to statistical and machine-learning methods for producing soil property and soil class maps, with workflows and code examples in R. Statistics for Soil Survey (Soil Survey Staff, 2025): An open, R-based textbook covering core statistical methods for soil survey, with practical examples and a strong focus on Algorithms for Quantitative Pedology using the{aqp} package. Spatial Data Science: With Applications in R (Pebesma &amp; Bivand, 2023): Book and online materials for spatial data analysis in R, with a focus on the {sf} package. Spatial Data Science with R and ‘terra’: Online materials for spatial data analysis and modeling in R, with a focus on {terra}. What They Forgot to Teach You About R: A short, practical guide with tips and workflows for working effectively in R. How to use this book This tutorial manual introduces the essential concepts of soil data management and DSM using examples based on the KSSL soil dataset in Kansas. The tutorial is organized in 6 modules with different chapters: Introduction to R and Preparation of Soil Data for Digital Soil Mapping Soil Sampling Design GloSIS Data Harmonization Digital Soil Mapping Soil Spectroscopy Soil Data Sharing Each module combines conceptual explanations with practical coding exercises, tabular datasets, and spatial data. To follow the workflows as intended, participants are expected to work locally with the predefined project structure used throughout the SoilFER training Required downloads All training materials are distributed across two complementary sources. Scripts and lightweight tabular data (GitHub) The GitHub repository contains: All scripts used throughout the manual Lightweight tabular datasets The full project folder structure, organised by module Download the repository as a ZIP file from: https://github.com/SoilFER/SoilFER-Training-Resources Large input datasets (Google Drive) The Google Drive folder contains files that are too large to be hosted on GitHub, including: Raster covariates and maps Large spectral datasets Other supporting input data used in selected modules Download the data from: https://drive.google.com/drive/folders/1K7tq9zX5HsqbqWcNoT27WtfPtehcKBCu Step-by-step setup instructions Follow these steps in order before starting any module exercises. Step 1: Download and extract the GitHub repository Download the ZIP file from the GitHub link above. Extract (unzip) the contents on your local machine. Rename the extracted folder if needed (e.g. SoilFER-Training-Resources). Use this folder as your main project directory. Step 2: Download the Google Drive data Download the full contents of the Google Drive folder. Keep all file names and folder structures unchanged. Do not work directly from the downloaded Google Drive folder. Step 3: Inspect the project folder structure Open the extracted project directory. Confirm that the top-level structure matches the following: SoilFER-Training-Resources/ ├── 01_data/ │ ├── module1/ │ ├── module2/ │ ├── module3/ │ ├── module4/ │ ├── module5/ │ └── module6/ ├── 02_scripts/ ├── 03_outputs/ ├── 04_assignments/ ├── README.md └── LICENSE Verify that each subfolder under 01_data/ corresponds to a module in this manual. Step 4: Place Google Drive files into the correct module folders Locate the downloaded Google Drive data on your computer. Copy the entire rasters/ folder into: 01_data/module2/ Copy the file MIR_KANSAS_data.xlsx into: 01_data/module3/ Confirm that the folders now resemble: 01_data/ ├── module2/ │ ├── shapes/ │ ├── rasters/ │ └── README.md ├── module3/ │ ├── MIR_KANSAS_data.xlsx │ └── README.md All Google Drive files must be placed inside the appropriate 01_data/moduleX/ folders. Scripts rely on relative paths and will not run correctly if files are stored elsewhere. "],["introduction-to-r-programming.html", "Introduction to R programming 0.1 What is R? 0.2 Installing R and RStudio 0.3 Understanding the RStudio Interface 0.4 R Packages: Extending R’s Capabilities 0.5 R Basics: Objects and Data Types 0.6 Data Structures in R 0.7 Operators in R 0.8 Control Structures: Conditional Statements 0.9 Control Structures: Loops 0.10 Functions in R 0.11 Data Manipulation with Base R 0.12 Data Manipulation with the Tidyverse 0.13 Combining Data Frames 0.14 Missing Data 0.15 Data Reshaping with {tidyr} package: Pivoting 0.16 Handling Below Detection Limit Data 0.17 Working with Soil Data 0.18 References", " Introduction to R programming 0.1 What is R? is a programming language and software environment designed for statistical computing, data analysis, and visualization. It was created in the early 1990s by Ross Ihaka and Robert Gentleman at the University of Auckland, New Zealand, and has since grown into one of the most widely used tools in data science, particularly for statistical modeling. R is open-source, meaning it is freely available and supported by a large global community of users and developers. This community continuously develops new tools and packages that extend R’s capabilities, making it highly adaptable to diverse fields such as ecology, genetics, economics, social sciences and, as in this case, soil science. 0.1.1 Why R for Soil Science? R is particularly well-suited for soil science applications because it offers comprehensive tools for: Data management: Efficiently handle, clean, and transform large soil datasets Statistical analysis: Perform descriptive and inferential statistics, ANOVA, regression models, and more Spatial analysis: Work with geographic data using packages like {terra} and {sf} Digital Soil Mapping: Apply machine learning algorithms for predictive soil mapping Visualization: Create publication-quality maps, charts, and graphs using {ggplot2} and other visualization tools Reproducibility: Share analyses through scripts that others can replicate and verify In the context of SoilFER, the FAO Global Soil Partnership (GSP) and Digital Soil Mapping initiatives, R provides standardized workflows that promote collaboration, transparency, and scientific rigor. R is a analytical go-to platform which unites statistical, spatial, and visualization capabilities in a single ecosystem. It powers digital soil mapping, supports machine learning and reproducible reporting, and connects seamlessly with GIS and database tools. 0.2 Installing R and RStudio To start using R, you need two main components: R: The core programming language and computational engine RStudio: An integrated development environment (IDE) that makes working with R easier 0.2.1 Installing R Step 1: Visit the Comprehensive R Archive Network (CRAN): https://cran.r-project.org/ Step 2: Choose your operating system: Windows: Click “Download R for Windows” → “base” → “Download R-4.x.x for Windows” macOS: Click “Download R for macOS” → Select the appropriate .pkg file for your macOS version Linux: Follow the distribution-specific instructions Step 3: Run the installer and follow the prompts. Accept the default settings unless you have specific preferences. 0.2.2 Installing RStudio Step 1: Visit RStudio’s website: https://posit.co/download/rstudio-desktop/ Step 2: Download the free RStudio Desktop version for your operating system Step 3: Install RStudio by running the installer Once installed, R provides the underlying engine for data analysis—but working directly in base R can be challenging due to its command-line interface. Important Install R before installing RStudio, as RStudio requires R to function. 0.2.3 Verifying Installation After installation, open RStudio. You should see a window with several panes: Console (bottom left): Where R commands are executed Source (top left): Where you write and edit scripts Environment/History (top right): Shows variables and command history Files/Plots/Packages/Help (bottom right): File browser, plot viewer, package manager, and help documentation Try typing a simple command in the Console window: # Simple arithmetic 2 + 2 # Check R version R.version.string If you see the results, R and RStudio are properly installed in your system. 0.3 Understanding the RStudio Interface RStudio provides an integrated environment that simplifies working with R. The interface is divided into four main panes (Fig. 1): 0.3.1 1. Console Pane (Bottom Left) The Console is where R code is executed. You can type commands directly here or run them from your script. The Console displays results, warnings, and error messages. # Type directly in the console 5 * 3 # R will immediately show the result # [1] 15 0.3.2 2. Code Editor Pane (Top Left) You can write commands directly in the Console and execute them line by line. However, it is usually more convenient to write code in a script editor—the Code Editor pane. Here you can create and edit R scripts (.R files), R Markdown documents (.Rmd files), and other file types. Scripts allow you to save your code and run it repeatedly, which is essential for reproducible analysis. Working in the Code Editor also makes it easier to organize code for later use. You can send any line or selected block of code to the Console for execution by pressing Ctrl + Enter (Windows/Linux) or Cmd + Enter (macOS). You can also run code by clicking the Run button in the top-right corner of the Code Editor pane. Code can be written and executed directly in the Console. Using the Code Editor is more convenient for organizing and reusing code. Send a line or block of code to the console with Ctrl + Enter (Windows(), Cmd + Enter (macOS), or the equivalent shortcut in Linux. You can also run code by clicking the Run button in the script editor. Creating a new script: - File → New File → R Script (or press Ctrl+Shift+N / Cmd+Shift+N) 0.3.3 3. Environment/History Pane (Top Right) Environment: Shows all objects (variables, datasets, functions) currently in your R session History: Records all commands you’ve run during the session 0.3.4 4. Files/Plots/Packages/Help Pane (Bottom Right) Files: Browse your computer’s file system Plots: View visualizations created with R Packages: Manage installed packages Help: Access R documentation and function help pages Best Practice Always work in scripts rather than typing directly in the Console. Scripts preserve your workflow and make your analysis reproducible. 0.4 R Packages: Extending R’s Capabilities R’s strength lies in its extensibility through packages. A package is a collection of functions, data, and documentation that extends R’s functionality for specific tasks. 0.4.1 What are Packages? The base R installation includes fundamental functions for data manipulation and statistical analysis. However, specialized tasks often require additional tools provided by contributed packages. For soil science work, key packages include: {tidyverse}: A collection of packages for data manipulation and visualization, including {dplyr} and {tidyr}, among others {terra}: Spatial data analysis and raster operations {sf}: Working with vector spatial data {aqp}: Algorithms for Quantitative Pedology (soil profile data) {ggplot2} : Advanced data visualization (part of tidyverse) 0.4.2 Installing Packages Packages need to be installed once before you can use them. Use install.packages(): # Install a single package install.packages(&quot;tidyverse&quot;) # Install multiple packages at once install.packages(c(&quot;terra&quot;, &quot;sf&quot;, &quot;aqp&quot;)) You only need to install a package once, but you must load it with library() each time you start a new R session. You can check which {packages} are installed with: installed.packages() 0.4.2.1 Install Packages from GitHub or Other Sources Some {packages} are not yet available on CRAN or you may want a newer development versions from GitHub, Gitlab, bitbucket, or an URL. In these cases, you can use the remotes or devtools {packages}, with the functions install_github(), install_gitlab(), install_bitbucket() or install_url(): # First, install remotes package (if not already installed) install.packages(&quot;remotes&quot;) # Install an R package from GitHub remotes::install_github(&quot;rspatial/terra&quot;) This is useful for accessing cutting-edge versions, experimental features, or tools developed by research groups. 0.4.2.2 Manual Installation of R Packages You may need to install R {packages} manually—especially when working in environments without internet access or when using custom-built {packages}. There are two common methods: 1. Installing from a compressed source package file (e.g., mypackage_1.0.0.tar.gz) install.packages(&quot;path/to/mypackage_1.0.0.tar.gz&quot;, repos = NULL, type = &quot;source&quot;) 2. Installing from a Local .zip file (Windows Binary) install.packages(&quot;path/to/mypackage.zip&quot;, repos = NULL, type = &quot;win.binary&quot;) This method does not require compilation and is usually faster on Windows. Note: Package installation typically requires an internet connection. Depending on the package size and your connection speed, installation may take several minutes. 0.4.3 Loading Packages After installation, you must load a package into your R session each time you start R. Use the library() function: # Load tidyverse package library(tidyverse) # Load multiple packages library(terra) library(sf) library(aqp) Key Difference: install.packages(): Downloads and installs a package (once) library(): Loads a package into your current session (every time you start R) 0.4.4 Finding Help on Packages # Get help on a package help(package = &quot;tidyverse&quot;) # Or use ?tidyverse # View vignettes (tutorials) for a package vignette(package = &quot;ggplot2&quot;) 0.5 R Basics: Objects and Data Types In R, everything you work with is an object (numbers, text, vectors, tables, models, and maps). When you create an object, R stores it in your computer’s memory (RAM) so you can reuse it later. Because memory is limited, especially when working with large soil datasets, rasters, or spatial objects, it is good practice to keep your workspace tidy: reuse objects when appropriate, remove objects you no longer need with rm(), and occasionally trigger garbage collection with ´gc()´ to free memory that is no longer in use. 0.5.1 Creating Objects with Assignment If you run an operation without assigning it to an object, R will compute the result but won’t store it for later use. You can assign values to objects using either &lt;- or =. However, the preferred and most common assignment operator is &lt;- while = is most commonly used inside function calls to name arguments. This avoids confusion between assigning objects and passing inputs to functions. # Assign a value to a variable soil_depth &lt;- 30 # Assignment `&lt;-` to objects and `=` to function arguments soil_depth &lt;- mean(x = ph_values, na.rm = TRUE) Naming Rules: - Object names must start with a letter - Names can contain letters, numbers, underscores _, and periods . - Names are case-sensitive: SoilDepth is different from soildepth - Avoid using reserved words like TRUE, FALSE, NA, function, etc. Good naming practices: # Descriptive names soil_ph &lt;- 6.5 organic_carbon_percent &lt;- 2.1 clay_content_gkg &lt;- 350 # Use consistent style plot_id &lt;- &quot;P001&quot; # snake_case (recommended) plotID &lt;- &quot;P001&quot; # camelCase (alternative) Avoid: # Too short, unclear x &lt;- 6.5 a &lt;- 2.1 # Too long the_ph_value_of_the_topsoil_at_site_one &lt;- 6.5 0.5.2 Data Types in R Every object in R has a data type, which tells R what kind of information it contains and how it can be used (for example, whether you can do math with it or use it as text labels). R has several basic data types: 0.5.2.1 1. Numeric (Numbers) Numeric values store measurements that can include decimals (e.g., pH, clay %, temperature). # Numeric values ph_value &lt;- 6.8 clay_percent &lt;- 25.5 temperature &lt;- 15.2 0.5.2.2 2. Integer (Whole Numbers) Integers are whole numbers. In R, you can create them explicitly using the L suffix. # Integer values (use L suffix) sample_count &lt;- 100L plot_number &lt;- 5L 0.5.2.3 3. Character (Text Strings) Character values store text (soil types, locations, notes). They must be written inside quotes. # Character values (use quotes) soil_type &lt;- &quot;Acrisol&quot; location &lt;- &quot;Kansas&quot; notes &lt;- &quot;Sample collected from topsoil&quot; 0.5.2.4 4. Logical (TRUE/FALSE) Logical values represent yes/no conditions and are commonly used for filtering and decision-making. # Logical values is_valid &lt;- TRUE has_missing_data &lt;- FALSE 0.5.2.5 5. Dates Dates often are imported as character strings, but it is better to convert them to the Date class for sorting, filtering, and plotting. # ISO 8601 date format, YYYY-MM-DD sampling_date &lt;- &quot;2024-03-12&quot; sampling_date &lt;- as.Date(sampling_date, format = &quot;%Y-%m-%d&quot;) class(sampling_date) The ISO 8601 date format (YYYY-MM-DD) is preferred for storing date values, since it is the date format adopted in the GloSIS database. 0.5.3 Checking Data Types # Check the class (high-level type) class(ph_value) # &quot;numeric&quot; class(soil_type) # &quot;character&quot; class(is_valid) # &quot;logical&quot; # Check the internal storage type typeof(ph_value) # Helpful checks is.numeric(ph_value) is.character(soil_type) is.logical(is_valid) class() is what you’ll use most in practice. typeof() is more ‘internal’ (how R stores the object). Every R object has a data type, which tells R how to store the information and what operations are possible. Numeric: measurements such as pH, clay (%), bulk density, or organic carbon. Numeric vectors: multiple numeric values (e.g., pH readings from several samples or horizons). Character: text labels such as soil type, land use, site code, or field notes. Character vectors: multiple text values (e.g., a list of soil classes). Logical: TRUE/FALSE values, often used for conditions, filtering, and quality checks. Caution: When importing data from external sources (CSV, Excel, databases), always check that columns have the expected data types. For example, pH or SOC may be imported as text instead of numeric. Check types with str(), class(), or typeof(). Convert types when needed using as.numeric(), as.character(), as.logical(), or as.Date(). 0.6 Data Structures in R R can store data in different structures, depending on how many values you have and how you want to organize them. For example, a single measurement can be stored as a number, a series of measurements as a vector, and a full dataset as a data frame. Understanding these structures is essential because it determines how you subset data, apply functions, summarize results, and prepare data for plotting or modeling. 0.6.1 Vectors A vector is the simplest data structure - a one-dimensional sequence of elements of the same type. 0.6.1.1 Creating Vectors # Create a numeric vector using c() (combine) ph_values &lt;- c(5.2, 6.5, 7.1, 5.8, 6.9) print(ph_values) # Character vector soil_types &lt;- c(&quot;Acrisol&quot;, &quot;Ferralsol&quot;, &quot;Vertisol&quot;, &quot;Andosol&quot;, &quot;Cambisol&quot;) print(soil_types) # Logical vector valid_samples &lt;- c(TRUE, TRUE, FALSE, TRUE, TRUE) print(valid_samples) # Create sequences depths &lt;- 0:100 # Integers from 0 to 100 depths_seq &lt;- seq(0, 100, by=10) # 0, 10, 20, ..., 100 0.6.1.2 Vector Operations # Arithmetic on vectors (element-wise) ph_values * 10 ph_values + 1 # Summary statistics mean(ph_values) median(ph_values) sd(ph_values) # standard deviation min(ph_values) max(ph_values) 0.6.1.3 Accessing Vector Elements # Access by index (position) ph_values[1] # First element ph_values[3] # Third element # Access multiple elements ph_values[c(1,3,5)] # Elements 1, 3, and 5 # Access by logical condition ph_values[ph_values &gt; 6] # All pH values greater than 6 # Negative indices exclude elements ph_values[-1] # All except first ph_values[-c(1,2)] # All except first two 0.6.2 Factors Factors are used to represent categorical data. They store values as levels (categories). # Create a factor from character vector soil_class &lt;- factor(c(&quot;Clay&quot;, &quot;Loam&quot;, &quot;Sand&quot;, &quot;Clay&quot;, &quot;Loam&quot;)) print(soil_class) # Check levels levels(soil_class) # Count observations per level table(soil_class) # Ordered factors (when order matters) texture_class &lt;- factor( c(&quot;Coarse&quot;, &quot;Fine&quot;, &quot;Medium&quot;, &quot;Fine&quot;, &quot;Coarse&quot;), levels = c(&quot;Coarse&quot;, &quot;Medium&quot;, &quot;Fine&quot;), ordered = TRUE ) print(texture_class) 0.6.3 Matrices A matrix is a two-dimensional structure where all elements must be of the same type. # Create a matrix soil_matrix &lt;- matrix( c(5.2, 25, 30, 6.5, 30, 28, 7.1, 18, 35), nrow = 3, ncol = 3, byrow = TRUE ) # Add column names colnames(soil_matrix) &lt;- c(&quot;pH&quot;, &quot;Clay&quot;, &quot;Sand&quot;) rownames(soil_matrix) &lt;- c(&quot;Sample1&quot;, &quot;Sample2&quot;, &quot;Sample3&quot;) print(soil_matrix) # Access elements soil_matrix[1, 2] # Row 1, Column 2 soil_matrix[1, ] # All of row 1 soil_matrix[, 2] # All of column 2 0.6.4 Data Frames A data frame is the most commonly used structure for storing datasets. It’s like a spreadsheet: rows represent observations, columns represent variables, and different columns can have different data types. 0.6.4.1 Creating Data Frames # Create a data frame soil_data &lt;- data.frame( plot_id = c(&quot;P001&quot;, &quot;P002&quot;, &quot;P003&quot;, &quot;P004&quot;, &quot;P005&quot;), latitude = c(-1.25, -1.27, -1.23, -1.29, -1.26), longitude = c(36.85, 36.83, 36.87, 36.81, 36.84), ph = c(5.2, 6.5, 7.1, 5.8, 6.9), organic_carbon = c(2.1, 3.2, 1.8, 2.7, 2.9), clay_content = c(25, 30, 18, 42, 35), soil_type = c(&quot;Acrisol&quot;, &quot;Ferralsol&quot;, &quot;Vertisol&quot;, &quot;Andosol&quot;, &quot;Cambisol&quot;) ) # View the data frame print(soil_data) # View structure str(soil_data) # View first few rows head(soil_data) # View last few rows tail(soil_data) # Get dimensions dim(soil_data) # rows, columns nrow(soil_data) # number of rows ncol(soil_data) # number of columns 0.6.4.2 Accessing Data Frame Elements # Access columns by name soil_data$ph soil_data$soil_type # Alternative: use brackets soil_data[, &quot;ph&quot;] soil_data[[&quot;ph&quot;]] # Access rows soil_data[1, ] # First row soil_data[c(1,3,5), ] # Rows 1, 3, and 5 # Access specific cells soil_data[2, 4] # Row 2, Column 4 (pH of second plot) # Subset based on conditions soil_data[soil_data$ph &gt; 6, ] # Plots with pH &gt; 6 soil_data[soil_data$soil_type == &quot;Acrisol&quot;, ] # Only Acrisols 0.6.4.3 Adding Columns # Add a new column soil_data$silt_content &lt;- c(40, 35, 52, 23, 30) # Calculate new columns from existing ones soil_data$clay_plus_silt &lt;- soil_data$clay_content + soil_data$silt_content # View updated data frame head(soil_data) 0.6.5 Lists A list is a flexible structure that can contain elements of different types and sizes (vectors, data frames, other lists, etc.). # Create a list soil_analysis &lt;- list( site_name = &quot;Kansas Field&quot;, coordinates = c(lat = -1.25, lon = 36.85), measurements = data.frame( depth = c(0, 10, 20, 30), ph = c(6.5, 6.2, 5.8, 5.5) ), notes = &quot;Collected during dry season&quot; ) # View list structure str(soil_analysis) # Access list elements soil_analysis$site_name soil_analysis[[1]] # First element soil_analysis[[&quot;measurements&quot;]] # measurements data frame 0.7 Operators in R Operators perform operations on objects. R has several types of operators: 0.7.1 Arithmetic Operators # Basic arithmetic 10 + 5 # Addition 10 - 5 # Subtraction 10 * 5 # Multiplication 10 / 5 # Division 10 ^ 2 # Exponentiation (10 squared) 10 %% 3 # Modulus (remainder: 10 mod 3 = 1) 10 %/% 3 # Integer division (10 divided by 3 = 3) # Order of operations (PEMDAS) result &lt;- (10 + 5) * 2 / 4 - 1 result 0.7.2 Comparison Operators # Comparison operators return TRUE or FALSE 5 == 5 # Equal to 5 != 3 # Not equal to 5 &gt; 3 # Greater than 5 &lt; 3 # Less than 5 &gt;= 5 # Greater than or equal to 5 &lt;= 6 # Less than or equal to # Use in subsetting ph_values &lt;- c(5.2, 6.5, 7.1, 5.8, 6.9) ph_values &gt; 6 # Logical vector ph_values[ph_values &gt; 6] # Values greater than 6 0.7.3 Logical Operators # AND operator: &amp; (element-wise) or &amp;&amp; (single values) TRUE &amp; TRUE # TRUE TRUE &amp; FALSE # FALSE # OR operator: | (element-wise) or || (single values) TRUE | FALSE # TRUE FALSE | FALSE # FALSE # NOT operator: ! !TRUE # FALSE !FALSE # TRUE # Combining conditions ph_values &lt;- c(5.2, 6.5, 7.1, 5.8, 6.9) clay_content &lt;- c(25, 30, 18, 42, 35) # Find samples with pH &gt; 6 AND clay &gt; 25 ph_values &gt; 6 &amp; clay_content &gt; 25 # Find samples with pH &gt; 6 OR clay &gt; 40 ph_values &gt; 6 | clay_content &gt; 40 0.8 Control Structures: Conditional Statements Conditional statements let your code make decisions—running different blocks depending on whether a condition is TRUE or FALSE. 0.8.1 If-Else Statements Use if / else when you are checking a single condition (one TRUE/FALSE value). # Basic if statement ph_value &lt;- 7.5 # Basic if statement if (ph_value &gt; 7) { print(&quot;Alkaline soil&quot;) } # If-else if (ph_value &gt; 7) { print(&quot;Alkaline soil&quot;) } else { print(&quot;Neutral or acidic soil&quot;) } # Multiple conditions (only the first TRUE branch runs) if (ph_value &gt; 7.5) { print(&quot;Strongly alkaline&quot;) } else if (ph_value &gt; 7) { print(&quot;Slightly alkaline&quot;) } else if (ph_value == 7) { print(&quot;Neutral&quot;) } else { print(&quot;Acidic&quot;) } if (...) expects a single logical value. If you have a vector of values, use a vectorized approach such as ifelse(), cut(), or case_when(). 0.8.2 Vectorized If-Else: ifelse() ifelse() is vectorized: it applies a condition to each element of a vector. # Vectorized conditional assignment ph_values &lt;- c(5.2, 6.5, 7.1, 5.8, 7.0) # Simple two-class example soil_reaction &lt;- ifelse(ph_values &gt; 7, &quot;Alkaline&quot;, &quot;Not alkaline&quot;) soil_reaction 0.8.3 Vectorized Cut: cut() Use cut() when you have a numeric variable and you want to classify values into interval-based categories (bins), such as pH classes, depth intervals, or temperature ranges. It is especially useful when: the variable is continuous or ordered (numeric), you can define meaningful break points, and you want a clear, readable alternative to many nested conditions. For multiple classes, cut() is often easier to read than nested ifelse(): soil_class &lt;- cut( ph_values, breaks = c(-Inf, 5.5, 7.0, Inf), labels = c(&quot;Acidic&quot;, &quot;Neutral&quot;, &quot;Alkaline&quot;), right = TRUE, include.lowest = TRUE ) soil_class # Notes: # - `right = TRUE` means intervals are **right-closed** (e.g., `(5.5, 7.0]`). # - `include.lowest = TRUE` ensures the smallest value is included in the first interval. 0.9 Control Structures: Loops Loops in R are used to repeat a block of code multiple times. They are helpful when you need to automate repetitive tasks, such as performing calculations over a sequence of values or processing each element of a dataset. R provides several loop constructs, but the most commonly used are for and while. 0.9.1 For Loops Use a for loop when you want to iterate over a sequence or over the elements of an object. # Loop through a sequence for (i in 1:5) { print(paste(&quot;Iteration:&quot;, i)) } # Loop through a vector soil_types &lt;- c(&quot;Acrisol&quot;, &quot;Ferralsol&quot;, &quot;Vertisol&quot;) for (soil in soil_types) { print(paste(&quot;Soil type:&quot;, soil)) } # Loop with conditional logic ph_values &lt;- c(5.2, 6.5, 7.1, 5.8, 6.9) for (i in 1:length(ph_values)) { if (ph_values[i] &gt; 6) { print(paste(&quot;Sample&quot;, i, &quot;has pH =&quot;, ph_values[i], &quot;(Acceptable)&quot;)) } else { print(paste(&quot;Sample&quot;, i, &quot;has pH =&quot;, ph_values[i], &quot;(Too acidic)&quot;)) } } 0.9.2 While Loops A while loop repeats as long as a condition is TRUE. It is useful when you do not know in advance how many iterations you will need. # While loop continues until condition is FALSE counter &lt;- 1 while (counter &lt;= 5) { print(paste(&quot;Counter value:&quot;, counter)) counter &lt;- counter + 1 # Increment counter } Caution: Always ensure loop conditions eventually become FALSE to avoid infinite loops! 0.10 Functions in R Functions are reusable blocks of code that take inputs (arguments), perform a task, and return an output. They help you avoid repeating code and make your scripts easier to read and maintain. 0.10.1 Using Built-in Functions R comes with thousands of built-in functions. You can view help pages with ?function_name (e.g., ?mean). # Statistical functions mean(c(5, 10, 15, 20)) median(c(5, 10, 15, 20)) sd(c(5, 10, 15, 20)) sum(c(5, 10, 15, 20)) # String functions toupper(&quot;acrisol&quot;) tolower(&quot;FERRALSOL&quot;) nchar(&quot;soil science&quot;) # Count characters # Math functions sqrt(16) log(10) exp(2) abs(-5) round(3.14159, 2) Function arguments can be provided by position (e.g., round(3.14159, 2)) or by name (e.g., round(x = 3.14159, digits = 2)). Using names is often clearer and reduces mistakes. 0.10.2 Creating Custom Functions You can write your own functions using function(): # Example: Bulk density (mass / volume) calculate_bulk_density &lt;- function(mass, volume) { if (any(volume &lt;= 0)) stop(&quot;volume must be &gt; 0&quot;) mass / volume } calculate_bulk_density(mass = 150, volume = 100) # Function with a default argument classify_soil_ph &lt;- function(ph, threshold = 7) { if (ph &gt; threshold) { &quot;Alkaline&quot; } else if (ph == threshold) { &quot;Neutral&quot; } else { &quot;Acidic&quot; } } classify_soil_ph(6.5) classify_soil_ph(6.5, threshold = 6) return() is optional in many cases, R returns the last evaluated expression. You can still use return() when you want to exit early or make the function’s output explicit. 0.10.3 Function Arguments and Defaults # SOC stock calculation (example) # Assumptions: # - soc_percent is in % # - bulk_density is in g/cm^3 # - depth is in cm # Output: SOC stock in Mg/ha calculate_soc_stock &lt;- function(soc_percent, bulk_density, depth, coarse_fragment = 0) { if (any(coarse_fragment &lt; 0 | coarse_fragment &gt; 100)) stop(&quot;coarse_fragment must be between 0 and 100&quot;) soc_percent * bulk_density * depth * (1 - coarse_fragment / 100) } calculate_soc_stock(soc_percent = 2.5, bulk_density = 1.3, depth = 30) calculate_soc_stock(soc_percent = 2.5, bulk_density = 1.3, depth = 30, coarse_fragment = 15) 0.11 Data Manipulation with Base R This section shows common data manipulation tasks using base R: filtering rows, selecting columns, sorting, and summarizing by groups. 0.11.1 Subsetting and Filtering Use subsetting and filtering to extract only certain rows or columns from a dataset. This is often the first step when you want to focus on data that meet specific criteria. In data frames, data access is organized by rows and columns using the pattern data[rows, columns], where: rows specifies which observations (records) to keep. It can be a number, a logical condition, or a vector of indices. columns specifies which variables to keep. It can be a name, a position, or a vector of indices. In this context, filtering usually refers to selecting rows, while subsetting often refers to selecting columns. In the example below, we use this structure to filter rows and subset columns from a soil_data data frame. # Example data frame soil_data &lt;- data.frame( plot_id = c(&quot;P001&quot;, &quot;P002&quot;, &quot;P003&quot;, &quot;P004&quot;, &quot;P005&quot;), latitude = c(-1.25, -1.27, -1.23, -1.29, -1.26), longitude = c(36.85, 36.83, 36.87, 36.81, 36.84), ph = c(5.2, 6.5, 7.1, 5.8, 6.9), clay_content = c(25, 30, 18, 42, 35), soil_type = c(&quot;Acrisol&quot;, &quot;Ferralsol&quot;, &quot;Vertisol&quot;, &quot;Andosol&quot;, &quot;Cambisol&quot;) ) # Select column by index: Show second column soil_data[, 2] # Select column by index: Show first and third columns soil_data[, c(1,3] # Select column by name: Show texture columns soil_data[, c(&quot;clay&quot;,&quot;silt&quot;,&quot;sand&quot;)] # Filter rows by index: keep first 2 records in the data frame soil_data[c(1:2),] # Filter rows by index: keep texture properties for the first 100 records in the data frame soil_data[c(1:2), c(&quot;clay&quot;,&quot;silt&quot;,&quot;sand&quot;)] # Filter rows based on conditions high_ph &lt;- soil_data[soil_data$ph &gt; 6, ] high_ph # Multiple conditions with &amp; and | high_ph_clay &lt;- soil_data[soil_data$ph &gt; 6 &amp; soil_data$clay_content &gt; 25, ] high_ph_clay # Select columns by index: Show second column subset_data &lt;- soil_data[, 2] # Select several columns by index subset_data &lt;- soil_data[, c(1:3] # Select by name subset_data &lt;- soil_data[, c(&quot;plot_id&quot;, &quot;ph&quot;, &quot;soil_type&quot;)] subset_data 0.11.2 Deleting Columns You can remove columns or rows from a data frame using names, indices, or logical conditions. This is useful when you want to drop unnecessary variables, remove incomplete records, or exclude outliers before analysis. # Example data frame data &lt;- data.frame( Plot = c(&quot;P001&quot;, &quot;P002&quot;, &quot;P003&quot;), Clay = c(25, 30, 18), Silt = c(40, 35, 52), pH = c(5.2, 6.5, 7.1) ) # --- Delete columns --- # Delete one column by name data$Clay &lt;- NULL data$Clay &lt;- c() # Empty vector # Delete multiple columns by name data[, c(&quot;Silt&quot;, &quot;pH&quot;)] &lt;- NULL # --- Delete rows --- # Delete a row by index (e.g., remove the 2nd row) data &lt;- data[-2, ] # Delete rows based on a condition (e.g., remove rows with pH &lt; 6) data &lt;- data[data$pH &gt;= 6, ] To delete columns, use NULL (recommended) or an empty vector. To delete rows, use negative indices (e.g., data[-2, ]) or a logical condition (e.g., data[data$pH &gt;= 6, ]). 0.11.3 Sorting Data Sorting is useful for quickly identifying extreme values (e.g., highest pH, highest clay content) or for preparing tables for reporting. # Sort by pH (ascending) soil_data[order(soil_data$ph), ] # Sort by pH (descending) soil_data[order(-soil_data$ph), ] # Sort by multiple columns soil_data[order(soil_data$soil_type, soil_data$ph), ] 0.11.4 Aggregating Data Aggregation means computing summary statistics by group, such as mean pH per soil type. # Mean pH by soil type aggregate(ph ~ soil_type, data = soil_data, FUN = mean) # Multiple summary values by group (mean and standard deviation of pH by soil_type) aggregate( ph ~ soil_type, data = soil_data, FUN = function(x) c(mean = mean(x), sd = sd(x)) ) When using aggregate(), make sure the grouping variable (here soil_type) is correctly imported as character or factor, and that the summarized variable (here ph) is numeric. 0.12 Data Manipulation with the Tidyverse While base R is powerful, the tidyverse makes many common data tasks easier to write, read, and maintain—especially when working with real datasets. Its functions use a consistent “verb” style (e.g., filter(), select(), mutate(), summarize()), and the pipe operator (%&gt;% or |&gt;) lets you build clear, step-by-step workflows. This is particularly useful in soil data analysis, where you often need to clean, subset, join, and summarize data repeatedly. The {tidyverse} is a collection of R packages designed for data science that share a common philosophy and syntax. Key packages include: {dplyr}: Data manipulation {ggplot2}: Data visualization {tidyr}: Data reshaping {readr}: Reading data {tibble}: Modern data frames 0.12.1 Loading the Tidyverse When you load {tidyverse}, it automatically attaches a core set of tidyverse packages (such as {dplyr} and {ggplot2}) so you can use them right away. # Install if not already installed install.packages(&quot;tidyverse&quot;) # Load tidyverse library(tidyverse) 0.12.2 Tibbles: Modern Data Frames A tibble is the tidyverse’s modern version of a data frame. It behaves like a regular data frame, but it prints in a cleaner way and is generally more user-friendly. For example, tibbles show only the first rows by default, keep long text from cluttering your console, and display column types so you can quickly confirm how R has interpreted your data. Tibbles also avoid some older base R behaviors (such as automatically converting text to factors in older R versions), which makes them a reliable default for data analysis workflows. # Convert data frame to tibble soil_tbl &lt;- as_tibble(soil_data) soil_tbl # Create tibble directly soil_tbl &lt;- tibble( plot_id = c(&quot;P001&quot;, &quot;P002&quot;, &quot;P003&quot;), ph = c(5.2, 6.5, 7.1), clay = c(25, 30, 18) ) 0.12.3 The Pipe Operator: %&gt;% The pipe operator %&gt;% (from {magrittr} package, loaded with {tidyverse}) helps you write code as a clear sequence of chained operations. Instead of nesting functions inside each other, you send (“pipe”) the output of one step into the next. This makes workflows easier to read and debug—especially when you are cleaning soil datasets where you often need to filter rows, select variables, create new columns, and then summarize or sort results. # Without pipe (nested functions) result &lt;- round(mean(soil_data$ph), 2) # With pipe (sequential operations) result &lt;- soil_data$ph %&gt;% mean() %&gt;% round(2) result # More complex example soil_data %&gt;% filter(ph &gt; 6) %&gt;% select(plot_id, ph, soil_type) %&gt;% arrange(desc(ph)) Key pipe benefits: Readable: Code flows logically from left to right Efficient: No need for intermediate objects Debuggable: Easy to add/remove steps by commenting out lines Natural: Matches how we think about data transformations step-by-step -️ Use Ctrl+Shift+M (Windows) or Cmd+Shift+M (Mac) to insert the pipe operator quickly in RStudio! 0.12.4 Data Manipulation with {dplyr} The {dplyr} package provides a clear set of ‘data verbs’ for manipulating tabular data. Its syntax is designed to be readable: you describe what you want to do (filter rows, select columns, create variables, summarize results) rather than how to do it step by step. This makes {dplyr} especially useful for soil datasets, where you often need to subset observations, compute derived indicators (e.g., pH classes), and summarize results by soil type, horizon, land use, or sampling site. 0.12.4.1 Selecting Columns Use select() to choose the variables you need for analysis or reporting. This helps keep your workflow focused and reduces the chance of mistakes when working with wide datasets (many columns). # Select specific columns soil_data %&gt;% select(plot_id, ph, clay_content) # Select range of columns soil_data %&gt;% select(plot_id:ph) # Remove columns soil_data %&gt;% select(-latitude, -longitude) # Select columns matching pattern soil_data %&gt;% select(contains(&quot;content&quot;)) 0.12.4.2 Filtering Rows Use filter() to keep only the rows that meet one or more conditions. This is commonly used to focus on samples within a range (e.g., pH &gt; 6) or to extract specific soil classes or land uses. # Filter rows based on condition soil_data %&gt;% filter(ph &gt; 6) # Multiple conditions soil_data %&gt;% filter(ph &gt; 6 &amp; clay_content &gt; 25) # Filter with OR soil_data %&gt;% filter(soil_type == &quot;Acrisol&quot; | soil_type == &quot;Ferralsol&quot;) # Use %in% for multiple values soil_data %&gt;% filter(soil_type %in% c(&quot;Acrisol&quot;, &quot;Ferralsol&quot;, &quot;Vertisol&quot;)) 0.12.5 Renaming Columns with rename() rename() changes column names while keeping all the data. The syntax is new_name = old_name. # Rename columns for clarity rename(soil_data, site_id = site) # You can rename multiple columns at once soil_data %&gt;% rename( location = site, acidity = pH, carbon_content = organic_carbon ) 0.12.5.1 Creating/Modifying Columns with mutate() Use mutate() to add new variables or update existing ones. This is where you typically compute derived soil indicators (classes, ratios, unit conversions) while keeping the original dataset intact. # Create new columns soil_data %&gt;% mutate( ph_class = ifelse(ph &gt; 7, &quot;Alkaline&quot;, &quot;Acidic&quot;), clay_percent = clay_content / 10 ) # Modify existing columns soil_data %&gt;% mutate( ph = round(ph, 1), soil_type = toupper(soil_type) ) 0.12.5.2 Arranging (Sorting) Data Use arrange() to sort rows by one or more column values. This is useful for quickly identifying extreme values (e.g., the highest pH) and for ordering results in tables and reports. Use arrange(desc()) for descending order. # Sort ascending soil_data %&gt;% arrange(ph) # Sort descending soil_data %&gt;% arrange(desc(ph)) # Multiple sort keys soil_data %&gt;% arrange(soil_type, desc(clay_content)) 0.12.6 Grouping and Summarizing Data by columns group_by() creates invisible groups in your data using common values on one or more columns, while summarise() calculates compute summary statistics. Combined with group_by(), it becomes a powerful way to calculate metrics by soil type, site, land use, horizon, or any other grouping variable. # Create example dataset soil_data &lt;- data.frame( site = c(&quot;Forest_A&quot;, &quot;Forest_B&quot;, &quot;Grassland_A&quot;, &quot;Grassland_B&quot;, &quot;Urban_A&quot;, &quot;Urban_B&quot;), ecosystem = c(&quot;Forest&quot;, &quot;Forest&quot;, &quot;Grassland&quot;, &quot;Grassland&quot;, &quot;Urban&quot;, &quot;Urban&quot;), pH = c(6.2, 6.8, 7.1, 6.9, 5.8, 6.0), organic_carbon = c(3.2, 2.8, 2.1, 2.4, 1.5, 1.8) ) # Summarize soil properties soil_data %&gt;% summarize( mean_ph = mean(pH), sd_ph = sd(pH), min_soc = min(organic_carbon), max_soc = max(organic_carbon), n_samples = n() ) # Group by and summarize soil_data %&gt;% group_by(ecosystem) %&gt;% summarize( mean_ph = mean(pH), mean_soc = mean(organic_carbon), # Average organic carbon count = n(), .groups = &quot;drop&quot; # Remove grouping ) An alternative function to create quick grouped summaries is count() soil_data &lt;- data.frame( site = c(&quot;Forest_A&quot;, &quot;Forest_B&quot;, &quot;Grassland_A&quot;, &quot;Grassland_B&quot;, &quot;Urban_A&quot;, &quot;Urban_B&quot;), ecosystem = c(&quot;Forest&quot;, &quot;Forest&quot;, &quot;Grassland&quot;, &quot;Grassland&quot;, &quot;Urban&quot;, &quot;Urban&quot;), pH = c(6.2, 6.8, 7.1, 6.9, 5.8, 6.0), organic_carbon = c(3.2, 2.8, 2.1, 2.4, 1.5, 1.8) ) # Count observations by group count(soil_data, ecosystem) # Count by multiple groups count(soil_data, ecosystem, site) # Count with weights of a column (sum of carbon instead of count) soil_data %&gt;% count(ecosystem, wt = organic_carbon, name = &quot;total SOC&quot;) 0.13 Combining Data Frames In soil research, information often comes from multiple sources. For example, one dataset may contain chemical properties (pH, organic carbon), another may include physical measurements (bulk density, nutrients), and a third may describe site conditions (land use, elevation, geology). To get a complete picture, we often need to combine datasets into a single table where all attributes are linked. This is usually done by matching tables using a shared identifier (a key), such as site_id, plot_id, or sample_id. In R, {dplyr} (part of {tidyverse}) provides clear functions to perform these joins. 0.13.1 Understanding Joins A join combines two data frames by matching values in one or more key columns. Each join type differs mainly in which rows (keys) are kept in the result. 0.13.1.1 Join types Join Keeps which keys? Typical use left_join(x, y) All keys in x Add attributes to a main table, keeping all records in x right_join(x, y) All keys in y Same as left join, but keeping all records in y inner_join(x, y) Keys present in both x and y Keep only records with matches in both tables full_join(x, y) Keys present in either x or y Keep everything; unmatched fields becomeNA Before joining, always check: The key columns use the same format and values in both tables (e.g., \"A\" is not the same as \"a\", and extra spaces can cause mismatches). If key columns have different names, map them explicitly, e.g. left_join(x, y, by = c(\"key_in_x\" = \"key_in_y\")) Keys are unique in at least one table. If both tables contain repeated keys, the join can create duplicate rows (a many-to-many join). 0.13.1.2 Example datasets and join operations # Soil data example soil_basic &lt;- data.frame( site_id = c(&quot;A&quot;, &quot;B&quot;, &quot;C&quot;, &quot;D&quot;), pH = c(6.2, 6.8, 7.1, 6.9), organic_carbon = c(3.2, 2.8, 2.1, 2.4) ) # Additional measurements (note: includes data on site E, but not on site A) nutrients &lt;- data.frame( site_id = c(&quot;B&quot;, &quot;C&quot;, &quot;D&quot;, &quot;E&quot;), phosphorus = c(0.15, 0.12, 0.18, 0.14), potassium = c(0.8, 0.9, 0.7, 0.6) ) # Site information site_info &lt;- data.frame( site_id = c(&quot;A&quot;, &quot;B&quot;, &quot;C&quot;, &quot;D&quot;), ecosystem = c(&quot;Forest&quot;, &quot;Forest&quot;, &quot;Grassland&quot;, &quot;Grassland&quot;), elevation = c(450, 520, 380, 420) ) # Join examples (same keys, different rules for which rows are kept) # 1) `left_join()`: Keep all rows from soil_basic left_join(soil_basic, nutrients, by = &quot;site_id&quot;) # 2) `right_join()`: Keep all rows from nutrients right_join(soil_basic, nutrients, by = &quot;site_id&quot;) # 3) `inner_join()`: Keep only rows that exist in both tables inner_join(soil_basic, nutrients, by = &quot;site_id&quot;) # 4) `full_join()`: Keep all rows from both tables (missing values become NA) full_join(soil_basic, nutrients, by = &quot;site_id&quot;) ## Joining more than two tables sequentially soil_basic %&gt;% left_join(nutrients, by = &quot;site_id&quot;) %&gt;% left_join(site_info, by = &quot;site_id&quot;) 0.13.2 Stacking Data with bind_rows() Sometimes datasets have the same columns but represent different campaigns (e.g., seasons, years, field visits). In that case, you combine them vertically (one under the other) using bind_rows(): # Data from different time periods spring_data &lt;- data.frame( site = c(&quot;A&quot;, &quot;B&quot;), season = &quot;Spring&quot;, pH = c(6.1, 6.7), temperature = c(12.5, 11.8) ) summer_data &lt;- data.frame( site = c(&quot;A&quot;, &quot;B&quot;), season = &quot;Summer&quot;, pH = c(6.3, 6.9), temperature = c(18.2, 17.5) ) # Combine the datasets bind_rows(spring_data, summer_data) Caution: bind_rows() works best when column names and types match. If one dataset has extra/missing columns, bind_rows() will create the missing columns and fill them with NA. 0.14 Missing Data Soil datasets often contain missing values. Missing values in R are stored as NA. Many functions will return NA if missing values are present, unless you specify how to handle them (e.g., na.rm = TRUE). Before running summaries, models, or maps, it is important to identify where values are missing and decide how to handle them. 0.14.1 Identifying Missing Data # Create data with missing values soil_data_na &lt;- data.frame( plot = c(&quot;P001&quot;, &quot;P002&quot;, &quot;P003&quot;, &quot;P004&quot;), ph = c(5.2, NA, 7.1, 6.5), clay = c(25, 30, NA, 35) ) # Check for missing values is.na(soil_data_na) # Count missing values per column colSums(is.na(soil_data_na)) # Identify complete cases (rows with no missing values) complete.cases(soil_data_na) # Extract complete cases soil_complete &lt;- soil_data_na[complete.cases(soil_data_na), ] soil_complete 0.14.2 Handling Missing Data How you handle missing data depends on the context. Sometimes you can remove incomplete records; other times you may replace missing values with a reasonable estimate or a fixed value. An example of handling missing data is provided in the Data Preparation section. In base R, is.na() helps you identify missing values, and na.omit() can be used to remove rows with missing data. Many functions (such as mean()) also include arguments that control how missing values are handled—for example, na.rm = TRUE tells R to ignore NA values when computing the result. # Remove rows with any missing values (base R) na.omit(soil_data_na) # Remove rows where a specific column is missing (keep rows with non-missing pH) soil_data_na[!is.na(soil_data_na$ph), ] # Replace missing pH values with the mean pH (na.rm = TRUE ignores NA in the mean) soil_data_na$ph[is.na(soil_data_na$ph)] &lt;- mean(soil_data_na$ph, na.rm = TRUE) soil_data_na If you are using the tidyverse, {tidyr} provides convenient helpers for handling missing data. For example, drop_na() removes rows with missing values (either across all columns or in selected columns), and replace_na() fills missing values with specified replacements. # Remove all rows with any NA soil_data_na %&gt;% drop_na() # Remove rows where ph is NA soil_data_na %&gt;% drop_na(ph) # Replace NA values with specified values soil_data_na %&gt;% replace_na(list(ph = 6.0, clay = 30)) Caution: na.omit() removes any row that contains at least one NA in any column. This can unintentionally drop many observations—especially in wide datasets. drop_na() (like na.omit()) can remove many rows if your dataset has missing values in multiple columns. Replacing missing values (imputation) can change your results and should be done carefully. Always document the method you used and consider whether missingness might be informative (e.g., values missing due to sampling or lab issues). 0.15 Data Reshaping with {tidyr} package: Pivoting Soil datasets are often stored in different formats depending on how they were collected or produced. The {tidyr} package helps you reshape data—changing its layout without changing the information. This is especially useful when preparing data for plotting, reporting, modeling, or joining with other tables. 0.15.0.1 Wide vs Long Data Format Wide format: each variable has its own column (common in spreadsheets and summary tables). Long format: values are stored in a single column, with one or more columns describing what the values represent (common for tidy workflows and ggplot2()). 0.15.0.2 pivot_longer(): Wide to Long Use pivot_longer() to turn several measurement columns (e.g., pH, carbon, nitrogen) into a tidy key–value structure. library(dplyr) # for piping %&gt;% ## ## Attaching package: &#39;dplyr&#39; ## The following objects are masked from &#39;package:stats&#39;: ## ## filter, lag ## The following objects are masked from &#39;package:base&#39;: ## ## intersect, setdiff, setequal, union library(tidyr) # for pivoting # Wide format data wide_soil &lt;- data.frame( site = c(&quot;A&quot;, &quot;B&quot;, &quot;C&quot;), ecosystem = c(&quot;Forest&quot;, &quot;Grassland&quot;, &quot;Urban&quot;), pH = c(6.2, 7.1, 5.8), carbon = c(3.2, 2.1, 1.5), nitrogen = c(0.25, 0.18, 0.12) ) print(wide_soil) ## site ecosystem pH carbon nitrogen ## 1 A Forest 6.2 3.2 0.25 ## 2 B Grassland 7.1 2.1 0.18 ## 3 C Urban 5.8 1.5 0.12 # Convert to long format long_soil &lt;- wide_soil %&gt;% pivot_longer( cols = c(pH, carbon, nitrogen), # Columns to pivot names_to = &quot;measurement_type&quot;, # Name for the variable column values_to = &quot;value&quot; # Name for the values column ) long_soil ## # A tibble: 9 × 4 ## site ecosystem measurement_type value ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 A Forest pH 6.2 ## 2 A Forest carbon 3.2 ## 3 A Forest nitrogen 0.25 ## 4 B Grassland pH 7.1 ## 5 B Grassland carbon 2.1 ## 6 B Grassland nitrogen 0.18 ## 7 C Urban pH 5.8 ## 8 C Urban carbon 1.5 ## 9 C Urban nitrogen 0.12 Many tidyverse workflows (especially with {ggplot2}) work best with long data. Functions such as slab() (from the Algorithms for Quantitative Pedology package -{aqp}) often return horizon or slice summaries in a long format (e.g., one row per profile × depth-slice). If you need a “one row per horizon” table for reporting or modeling, you may need to reshape that output to wide using pivot_wider(). In many relational databases such as PostgreSQL, measurements are often stored in a long (tidy) format, where each row represents one observation and additional columns describe what was measured (e.g., property, method, unit) and the result (e.g., value). This structure is flexible: you can store many different variables in a single table and add new measurement types over time without changing the table schema. This is also the approach used in the GloSIS relational database: soil analytical results are typically stored as one record per sample/observation × property, rather than as many property columns in a single wide table. For this reason, reshaping data between wide (common in spreadsheets) and long (common in databases) formats is a frequent step when preparing data for insertion into GloSIS or extracting data for analysis. 0.15.0.3 pivot_wider(): Long to Wide Use pivot_wider() to spread a variable column back into multiple columns. This is helpful when you want a compact table for reporting or when a model expects predictors in separate columns. # Convert back to wide format long_soil %&gt;% pivot_wider( names_from = measurement_type, # Column containing variable names values_from = value # Column containing values ) ## # A tibble: 3 × 5 ## site ecosystem pH carbon nitrogen ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 A Forest 6.2 3.2 0.25 ## 2 B Grassland 7.1 2.1 0.18 ## 3 C Urban 5.8 1.5 0.12 0.15.0.4 Advanced Pivoting: Multiple Measurements per Site (Replicates) In real datasets, you may have repeated measurements (e.g., replicate samples). In that case, keep replicate identifiers as separate columns and pivot the measurement names into columns. # More complex examples with multiple measurements per site field_data &lt;- data.frame( site = rep(c(&quot;Forest&quot;, &quot;Grassland&quot;), each = 6), measurement = rep(c(&quot;pH&quot;, &quot;carbon&quot;, &quot;nitrogen&quot;), 4), replicate = rep(c(&quot;R1&quot;, &quot;R2&quot;), 6), value = c(6.2, 6.1, 3.2, 3.0, 0.25, 0.23, 7.1, 7.0, 2.1, 2.3, 0.18, 0.19) ) field_data ## site measurement replicate value ## 1 Forest pH R1 6.20 ## 2 Forest carbon R2 6.10 ## 3 Forest nitrogen R1 3.20 ## 4 Forest pH R2 3.00 ## 5 Forest carbon R1 0.25 ## 6 Forest nitrogen R2 0.23 ## 7 Grassland pH R1 7.10 ## 8 Grassland carbon R2 7.00 ## 9 Grassland nitrogen R1 2.10 ## 10 Grassland pH R2 2.30 ## 11 Grassland carbon R1 0.18 ## 12 Grassland nitrogen R2 0.19 # Pivot to have measurements as columns print(&quot;Pivoted data:&quot;) ## [1] &quot;Pivoted data:&quot; field_data %&gt;% pivot_wider( names_from = measurement, values_from = value ) ## # A tibble: 4 × 5 ## site replicate pH carbon nitrogen ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Forest R1 6.2 0.25 3.2 ## 2 Forest R2 3 6.1 0.23 ## 3 Grassland R1 7.1 0.18 2.1 ## 4 Grassland R2 2.3 7 0.19 If your long dataset contains more than one value for the same combination of identifiers (e.g., site + replicate + measurement), pivot_wider() may create list-columns or produce an error. In those cases, summarize duplicates first, or use values_fn, for example: pivot_wider(..., values_fn = mean) 0.16 Handling Below Detection Limit Data In soil laboratory datasets, some results are reported as below the detection (or reporting) limit, for example &lt;0.05. When imported into R, these values often cause the entire column to be read as character. A common practical approach for basic summaries is to convert these entries to a numeric value equal to half the detection limit (DL/2), while keeping the original raw values for traceability. Example: Converting “&lt;DL” to DL/2 soil_lab &lt;- data.frame( plot_id = c(&quot;P001&quot;, &quot;P002&quot;, &quot;P003&quot;, &quot;P004&quot;), no3_mgkg_raw = c(&quot;0.12&quot;, &quot;&lt;0.05&quot;, &quot;0.31&quot;, &quot;&lt;0.05&quot;), stringsAsFactors = FALSE ) soil_lab soil_lab &lt;- soil_lab %&gt;% mutate( censored = str_detect(no3_mgkg_raw, &quot;^\\\\s*&lt;&quot;), dl = if_else(censored, as.numeric(str_remove(no3_mgkg_raw, &quot;^\\\\s*&lt;\\\\s*&quot;)), NA_real_), no3_mgkg = if_else(censored, dl / 2, as.numeric(no3_mgkg_raw)) ) soil_lab Caution: Replacing BDL values with DL/2 is a simple convention, but it can bias results when many values are censored. Always report the detection limit and the percentage of BDL values. Other approaches for BDL data: DL (or 0) substitution: simple sensitivity checks (compare results using 0, DL/2, and DL). Censored-data methods (recommended for inference): Kaplan–Meier (KM) (e.g., with the {NADA} package): estimates summary statistics without assuming substituted values by treating BDL results as left-censored. *ROS (Regression on Order Statistics)** (e.g., with the {NADA} package): commonly used for environmental data and often a good default when there are multiple detection limits. Censored regression (e.g., Tobit): useful for modeling relationships while accounting for censoring. 0.17 Working with Soil Data Soil data analysis usually starts with importing data, making sure it was read correctly, and then doing a few basic cleaning and selection steps before any modeling or mapping. In this section, you will learn how to set your R environment (manage file paths and your working directory), load the packages you need, import soil datasets from CSV and Excel files, and explore and manipulate the imported data to prepare it for further analyses. 0.17.1 Setting Working Directory The working directory is the folder where R looks for input files (by default) and where it saves output files unless you specify a different location. Any relative file path you use (e.g., \"data/soil_data.csv\") is interpreted relative to the working directory. It is good practice to set the working directory at the beginning of a script. You can set the working directory manually with setwd(), which takes the target folder path as a character string. RStudio also provides a menu option: Session → Set Working Directory. You can always check the current working directory with getwd(), which returns the path of the folder where R is currently operating. # Check current working directory getwd() # Set new working directory setwd(&quot;C:/Users/YourName/Documents/YourSoilProject&quot;) # On Mac/Linux setwd(&quot;/Users/YourName/Documents/YourSoilProject&quot;) If you are using RStudio, an option is to set the working directory to the location of the active script: # Set working directory to script location (recommended) setwd(dirname(rstudioapi::getActiveDocumentContext()$path)) Best practice: Use an RStudio Project (.Rproj) to avoid hard-coded paths. File → New Project creates an .Rproj file. Keep your data inside the project folder and use relative paths (e.g., \"data/KSSL_data.csv\"). If you prefer, you can also set the working directory to the active script location: setwd(dirname(rstudioapi::getActiveDocumentContext()$path)) If you are not using RStudio (or rstudioapi is not installed), the script-location method will not work. In that case, use an RStudio Project or set the working directory manually. 0.17.2 Load packages library(readxl) # Read Excel files library(tidyverse) # Data manipulation and visualization 0.17.3 Importing Data from Files Most soil science projects use data from external sources (field surveys, lab results, sensors, or remote sensing). These datasets are commonly stored as CSV or Excel files, but they can also come from databases, spatial formats (e.g., GeoPackages/shapefiles), or web APIs. R provides tools to import, clean, and analyze these data. In this tutorial, we focus on CSV (.csv) and Excel (.xlsx) because they are the most widely used formats in soil science workflows. 0.17.3.1 Reading CSV Files CSV (comma-separated values) and plain text (.txt) files are widely used for exchanging tabular data. In base R, you can import them with read.csv() or the more general read.table() (both from {utils}). These functions assumes the separator is a comma (,) and by default use header = TRUE, meaning the first row is treated as column names. The output is a data.frame. For larger files, you can use read_csv(), from {readr} which is usually faster and often does a better job of parsing column types. # Read CSV file soil_data &lt;- read.csv(&quot;path/to/soil_data.csv&quot;) # View structure str(soil_data) # View first rows head(soil_data) # If CSV uses different separator (semicolon, tab) soil_data &lt;- read.csv(&quot;path/to/file.csv&quot;, sep = &quot;;&quot;) soil_data &lt;- read.delim(&quot;path/to/file.txt&quot;, sep = &quot;\\t&quot;) If you are using an RStudio Project and the dataset is stored inside your project, you can use a relative path. For example, for the Kansas soil profiles dataset in this project: # Read CSV file soil_data &lt;- read_csv(&quot;01_data/module1/KSSL_data.csv&quot;) # View structure str(soil_data) # View first rows head(soil_data) When possible, prefer project-relative paths like \"01_data/module1/KSSL_data.csv\". They are easier to read and less likely to break if you reorganize folders. 0.17.3.2 Reading Excel Files To read Excel files, use read_excel() from the {readxl} package: # Read Excel file soil_data &lt;- read_excel(&quot;01_data/module1/KSSL_data.xlsx&quot;, sheet = 1) # Or read a specific sheet by name soil_data &lt;- read_excel(&quot;01_data/module1/KSSL_data.xlsx&quot;, sheet = &quot;SoilData&quot;) 0.17.4 Exploring the Data After importing, always confirm the structure, column names, and data types before analysis. R provides several built-in functions to help you inspect the structure, preview the values, and summarize the dataset quickly. str(soil_data) # Structure + column types summary(soil_data) # Quick summaries for each column names(soil_data) # Column names head(soil_data) # First rows tail(soil_data) # Last rows View(soil_data) # opens a spreadsheet-style viewer in RStudio Caution: Imported columns are sometimes read with the wrong type (e.g., numbers stored as text). The str() function is especially useful when you’re not sure how R is interpreting your variables — for example, whether a column is being read as text (character) or as a categorical variable (factor). Check types with str() and convert when needed using as.numeric(), as.character(), or as.Date(). 0.18 References Brus, D.J.(2022). Spatial Sampling with R, CRC Press, New York. https://dickbrus.github.io/SpatialSamplingwithR/ Grolemund, G., &amp; Wickham, H. (2017). R for Data Science. O’Reilly Media. Hengl, T., &amp; MacMillan, R. A. (2019). Predictive Soil Mapping with R. Zenodo. https://doi.org/10.5281/zenodo.2561035 Pebesma, E.; Bivand, R. (2023). Spatial Data Science: With Applications in R (1st ed.). 314 pages. Chapman and Hall/CRC, Boca Raton. https://doi.org/10.1201/9780429459016 Soil Survey Staff (2025). Statistics for Soil Survey. https://ncss-tech.github.io/stats_for_soil_example/ Spatial Data Science with R and ‘terra’ (2025). https://rspatial.org/ "],["data-preparation-for-digital-soil-mapping-in-r.html", "Chapter 1 Data preparation for Digital Soil Mapping in R 1.1 Introduction 1.2 Loading and exploring raw soil data 1.3 Preparing site data for analysis 1.4 Coordinate validation and correction 1.5 Soil depth validation and correction 1.6 Preparing lab data: harmonization and validation 1.7 Resolving duplicated data in soil profiles 1.8 Harmonizing data for DSM 1.9 Preparing data for spectroscopy analyses 1.10 Summary of exported files References", " Chapter 1 Data preparation for Digital Soil Mapping in R 1.1 Introduction Data preparation is one of the most critical and time-consuming steps in Digital Soil Mapping (DSM). Raw soil laboratory data typically contains numerous inconsistencies, errors, and redundancies that must be systematically identified and resolved before use in modeling. Poor data quality at this stage directly translates to poor prediction models and unreliable soil property maps. This section demonstrates systematic approaches to soil data validation, cleaning, and harmonization using the Kansas KSSL dataset as a practical example. The training dataset as well as a tutorial on how to download it can be found in the following section How to use this book of this Technical Manual. The goal is to transform raw soil measurements into clean, consistent data ready for Digital Soil Mapping analysis and modeling. Raw soil laboratory data commonly contains the following types of quality issues: Geographic errors: Missing or out-of-bounds coordinates, coordinate swaps, and unit inconsistencies Depth inconsistencies: Missing soil depths, zero-thickness horizons ((bottom = top), invalid depth logic (bottom &lt; top), overlapping intervals, and duplicate measurements Laboratory data problems: Missing values, out-of-range soil properties, texture validation failures (clay+silt+sand \\(\\neq\\) 100%) Duplicate profiles: Multiple measurement sequences at the same location with conflicting depth intervals Logical inconsistencies: Values that are physically or chemically impossible given soil science constraints The Kansas KSSL dataset used throughout this section provides real-world examples of these issues, allowing you to practice identifying and correcting them systematically using reproducible R workflows. Considerations Every soil dataset is unique and may have specific data-quality issues. Proper identification of quality problems requires prior understanding of the dataset’s structure. When using a different database, the code shown here must be adapted to your actual database column names and data types. 1.2 Loading and exploring raw soil data Before performing any data cleaning or validation, you must first understand the structure and content of your raw data. This exploratory phase reveals data types, identifies potential problems, and informs your validation strategy. 1.2.1 Basic data loading and structure # Load libraries library(tidyverse) # Data manipulation and visualization library(readxl) # Read Excel files library(knitr) # For formatted tables # Read Excel file containing raw soil data raw_data &lt;- read_excel(&quot;../../01_data/module1/KSSL_data.xlsx&quot;, sheet = 1) # Define the folder to store the results of the exercise output_dir &lt;-&quot;../../03_outputs/module1/&quot; # Create the output directory if not existing if (!file.exists(output_dir)){ # create a new sub directory inside the main path dir.create(output_dir) } str(raw_data) # Examine the structure of the data head(raw_data, 10) # Show the first 10 rows summary(raw_data[,1:25]) # Summarize the data in 25 first columns This dataset contains four types of information: site data (location and depth), laboratory data (measured soil properties), and spectral data (mid-infrared, MIR, reflectance). In addition, each row includes identifiers for the sample (smp_id), and other keys related to the spectral analyses (join_key, scan_path_name, file_name). In many cases, soil datasets do not include spectral data. Understanding these components will help you organize the cleaning process systematically. 1.2.2 Data components and organization Typically, a raw soil dataset includes several types of information: Site information defines where and at what depth a soil sample was collected: Geographic coordinates (longitude and latitude, often in WGS84 or X, Y in Cartesian projected systems) Unique identifiers (profile ID, horizon ID, sample ID) Depth boundaries (top and bottom depth of the analyzed horizons in each soil profile) Laboratory data contains measured soil properties: Physical properties (texture: clay, silt, sand percentages) Chemical properties (pH, organic carbon, cation exchange capacity) Other analyzed parameters (salinity, nutrients, etc.) Spectral data (optional) includes: Spectral reflectance values at multiple wavelengths (MIR, VIS-NIR) Used for predicting soil properties through spectroscopy 1.3 Preparing site data for analysis Data cleaning involves extracting relevant information from raw data, standardizing column names, assigning unique identifiers, and validating data quality. To track changes through this process, we add a unique row identifier to the raw dataset before making any modifications. We operationally define a site as the set of horizons or layers that share the same geographic location (i.e., identical coordinates at the chosen precision). The site has information on location (lat and long coordinates), depth (upper and lower depth boundaries) and metadata-related information (horizon ID). 1.3.1 Adding a unique row identifier During data cleaning, some records may be removed, merged, or modified. Good practice is to add a unique sequential rowID to the raw (unchanged) dataset so you can track each original record throughout the entire workflow. This identifier allows you to trace any result back to its source data and understand which raw records were retained or excluded. # Add unique row identifier to track individual records through processing raw_data &lt;- raw_data %&gt;% mutate(rowID = row_number(), .before = 1) The rowID column preserves the link between processed data and raw data, enabling full transparency and reproducibility in your cleaning workflow. 1.3.2 Extracting and standardizing column names for sites Raw soil data often uses inconsistent column naming conventions that vary between data sources, laboratories, and surveys. Standardizing these names prevents errors and makes your code more readable and reusable across different projects The R object site will store the location, depth, and metadata from the full set of observations, including all sites and their associated horizons or layers in the database. # Select only the columns needed for site data preparation site &lt;- raw_data %&gt;% select( rowID, Long_Site.x, # Raw column name for longitude Lat_Site.x, # Raw column name for latitude smp_id, # Sample/horizon identifier Top_depth_cm.x, # Top depth in centimeters Bottom_depth_cm.x # Bottom depth in centimeters ) # Rename columns to standard, consistent names site &lt;- site %&gt;% rename( lon = Long_Site.x, lat = Lat_Site.x, HorID = smp_id, top = Top_depth_cm.x, bottom = Bottom_depth_cm.x ) 1.3.3 Creating unique profile identifiers Each unique geographic location represents a single soil profile. A profile may contain multiple horizons or layers sampled at different depths. We create a unique identifier ProfID for each location based on coordinates so that all horizons from the same location can be grouped together in the same profile. site &lt;- site %&gt;% # Group all horizons at the same location group_by(lon, lat) %&gt;% # Assign sequential ID to each unique location (cur_group_id() returns group number) mutate(ProfID = cur_group_id()) %&gt;% ungroup() %&gt;% # Format as standardized IDs: PROF0001, PROF0002, etc. with 4 digit resolution mutate(ProfID = sprintf(&quot;PROF%04d&quot;, ProfID)) # Reorder columns for clarity site &lt;- site %&gt;% select(rowID, ProfID, HorID, lon, lat, top, bottom) After this step, horizons from the same location share a common ProfID, while those from different locations receive distinct ProfID values. When unique profile IDs are not available in the raw data, this method reconstructs profile identity using the geographic position of the samples. 1.3.4 Remove exact duplicate site records Multiple site records may correspond to identical observations (i.e., the same location and depth). In this dataset, spectroscopic measurements were performed four times per sample, resulting in duplicate entries. Remove duplicates and retain a single record to prevent redundancy and double counting in subsequent analyses. # Remove exact duplicate rows site &lt;- site %&gt;% distinct(across(-rowID), .keep_all = TRUE) Standard column naming convention ensures consistency across all your code and projects: ProfID: Profile identifier HorID: Horizon or sample identifier lon, lat: Geographic coordinates (decimal degrees, WGS84) top,bottom: Depth boundaries in centimeters Adjust ProfID for locations with multiple profile descriptions Horizons or layers measured at different times or for different purposes may occur at the same location. If the ProfID is based only on spatial position, these observations may share the same identifier. Identify these profiles and assign a unique ProfID to each one (see the procedure in Section 6 of this chapter). Proper identification of unique profiles is necessary to ensure consistent data management and reliable Digital Soil Mapping results. 1.4 Coordinate validation and correction Spatial coordinates form the basis of all spatial analyses. Invalid or inaccurate coordinates result in erroneous maps and unreliable spatial predictions. Systematic validation is therefore essential to identify and correct coordinate errors before they propagate through the analytical workflow. Coordinates can be expressed in different coordinate reference systems (CRS): Geographic coordinates (longitude and latitude in decimal degrees, typically WGS84) Projected coordinates (X and Y in a projected system, such as UTM or a local projection) Any dataset should explicitly document which CRS is used. For Digital Soil Mapping workflows, it is recommended to standardize all coordinates to WGS84 (EPSG:4326) geographic coordinates (longitude and latitude) to ensure interoperability and consistency across projects. 1.4.1 Check 1: Missing coordinates Records lacking valid spatial coordinates cannot be georeferenced and must be excluded from spatial analyses. Check for missing or null values in both coordinate dimensions (e.g., latitude/longitude or projected X/Y) before proceeding. # Remove records with missing coordinates site &lt;- site %&gt;% dplyr::filter(!is.na(lon) &amp; !is.na(lat)) 1.4.2 Check 2: Valid coordinate ranges For geographic coordinates (longitude and latitude expressed in decimal degrees), values must fall within the following absolute ranges: Longitude: -180° to +180° (negative = West, positive = East) Latitude: -90° to +90° (negative = South, positive = North) Values outside these limits indicate invalid or incorrectly formatted coordinates. The following validation routine applies only to geographic coordinates stored in decimal degrees. # Keep only rows with valid lon/lat geographic coordinates inside valid ranges site &lt;- site %&gt;% dplyr::filter( lon &gt;= -180, lon &lt;= 180, lat &gt;= -90, lat &lt;= 90 ) If projected coordinates are used, valid ranges depend on the specific projection and the spatial extent of the study area. In such cases, define the expected bounds for the coordinate reference system (CRS) before performing range checks. Coordinate system considerations: Geographic coordinates (longitude/latitude in decimal degrees) have fixed global bounds (−180° to +180°, −90° to +90°) that must not be exceeded. Projected coordinates (e.g., X/Y in meters, such as UTM) require CRS-specific limits. To minimize CRS-related errors, standardize coordinates to WGS84 (EPSG:4326) early in the workflow. Document the original CRS and any transformations applied to ensure reproducibility and transparency. 1.5 Soil depth validation and correction Depth intervals define the soil layer represented by each observation. Missing or inconsistent depth information prevents harmonization across profiles and can lead to incorrect interpretation of depth-dependent patterns and model outputs. This section describes a set of quality-control checks to validate and, where possible, correct depth interval data. The checks are applied sequentially, removing structurally invalid records before attempting logical corrections. Understanding depth conventions Soil horizons (or layers) are represented as depth intervals with explicit upper and lower boundaries, typically recorded as top depth and bottom depth (e.g., “0–15 cm” indicates measurements from 0 to 15 cm below the soil surface). The following rules apply: Depth boundaries define a continuous interval in the soil profile. Bottom depth must be strictly greater than top depth. Depths are measured downward from the soil surface (0 cm). Horizon thickness is computed as bottom − top and must be positive. 1.5.1 Check 1: Missing depth boundaries Records with missing top or bottom depths cannot be assigned to a valid interval and should be excluded unless depth information can be recovered from the source. # Keep records where `top` or `bottom` are not NA site &lt;- site %&gt;% dplyr::filter(!is.na(top) &amp; !is.na(bottom)) 1.5.2 Check 2: Negative depth values Depth values must be non-negative. Negative depths indicate invalid input and should be removed unless the error can be corrected using metadata or original field notes. # Keep records where `top` or `bottom` are positive site &lt;- site %&gt;% filter(!(top &lt; 0 | bottom &lt; 0)) 1.5.3 Check 3: Zero-thickness intervals Horizons with top = bottom have zero thickness and do not represent a measurable soil layer. These records should be excluded from analysis unless the error can be corrected. # Remove zero-thickness horizons site &lt;- site %&gt;% filter(!(bottom - top == 0)) 1.5.4 Check 4: Invalid depth logic For a valid depth interval, bottom &gt; top must hold. Violations typically indicate data entry errors (e.g., swapped boundaries or incorrect units). # Remove invalid depth logic site &lt;- site %&gt;% filter(bottom &gt; top) 1.5.5 Check 5: Profiles without a surface horizon (top &gt; 0) Each profile should represent the complete soil column starting at surface. This is essential for depth harmonization in a later step for Digital Soil Mapping. ## Keep only profiles that start at the surface (min top == 0) site &lt;- site %&gt;% dplyr::group_by(ProfID) %&gt;% dplyr::filter(!is.na(top) &amp; min(top, na.rm = TRUE) == 0) %&gt;% dplyr::ungroup() Further checks for depth integrity required After applying the basic depth validation steps above, some profiles may still contain duplicated or overlapping horizons, or multiple depth sequences for the same ProfID. These situations often arise from repeated sampling or laboratory analyses conducted at different times. Do not remove these records prematurely. Duplicate horizons may contain complementary analytical measurements that are not consistently available across all repetitions. Eliminating them at this stage may result in unintended data loss. Depth-sequence conflicts should therefore be resolved only after laboratory data have been cleaned, validated, and consolidated at the horizon level. Once analytical completeness has been ensured, apply the dedicated profile-level procedure described in ‘Detecting and resolving duplicate soil profiles’ to identify and retain a single representative depth sequence per profile. This staged approach prevents loss of valid measurements and ensures consistent profile harmonization. 1.6 Preparing lab data: harmonization and validation Laboratory analyses provide the soil property measurements used as soil property inputs for Digital Soil Mapping. Laboratory data must be quality-controlled to ensure completeness, plausible value ranges, and internal consistency (e.g., particle-size fractions summing to ~100%). This section describes standard extraction, harmonization, and validation checks. Laboratory analyses provide the soil property measurements used as inputs for Digital Soil Mapping. Prior to modelling, laboratory data must be quality-controlled to ensure: completeness of critical variables, numeric data types for analytical parameters, plausibility of measured values based on feasible analytical thresholds, and traceable handling of potential errors (flagging, correction, or exclusion). 1.6.1 Prepare and standardize laboratory columns In this step, you will extract horizon-level laboratory results from the raw dataset, convert relevant fields to numeric, and retain only the records associated with the profiles preserved in the cleaned site dataset. Finally, you will merge the laboratory data back into the site dataset using rowID as the unique record identifier. # Extract laboratory columns with standardized names lab &lt;- raw_data %&gt;% select( rowID, SOC, Carbon_Total, # Soil Organic Carbon and Total Carbon (%) Bulk.Density_1_3.BAR, Bulk.Density_ovendry, # Bulk density at 1.3 bar and oven dry (g/cm³) Sand, Silt, Clay, # Texture (%) pH, # pH H2O CEC, # CEC in cmol(+)/kg Nitrogen_Total, # Total nitrogen (%), Phosphorus_Mehlich3, Phosphorus_Olsen, Potassium, # Available P (mg/kg), Exchangeable K (cmol(+)/kg) Calcium_Carbonate_equivalent # CaCO3 equivalent (%) ) # Ensure numeric type for all analytical parameters (prevents issues if stored as text) lab &lt;- lab %&gt;% mutate(across(-rowID, as.numeric)) # Keep only lab records that are present in the cleaned site dataset lab &lt;- lab %&gt;% filter(rowID %in% site$rowID) # Join both site and lab data by the common identifier &#39;rowID&#39; site_lab &lt;- site %&gt;% left_join(lab, by = &quot;rowID&quot;) After preparing the laboratory dataset, the analytical results are validated through three quality-control checks: Check 1: Identify out-of-bounds values — flag measurements that fall outside plausible or admissible ranges. Check 2: Texture validation — verify that sand, silt, and clay values are internally consistent (e.g., within expected limits and summing appropriately when expressed as percentages). Check 3: Correction of out-of-bounds laboratory values — apply a proper correction strategy (targeted fixes where justified, or replacing suspect values with NA). 1.6.2 Check 1: Check each property against feasible analytical thresholds Soil properties have physically and analytically plausible bounds. Values outside these bounds typically indicate measurement issues, unit inconsistencies, or transcription/data entry errors. Thresholds should be defined a priori and documented (e.g., based on existing soil datasets or peer-reviewed literature), then adjusted as needed for the target region and soil types. 1.6.2.1 Load property thresholds Store thresholds in a configuration file (e.g., property_thresholds.csv) to ensure transparency and reproducibility. # Load valid ranges for soil properties # NOTE: These thresholds are based on global soil datasets and literature. # Adjust for your specific region and soil types. property_thresholds &lt;- read_csv(&quot;property_thresholds.csv&quot;) # Display thresholds print(property_thresholds) 1.6.2.2 Identify out-of-bounds values Each soil analytical property is evaluated against its min/max thresholds. Out-of-bounds values are compiled into a structured issue log (out_of_bounds_issues) to support inspection, reporting, and correction. # Identify out-of-bounds values out_of_bounds_issues &lt;- list() for (i in seq_len(nrow(property_thresholds))) { prop &lt;- property_thresholds$property[i] prop_desc &lt;- property_thresholds$description[i] min_val &lt;- property_thresholds$min_valid[i] max_val &lt;- property_thresholds$max_valid[i] # Check property exists in the dataset if (prop %in% names(site_lab)) { x &lt;- site_lab[[prop]] # Detect out-of-bounds: non-missing values outside [min_val, max_val] idx &lt;- which(!is.na(x) &amp; (x &lt; min_val | x &gt; max_val)) if (length(idx) &gt; 0) { out_of_bounds_issues[[prop]] &lt;- tibble( rowID = site_lab$rowID[idx], property = prop, description = prop_desc, value = x[idx], min_valid = min_val, max_valid = max_val, issue = ifelse( x[idx] &lt; min_val, paste0(&quot;Below minimum: &quot;, round(x[idx], 2), &quot; &lt; &quot;, min_val), paste0(&quot;Above maximum: &quot;, round(x[idx], 2), &quot; &gt; &quot;, max_val) ) ) } } } # Remove temporary objects rm(i,idx,max_val,min_val, prop,prop_desc,x) 1.6.2.3 Reporting out-of-bounds for properties and audit trail Generate a summary by property, identify records with issues (often indicative of systematic errors), and export a full QC report for review and documentation. # Report out-of-bounds if present if (length(out_of_bounds_issues) &gt; 0) { all_issues &lt;- bind_rows(out_of_bounds_issues) cat(&quot;\\n Out-of-bounds properties found\\n&quot;) # Summary by property issue_summary &lt;- all_issues %&gt;% group_by(property, description) %&gt;% summarise( count = n(), min_value_found = min(value, na.rm = TRUE), max_value_found = max(value, na.rm = TRUE), min_valid = first(min_valid), max_valid = first(max_valid), .groups = &quot;drop&quot; ) %&gt;% arrange(desc(count)) cat(&quot;Issues by property:\\n&quot;) print(issue_summary) # Rows with multiple issues rows_with_multiple_issues &lt;- all_issues %&gt;% group_by(rowID) %&gt;% summarise( n_issues = n(), properties = paste(property, collapse = &quot;, &quot;), .groups = &quot;drop&quot; ) %&gt;% filter(n_issues &gt; 1) %&gt;% arrange(desc(n_issues)) if (nrow(rows_with_multiple_issues) &gt; 0) { cat(&quot;\\n Records with MULTIPLE property issues:\\n&quot;) print(head(rows_with_multiple_issues, 10)) cat(&quot;\\nThese records likely have data entry errors and should be reviewed.\\n&quot;) } # Export QC report write_xlsx( list( Summary = issue_summary, Issues_by_record = rows_with_multiple_issues, All_issues = all_issues ), paste0(output_dir,&quot;soil_property_validation_report.xlsx&quot;) ) cat(&quot;\\n Detailed report saved to: soil_property_validation_report.xlsx\\n&quot;) rm(all_issues, issue_summary, rows_with_multiple_issues) } else { cat(&quot;\\n All soil properties within valid ranges!\\n&quot;) } 1.6.3 Check 2: Texture validation Particle-size fractions should sum to approximately 100%. This check is used to flag potential inconsistencies (rounding, unit conversion issues, or data errors). The following code just flag inconsistencies in Particle-size fractions. Values failing this check should be reviewed rather than automatically removed. # Print rows with texture validation incosistencies texture_problems &lt;- site_lab %&gt;% mutate( texture_sum = Clay + Silt + Sand, texture_valid = abs(texture_sum - 100) &lt; 2 ) texture_problems &lt;- texture_problems %&gt;% filter(!texture_valid) if (nrow(texture_problems) &gt; 0) { cat(&quot; Found&quot;, nrow(texture_problems), &quot;records with invalid texture sums\\n\\n&quot;) print(texture_problems %&gt;% select(rowID, ProfID, Clay, Silt, Sand, texture_sum)) # Flag for review (do not automatically remove) } 1.6.4 Check 3: Correction of out-of-bounds laboratory values Out-of-bounds values identified during validation should be handled systematically and transparently. Corrections must follow clearly defined rules to avoid introducing subjective bias or undocumented changes. Two complementary approaches are recommended: Option 1 (preferred): targeted correction, when the error mechanism is known and the true value can be reasonably inferred (e.g., sign errors or unit-scaling mistakes). Option 2: replacement with NA, when the true value cannot be reliably reconstructed. This preserves the observation while preventing propagation of erroneous measurements into subsequent analyses. Option 1 requires a clear understanding of the relevant properties and thresholds to correctly identify errors, which can then be fixed with dedicated code. Option 2 is a more drastic approach, as it will automatically replace all potential mistakes with NA. This can result in a critical decrease in records for some measured properties. Choosing between these options is a decision for the data manager. Handle both with care. 1.6.4.1 Option 1: Targeted corrections (when error mechanism is known) Apply deterministic corrections only when the source of error is clearly understood and scientifically justified. The summary of the critical values provides information on the nature of each issue. for (property in names(out_of_bounds_issues)){ cat(&quot;Total errors in&quot;,property, &quot;:&quot;,n_distinct(out_of_bounds_issues[[property]]$rowID), &quot;\\n&quot;) print(summary(data.frame(out_of_bounds_issues[property])[4])) } In the KSSL dataset provided, SOC is negative in 43 rows while Phosphorus_Mehlich3 has wrong values in 1 row. Example corrections shown below assume: - Negative SOC values arise from sign errors, and - Extremely high Phosphorus_Mehlich3 values arise from unit scaling (e.g., ppb recorded instead of mg/kg). # Correction: Negative SOC values idx &lt;- !is.na(site_lab$SOC) &amp; site_lab$SOC &lt; 0 if (any(idx)) site_lab$SOC[idx] &lt;- abs(site_lab$SOC[idx]) # Remove temporary objects rm(idx) # Correction: Phosphorus Mehlich 3 &gt; 2000 mg/kg (likely 1000× error - ppb instead of ppm) idx &lt;- !is.na(site_lab$Phosphorus_Mehlich3) &amp; site_lab$Phosphorus_Mehlich3 &gt; 2000 n_idx &lt;- sum(idx) if (n_idx &gt; 0) site_lab$Phosphorus_Mehlich3[idx] &lt;- site_lab$Phosphorus_Mehlich3[idx] / 1000 # Remove temporary objects rm(idx, n_idx) 1.6.4.2 Option 2: Replace out-of-bounds values with NA When values cannot be corrected with confidence, replace only the problematic measurements with NA while retaining the rest of the record. # Loop through each property in the out_of_bounds_issues list for (property in names(out_of_bounds_issues)) { # Get the rowIDs with issues for this property rowIDs_with_issues &lt;- unique(out_of_bounds_issues[[property]]$rowID) # Change the values of the property in those rows to NA site_lab &lt;- site_lab %&gt;% dplyr::mutate( &quot;{property}&quot; := dplyr::if_else(rowID %in% rowIDs_with_issues, as.numeric(NA), .data[[property]]) ) } 1.7 Resolving duplicated data in soil profiles Duplicate or repeated soil profile descriptions may occur when the same location is sampled or analysed multiple times. These situations can produce multiple horizon sequences under the same ProfID, resulting in inconsistent depth intervals, overlapping layers, or conflicting analytical values. Such inconsistencies must be resolved before depth harmonization and Digital Soil Mapping, as it requires one coherent and unique vertical profile per location. The objective of this section is to: -Detect duplicated or overlapping depth descriptions -Merge replicated analytical measurements when appropriate Separate or select among competing depth sequences -Retain only complete and internally consistent profiles 1.7.1 Why duplicates occur Duplicates typically arise from: Re-sampling of the same location in later campaigns (temporal duplicates) Multiple laboratory analyses of the same sample (analytical replicates) Re-use of identifiers across merged surveys These situations may produce: Multiple horizon sequences per ProfID Overlapping or conflicting depth intervals Repeated horizons with different values Ambiguity about which sequence should be used for modelling 1.7.2 Types of duplicate situations It is important to distinguish between two fundamentally different cases: - Case A — Replicated analyses of the same horizons (same depths) - Identical top–bottom intervals in the same profile - Multiple measurements of the same soil layer - Depth integrity is preserved - Typical in monitoring databases Action: If used for monitoring, use the data for the corresponding period; otherwise, merge measurements (e.g., average values) - Case B — Multiple depth sequences (different depths) - Different top–bottom structures within the same ProfID - Represents independent profile descriptions Action: select one representative sequence 1.7.3 Check 1: Detect potential horizon duplicates within profiles Profile identifiers ProfID have been previously constructed by grouping horizons with identical coordinates into the same profile. However, if multiple profile descriptions have been created at the same location, the ProfID will be the same. The objective now is to flag ProfID values that appear to contain more than one distinct depth sequence (i.e., multiple sets of horizon boundaries). ### Detect potential horizon duplicates within profiles profile_analysis &lt;- site_lab %&gt;% group_by(ProfID) %&gt;% summarise( n_horizons = n(), n_unique_tops = n_distinct(top), n_unique_bottoms = n_distinct(bottom), max_depth = max(bottom, na.rm = TRUE), .groups = &quot;drop&quot; ) %&gt;% mutate( # If all horizons have unique top/bottom values, # depths are consistent (no duplicates) consistent = (n_unique_tops == n_horizons &amp; n_unique_bottoms == n_horizons), likely_duplicates = !consistent ) # Find profiles with likely duplicates duplicates &lt;- profile_analysis %&gt;% filter(likely_duplicates) if (nrow(duplicates) &gt; 0) { cat(&quot; Found&quot;, nrow(duplicates), &quot;profiles with likely duplicates measurement sequences\\n\\n&quot;) print(duplicates) } # Select all profiles presenting duplicate horizons duplicates &lt;- site_lab %&gt;% filter(ProfID %in% duplicates$ProfID) 1.7.4 Resolving Profile Duplicates Duplicate handling should follow the order below: 1 → merge duplicated horizons 2 → resolve competing depth sequences 3 → remove incomplete profiles This order prevents premature data loss and preserves maximum analytical information. 1.7.4.1 Check 1: Average duplicated horizons (same depth intervals) When multiple records share identical ProfID, top, and bottom, they represent repeated measurements of the same soil layer. These should be consolidated into a single horizon. Numeric properties are averaged and common identifiers are retained from the first occurrence. # ----- # Correction 1: Summarize property in duplicated horizons my mean # (e.g. PROF0237, PROF0262, PROF0271, PROF0284, PROF0368) # ----- site_lab &lt;- site_lab %&gt;% group_by(ProfID, top, bottom) %&gt;% summarise( # keep identifiers as the first value in each group across(c(rowID, HorID, lon, lat), ~ first(.x)), # compute mean for all other numeric columns (NA-safe) across( where(is.numeric) &amp; !any_of(c(&quot;rowID&quot;,&quot;HorID&quot;,&quot;lon&quot;,&quot;lat&quot;,&quot;top&quot;,&quot;bottom&quot;)), ~ if (all(is.na(.x))) NA_real_ else mean(.x, na.rm = TRUE) ), .groups = &quot;drop&quot; ) %&gt;% select(names(site_lab)) # &lt;- restores original column order Replicates increase measurement reliability. Averaging avoids discarding valid analytical information. 1.7.4.2 Check 2: Resolve ProfID for multiple depth sequences Multiple independent profile descriptions likely exist in a single location. This data is valid and must remain in the dataset, but it has to be differentiated by its ProfID. A chain_horizons function has been created to identify different sequences in horizons within each profile. # Detect ProfID series with different top-bottom depth sequences # 1. For each profile, check if all rows form ONE continuous depth sequence # 2. If YES → Single profile (done) # 3. If NO → Find the consecutive horizons that ARE continuous # 4. If we find blocks with no gaps → Split the series into subprofiles # Create a function to identify sequences of horizons for each profile chain_horizons &lt;- function(top, bottom) { n &lt;- length(top) remaining &lt;- seq_len(n) chain_id &lt;- integer(n) cid &lt;- 1 while(length(remaining) &gt; 0) { # start new chain at smallest top cur &lt;- remaining[which.min(top[remaining])] repeat { chain_id[cur] &lt;- cid remaining &lt;- setdiff(remaining, cur) nxt &lt;- remaining[top[remaining] == bottom[cur]] if(length(nxt) == 0) break cur &lt;- nxt[1] } cid &lt;- cid + 1 } chain_id } site_lab &lt;- site_lab %&gt;% group_by(lon, lat, ProfID) %&gt;% mutate(chain = chain_horizons(top, bottom)) %&gt;% # detect sequences arrange(chain, top, .by_group = TRUE) %&gt;% # sort within each chain mutate( ProfID = paste0(ProfID, &quot;_&quot;, chain) # add a numeric suffix ) %&gt;% ungroup() if (max(site_lab$chain, na.rm = TRUE) &gt; 1) { corrected_profiles &lt;- unique(site_lab$ProfID[site_lab$chain &gt;= 2]) cat(&quot;→ Corrected depth continuity in&quot;, length(corrected_profiles), &quot;profiles\\n&quot;) cat(&quot; Corrected Profiles:&quot;, paste(sub(&quot;_2$&quot;, &quot;&quot;, corrected_profiles), collapse = &quot;, &quot;), &quot;\\n&quot;) } else { cat(&quot;→ No depth continuity corrections were needed\\n&quot;) } # Delete the chain column site_lab &lt;- site_lab %&gt;% select(-chain) # Delete temporary objects rm(corrected_profiles,chain_horizons) 1.7.5 Remove profiles not starting at the surface Profiles whose shallowest horizon does not begin at 0 cm represent mistakes or incomplete descriptions and may bias depth harmonization and DSM modelling. Only profiles whose first horizon begins at the surface (top = 0 cm) should be retained for analyses requiring complete vertical representation. 1.7.5.1 Check 1: Remove profiles with incomplete surface coverage As a result of the previous steps, some horizons may remain without forming a continuous sequence within their profile. The objective here is to retain only those profiles whose shallowest recorded depth starts at 0 cm. site_lab &lt;- site_lab %&gt;% group_by(ProfID) %&gt;% filter(min(top, na.rm = TRUE) == 0) %&gt;% arrange(ProfID, top, bottom, HorID) %&gt;% ungroup() 1.7.6 Result After this procedure, we obtain a clean horizon-level dataset containing validated site, analytical, and metadata information. The dataset contains: One consistent depth sequence per ProfID No duplicated horizons Consolidated analytical measurements Alternative profiles at the same location can coexist This database can be exported to csv and .xlsx files. # Save to CSV output &lt;- paste0(output_dir,&quot;KSSL_cleaned.csv&quot;) write.csv(site_lab, output, row.names = FALSE) # Save to Excel output &lt;- paste0(output_dir,&quot;KSSL_cleaned.xlsx&quot;) write_xlsx(site_lab, output) 1.8 Harmonizing data for DSM As shown in the previous step, the database produced can still present several alternative profiles coexisting at the same location. These profiles present different top-depth valid horizon sequences that must be retained in the soil database. When multiple valid profiles exist at the same coordinates, DSM requires a single profile description at each single location. There are different profile selection criteria based in the data available and/or modeling purpose. Select the option using one of these criteria: : - **Most complete**: Keep sequence with most horizons (more depth detail) - **Best coverage**: Keep sequence extending deepest (most information) - **Best quality** : Keep sequence with fewest missing values - **Monitoring** : Keep sequence analyzed at the period of interest (requires `Date` information) In general, for monitoring activities, profiles can be selected upon a time key (e.g., Date, campaign_id) and treat profiles as separate observations information. This will allow to model temporal trends, or time-specific DSM surfaces. When sampling dates or campaign metadata are unavailable, the most complete (more depth detail) sequence is typically safest. # Create a new object to store DSM harmonized data # Keep most complete profiles at each location to avoid duplicated profiles horizons &lt;- site_lab %&gt;% group_by(lon, lat, ProfID) %&gt;% summarise(n_hz = n_distinct(paste(top, bottom)), .groups = &quot;drop&quot;) %&gt;% group_by(lon, lat) %&gt;% dplyr::slice_max(n_hz, n = 1, with_ties = FALSE) %&gt;% select(lon, lat, ProfID) %&gt;% inner_join(site_lab, by = c(&quot;lon&quot;, &quot;lat&quot;, &quot;ProfID&quot;)) %&gt;% ungroup() DSM requires one profile per location If no Date/campaign metadata exists → keep the most complete profile per location. If Date/campaign exists and the purpose is monitoring → stratify by Date/campaign and keep one profile per location per Date/campaign 1.8.1 Depth standardization Finally, DSM requires analytical data calculated at standardized depth intervals (the same depths for every profile). The standard depths typically used are 0–30 cm (topsoil), 30–60 cm (subsoil), and 60–100 cm (deep subsoil). These represent meaningful soil zones in terms of soil fertility, root penetration, weathering, and soil formation. Since the cleaned dataset contains properties at variable-depth horizons for each profile, depth harmonization is needed. The Algorithms for Quantitative Pedology (aqp) package provides the slab() function for standardizing variable-depth soil data using weighted averaging. library(aqp) # Define standard depth intervals standard_depths &lt;- c(0, 30, 60) # 0-30, 30-60 cm # Select properties to standardize properties_to_standardize &lt;- names(site_lab)[!names(site_lab) %in% c(&quot;rowID&quot;,&quot;ProfID&quot;,&quot;HorID&quot;,&quot;lon&quot;,&quot;lat&quot;,&quot;top&quot;,&quot;bottom&quot;,&quot;texture_sum&quot;,&quot;texture_valid&quot;)] # Prepare data for aqp # Create SoilProfileCollection Object # aqp needs profiles + depth structure for proper interpolation depths(horizons) &lt;- ProfID ~ top + bottom # Add Spatial Information to SoilProfileCollection # Links geographic location to soil profiles initSpatial(horizons, crs = &quot;EPSG:4326&quot;) &lt;- ~ lon + lat # Build the standardization formula fml &lt;- as.formula( paste(&quot;ProfID ~&quot;, paste(properties_to_standardize, collapse = &quot; + &quot;)) ) # Apply slab() to interpolate to standard depths KSSL_standardized &lt;- slab( horizons, fml, slab.structure = standard_depths, # Target standard depths na.rm = TRUE # Ignore NA values in calculations ) The slab() function produces output with: - p.q5: 5th percentile (lower confidence bound) - p.q50: Median estimate (best estimate) - p.q95: 95th percentile (upper confidence bound) The 5th to 95th percentile range provides a 90% confidence interval, quantifying uncertainty in the standardized estimates. # Create Confidence Interval of the estimations (CI column) # Shows range of uncertainty (p.q5 to p.q95) KSSL_standardized &lt;- KSSL_standardized %&gt;% mutate( # Create 90% confidence interval string (p.q5 to p.q95) CI = paste0( round(p.q5, 3), # Lower bound (5th percentile) &quot;-&quot;, round(p.q95, 3) # Upper bound (95th percentile) ) ) 1.8.2 Processing standardized data The output of the slab() function is a data frame in long format, where soil properties are stored as rows and the estimated percentile values are stored as columns. Each record is identified by ProfID, top, and bottom. For most downstream analyses, the data must be reshaped to wide format (i.e., one row per depth interval, with properties as columns). # Convert from long to wide format KSSL_standardized &lt;- KSSL_standardized %&gt;% pivot_wider( id_cols = c(ProfID, top, bottom), # Keep these as-is names_from = variable, # Property names become column names values_from = c(p.q50, CI), # Both point estimate and CI names_glue = &quot;{variable}_{.value}&quot; # Create names like &quot;SOC_p.q50&quot;, &quot;SOC_CI&quot; ) # Add geographic coordinates back KSSL_standardized &lt;- KSSL_standardized %&gt;% # Get coordinates from original data (one per profile) left_join( site_lab %&gt;% distinct(ProfID, .keep_all = TRUE) %&gt;% # One row per profile select(ProfID, lon, lat), by = &quot;ProfID&quot; ) %&gt;% # Move coordinates to front for readability relocate(lon, lat, .after = ProfID) # Since ProfIDs are now unique at each location, remove tailings ProfID values KSSL_standardized$ProfID &lt;- sub(&quot;_[12]$&quot;, &quot;&quot;, KSSL_standardized$ProfID) # Result: One row per profile-depth, with standardized soil properties head(KSSL_standardized) The final step is to save this dataset with standardized soil properties at two depths. # Save to CSV output &lt;- paste0(output_dir,&quot;KSSL_standardized.csv&quot;) write.csv(KSSL_standardized, output, row.names = FALSE) # Save to Excel output &lt;- paste0(output_dir,&quot;KSSL_standardized.xlsx&quot;) write_xlsx(KSSL_standardized, output) 1.8.3 Exporting standardized data for Digital Soil Mapping At this stage, the dataset is standardized to one row per profile and standard depth interval. Soil properties at these depths are derived from the original horizon measurements using depth-weighted averaging, and the output includes the percentiles of the weighted estimates (e.g., p05, p50, p95). For the Digital Soil Modelling exercise in this tutorial, we will work with a subset focused on the topsoil (0–30 cm) and use the median (p50) estimates for the following properties: clay, silt, sand, SOC, and pH. # Keep only 0-30 cm depth and select relevant columns (Clay, Silt, Sand, SOC &amp; pH) subset_data &lt;- data %&gt;% filter(top == 0 &amp; bottom == 30) %&gt;% select( ProfID, lon, lat, top, bottom, Clay = Clay_p.q50, Silt = Silt_p.q50, Sand = Sand_p.q50, SOC = SOC_p.q50, pH = pH_p.q50 ) # Save to CSV output_csv &lt;- paste0(output_dir,&quot;KSSL_DSM_0-301.csv&quot;) write.csv(subset_data, output_csv, row.names = FALSE) cat(&quot; Saved to:&quot;, output_csv, &quot;\\n&quot;) # Save to Excel output_xlsx &lt;- paste0(output_dir,&quot;KSSL_DSM_0-30.xlsx&quot;) write_xlsx(subset_data, output_xlsx) cat(&quot; Saved to:&quot;, output_xlsx, &quot;\\n&quot;) cat(&quot; Subset data ready for Digital Soil Mapping\\n&quot;) cat(&quot; Output file: KSSL_DSM\\n&quot;) 1.9 Preparing data for spectroscopy analyses The original KSSL dataset includes visible–near infrared (vis–NIR) spectral observations associated with each soil horizon. To support spectroscopy-based estimation of soil properties, spectral observations must be integrated with the cleaned horizon dataset (site_lab) that contains unique and consistent soil profiles (ProfID), validated and harmonized horizon depths (top, bottom), corrected laboratory measurements. Depth-consistent and quality-controlled reference data are essential for building robust spectral calibration models and avoiding bias introduced by duplicated horizons or invalid analytical values. In this dataset, each soil sample/horizon was measured four times by spectroscopy. Therefore, after merging spectra to the cleaned horizon dataset, the resulting dataset is expected to contain approximately 4× more rows than the resulting cleaned dataset (subject to missing spectra or incomplete records). Use a left join to preserve the cleaned horizon dataset as the reference. This ensures that every cleaned horizon remains in the merged dataset (even if spectra are missing), and no spectral-only records are introduced without corresponding cleaned horizon metadata. The join key for this operation is HorID in the cleaned dataset and smp_id in the spectral dataset. Then save the results as -csvand .xlsx files. # Read and subset spectral data from the original dataset raw_data &lt;- read_excel(&quot;../../01_data/module1/MIR_KANSAS_data.xlsx&quot;, sheet = 1) spec &lt;- raw_data[,-c(1,2,4:22)] # Merge site_lab data to the original Spectral data by their common IDs site_lab_spec &lt;- left_join (site_lab,spec, by=c(&quot;HorID&quot;=&quot;smp_id&quot;) ) # Save to CSV output &lt;- paste0(output_dir,&quot;KSSL_spectral_cleaned.csv&quot;) write.csv(site_lab_spec, output, row.names = FALSE) # Save to Excel output &lt;- paste0(output_dir,&quot;KSSL_spectral_cleaned.xlsx&quot;) write_xlsx(site_lab_spec, output) # Remove spectral data object rm(spec)  - Best practices and recommendations Trust but verify: Never assume data is correct. Always validate systematically. Document everything: Record what was removed, why it was removed, and how many records were affected. This documentation is essential for transparency and reproducibility. Preserve data lineage: Use unique row identifiers to track which raw records became which cleaned records. This allows you to trace any result back to its source data. Be conservative with removal: Only remove records if certain they are wrong. When uncertain, flag records for manual review rather than automatically excluding them. Automate, don’t manually edit: Write code that performs all cleaning steps, rather than manually editing spreadsheets. Code-based approaches are reproducible, transparent, and less prone to error. Save intermediate steps: Keep clean versions after each major processing step. This allows you to backtrack if a decision doesn’t work out.  - Common pitfalls to avoid Pitfall Problem Prevention Removing too much data Biased results from non-random loss Document removal rate; flag &gt;30% Skipping validation Problems propagate to analysis Use systematic checklists Manual edits Not reproducible, hard to verify Everything in code Ignoring depth issues Impossible harmonization Verify bottom &gt; top for all No documentation Can’t explain analysis later Keep detailed notes Overconfident correction Guessing wrong fixes errors Only correct if confident 1.10 Summary of exported files File Description KSSL_cleaned Clean horizon-level dataset with validated analytical data KSSL_spectral_cleaned Clean horizon-level dataset with validated analytical and spectroscopic data KSSL_standardized Depth-harmonized dataset (0–30 cm; 30–60 cm) of all soil properties KSSL_DSM Depth-harmonized dataset (0–30 cm) for Digital Soil Mapping soil_property_validation_report Detailed report of analytical properties outside valid ranges References "],["sampling-design-for-soil-surveys.html", "Chapter 2 Sampling design for soil surveys 2.1 Introduction 2.2 Sampling methodologies for soil spatial survey 2.3 SoilFER Sampling Design 2.4 Tutorial Using R 2.5 Principal Component Analysis (PCA) 2.6 Organize the Sampling Universe 2.7 Create Primary Sampling Units (PSU Grid) 2.8 Select PSUs with Sufficient Land Use Coverage 2.9 Rasterize PSUs for Covariate Space Coverage 2.10 Compute Optimal Sample Size 2.11 Covariate Space Coverage - Computing PSUs 2.12 Compute SSUs and TSUs 2.13 Export Sampling Units 2.14 Compute Alternative PSUs", " Chapter 2 Sampling design for soil surveys 2.1 Introduction Soil surveys play a critical role in environmental monitoring, evaluating soil degradation, and formulating sustainable land use strategies (bui2020?). They offer essential information on soil characteristics, including texture, structure, pH levels, organic matter content, nutrient availability, and other physicochemical attributes of interest. These data are essential for making informed choices about crop selection, fertilizer use, irrigation planning, and soil conservation and management strategies. Therefore, soil information and data are key components in any comprehensive soil survey that supplies necessary information for soil management decisions at local to global scales. In this sense, the Soil Mapping for Resilient Agrifood Systems (SoilFER) framework aims to provide open-access soil information system to support the formulation of digital soil maps, fertilizer recommendations, establish crop suitability, determine soil quality indicators, assess soil health, implement soil conservation practices, and create a national soil spectral library. State-of-the-art soil sampling protocols incorporate a systematic approach that considers factors such as soil variability, sampling depth, current use of the land, farming activities, and sampling density (brevik2016?; clay2018?; morton2000?; norris2020?). The SoilFER Sampling Design Technical Manual provides a general guidance on soil sampling design methodologies to ensure precision, reproducibility, and reliability in the proper collection of soil samples for digital soil mapping and monitoring. This manual presents a three-stage hierarchical hybrid method incorporating both probability and non-probability sampling for soil mapping and monitoring within the SoilFER framework. The first and second hierarchical levels select respectively the primary and secondary sampling units using a covariate space coverage sampling method on environmental covariates (model-based), while the hierarchical third-level sampling selects the tertiary sampling units using simple random sampling, without replacement (design-based). This implementation standardizes soil sampling methodologies across the various countries involved in the project. 2.2 Sampling methodologies for soil spatial survey Soil monitoring and mapping are essential to understand soil variability and manage soil resources efficiently. In this context, it is important to understand the basics of choosing a sampling methodology that best fits the project’s objectives, whether focused on mapping, monitoring, or a combination of both. The aim of this section is to provide an overview of sampling design concepts, making them understandable and accessible for both academic and non-academic audiences. 2.2.1 Selection of the sampling methodology Purposive (also known as traditional or expert) and probability sampling are the most used methods for selecting sampling units in soil surveys. Both methods differ basically in their objectives. In simple terms, expert sampling involves selecting specific points along a slope or landscape feature, either at individual locations or along transects, such as a catena or toposequence. This approach aims to capture the relationships between the variation in soil properties and the different geographical features that make up the landscape. This concept has been widely used as stated in the literature for soil mapping (borden2020?; gobin2000?; milne1936?). This selection method effectively captures local variations in soil properties and is useful for understanding how these properties change in relation to terrain and hydrological characteristics. However, this sampling method may not always reflect the overall variability of soil properties across the entire study area. Moreover, implementing this approach generally requires significant effort, time, and funding, particularly for large study areas. On the other hand, probability sampling assigns a known probability of selection to each sampling unit, ensuring that each sampling unit has a chance of being included in the sample. This approach focuses on obtaining representative samples to make unbiased inferences about the entire population under study (degruijter2006?). This method requires careful planning and implementation to ensure that the sampling design is both unbiased and representative of the population. In addition, its implementation can be more complex compared to the expert sampling method. However, its main advantage is that every sampling unit has a chance of being included in the sample, resulting in a more representative sample of the variability in the study area. Additionally, it allows statistical inferences about the entire population from the sample. This last aspect, if not the most important, allows soil surveyors to monitor and better report the current and future status of soil properties. In general, traditional/expert (non-probabilistic) sampling is less flexible than probabilistic sampling since the latter allows for both types of inference methods, as we will see in the next subsection. 2.2.2 Inference Methods Two primary statistical inference methods are used in soil survey: design- and model-based inference approaches (degruijter2006?). Design-based inference utilizes the sample design to make inferences about the entire population including probabilities of the sampling units. This approach is suitable for estimating global quantities, such as means and totals, or local quantities for large domains based on the design of the sampling scheme. Briefly, global quantities encompass the spatial cumulative distribution function of the response variable in the sampling universe, or quantities derived from it, such as quantiles, median, mean, and standard deviation. Local quantities, on the other hand, pertain to individual estimates within sub-areas of the sampling universe, also known as domains. Model-based inference uses statistical models to make inferences about unsampled locations based on the relationships observed in the sampled data. This approach is valuable for predicting soil properties at unsampled locations, especially in areas where sampling is limited. Further information about design- and model-based approaches can be found in (brus2021?). 2.2.3 Purpose of Sampling Bounding together selection and inference methods, one can better determine which one of those combinations works best for their project goals. The choice of sampling method in soil survey and monitoring depends on the survey’s purpose (brus2022?). For monitoring soil properties over time, design-based inference and probability sampling selection are suitable. These methods allow for the unbiased estimation of changes in soil properties at specific locations over time. For mapping soil properties across a landscape, model-based inference and non-probabilistic sampling are more appropriate. These methods allow for the spatial prediction of soil properties at unsampled locations based on the relationships observed in the sampled data. Monitoring differs from estimating soil parameters of domains at a given point in time. However, similar to monitoring, design-based inference and probability sampling are the most suitable methods for estimating soil properties within a certain domain. These approaches ensure that the estimates of soil parameters (e.g., mean, variance, mode, median) are unbiased and representative of the study area. 2.2.4 Sampling Design Types The sequence of decision-making for a soil survey or monitoring project should follow a logical order to ensure that the objectives of the project are met effectively. Begin by defining the purpose of the project. Determine whether the goal is to monitor changes in soil properties over time or to map them across landscape. Based on the purpose, one can choose an appropriate selection method. After that, one can select the inference method which best suits the project. For instance, if the goal is monitoring, consider probability sampling. For mapping, consider either non-probability or probability methods. Once the purpose, selection and inference methods are defined, one can select the appropriate sampling design. Before diving into the types of sampling design, it is important to take into consideration the stratification of the sampling universe. Stratification involves dividing the sampling universe into strata or sub-areas based on certain criteria, such as soil-climate types or land-use. This approach can increase the efficiency of the sampling design by obtaining separate estimates for each stratum, leading to more precise estimates for specific regions of interest (ROI). For example, if the ROI has distinct soil types, stratifying the sample by soil type can ensure that each soil type is adequately represented in the sample. Another example is using explanatory variables from remote sensing as a means of spatial coverage and stratification. We divided the sampling design types into monitoring and mapping (Table 2.1). For monitoring, one can certainly choose among simple random sampling (SRS), stratified SRS, or two-stage cluster random sampling. In SRS, each sampling unit in the ROI has equal probability of being selected. This approach is straightforward and easy to implement but may not account for spatial variability in soil properties. Stratified simple random sampling divides the ROI into strata based on certain criteria, such as soil-climate types or land-use. Then, SRS is conducted within each stratum. This method ensures that each stratum is adequately represented in the sample, leading to more precise estimates for specific ROI. Finally, two-stage cluster random sampling, the sampling units are grouped into clusters, and then clusters are randomly selected for sampling. This method is useful for large study areas where it may be impractical to sample every unit individually. One interesting method, which is not specifically categorized as simple, stratified, or two-stage cluster random sampling, is called balanced acceptance sampling (robertson2013?). This methodological approach can be applied within any of these sampling design types to ensure a balanced representation of different strata in the sample. For example, in a simple random sampling design, balanced acceptance sampling (BAS) would involve ensuring that each stratum is adequately represented in the random sample. In a stratified sampling design, BAS would aim to balance the number of sampling points across strata. In a two-stage cluster random sampling design, BAS would involve ensuring a balanced selection of clusters from each stratum. In essence, BAS is a principle or strategy that can be incorporated into various sampling design types to reduce bias in the estimation of soil properties. If the main goal of a sampling design is to map soil properties, non-probability methods such as conditioned Latin Hypercube sampling (minasny2006?), catena or topo-sequence sampling (milne1936?), grid sampling, or covariate space coverage sampling (brus2018?; degruijter2006?) are the most suitable choices. Conditioned Latin Hypercube sampling (CLHS) selects sampling points that ensure a representative distribution of soil properties across the study area. This approach is particularly useful for capturing spatial variability within the explanatory variables that describe the pattern of the response variable, leading to more accurate soil property maps. Catena or topo-sequence sampling was already discussed in section 2.2.1. Covariate space coverage sampling (CSCS) selects sampling points to provide complete spatial coverage of the ROI. (ma2020?) compared the efficiency of SRS, CLHS, and CSCS, being the latter the most efficient. In addition to the aforementioned methods, there are sampling approaches designed to produce samples that are evenly distributed across the study area, known as spatially balanced designs. These methods ensure that all areas are considered for inclusion in fieldwork, often requiring fewer samples compared to simple random sampling (SRS). Generalized Random Tessellation Stratified (GRTS) sampling (stevens1999?) and BAS are two popular approaches within the balanced sampling framework. The rationale behind these methods is that for estimating overall properties over a landscape, such as mean soil carbon, the estimate is most reliable when samples cover all areas of interest. While SRS, GRTS, and BAS are among the many sampling methods available, each with its own set of advantages and disadvantages, selecting the most suitable method can be daunting for field scientists. They often need to choose sites for fieldwork while adhering to constraints such as budget and time. For instance, the GRTS method can be used to provide a spatially balanced, probability sample with design-based, unbiased variance estimators with a minimum sample size. Table 2.1: Short description of pros and cons within each soil sampling design type Sampling.Design Purpose Pros Cons Simple Random (SR) Monitoring Simple, unbiased May miss rare features Stratified Simple Random (SSR) Monitoring Better precision per stratum Requires prior stratification Two-Stage Cluster Random (TSCR) Monitoring Cost-effective for large areas Higher variance Conditioned Latin Hypercube (CLH) Mapping Environmental representativeness Complex implementation Catena/Toposequence (CT) Mapping Captures terrain relationships Subjective, labor-intensive Covariate Space Coverage (CSC) Mapping Most efficient for mapping Computationally intensive Generalized Tessellation Stratified (GRT) Both Spatially balanced Complex variance estimation Balanced Acceptance (BA) Both Reduces bias Requires careful planning 2.2.4.1 Example: Soils4Africa Sampling Design The EU-funded project Soils4Africa aimed to provide open-access information about the condition and spatio-temporal dynamics of African soils, accompanied by a methodology for repeated soil monitoring and mapping across the African continent to support sustainable agriculture in Africa. The project’s purpose was to monitor and map African Soils. Given this purpose, probability sampling was identified as the most suitable selection method. This method allowed both inference methods, design- and model-based mapping. Once the purpose, selection and inference methods are defined, the final step was to choose the appropriate sampling design, which was a three-stage random sampling. The sampling design was based in a hierarchical sampling method with involves the selection of sampling sites at three scale levels: Primary Sampling Units (PSUs): PSUs constitute the broadest spatial units within the study area, typically defined based on geographical, administrative, ecological boundaries, or soil management types. The size of each PSU is 4 square kilometers (2 × 2 km grid cells). The total number of PSUs selected depends on the sample size and ideally should be sufficient to cover the range of environmental gradients present in the study area. Secondary Sampling Units (SSUs): Within each PSU, a predefined number of SSUs are selected for detailed sampling. SSUs offer a finer resolution level of sampling within each PSU. The size of an SSU is determined based on the need to capture localized environmental conditions and variability. In this case the size of the SSUs is 1 hectare (100 × 100 m grid cells). This approach involves a random selection of 7 SSUs per PSU to ensure a comprehensive coverage of the PSU’s environmental diversity. Four of these SSU are target SSUs and 3 are alternative SSUs to be used as replacement areas due to potential sampling challenges encountered in the field. Tertiary Sampling Units (TSUs): TSUs represent the most detailed level of sampling, focusing on specific points within SSUs. The number of TSUs within each SSU is 3, one which acts as the primary target point and 2 as replacement points for the primary point. The sampling design in Soils4Africa proceeds as follows: (i) A first selection of PSUs is made using a stratified random sampling on farming systems strata. The farming system classification data comes from the FAO farming systems map for Africa which provides information for 46 different farming classes or stratum. (ii) Within each of the selected PSUs, a second sampling level is done upon a regular division of the PSU into cells of 1 ha, the SSU. For each PSU, 4 random SSUs are selected for sampling and 3 additional cells are stored as alternative cells. These cells are used as replacements in case some of the target SSUs cannot be sampled for some reason. The selection of SSUs is done by simple random sampling within the PSU. (iii) Within each SSU, 3 simple random sampling points are located. The first point serves as the target point to be sampled and the remaining 2 are stored as replacement points to be used as in the previous step. This Hierarchical Sampling design helps reduce bias in the data, allowing for accurate assessments of soil constraints, quantification of risk factors, and reliable evaluations of changes in soil health/quality. 2.3 SoilFER Sampling Design The SoilFER soil spatial survey aims to estimate means and totals for a set of target soil properties and quality indicators within three domains: croplands (85%), grasslands (10%), and forests (5%) in a country. The percentages for each domain are relative to the total sampling points, considering the sampling density required for high-resolution digital soil maps. Once the purpose was defined, the most suitable selection method identified was probability sampling, which allows for both design- and model-based inference methods. Given the purpose, selection and inference methods, the final step was to choose the appropriate sampling design. Like the Soils4Africa project, the SoilFER project employs three sampling units, which are discrete entities in space, and thus a three-stage hierarchical hybrid method incorporating both probability and non-probability sampling was selected for mapping and monitoring within the SoilFER sampling design. In the literature, covariate space coverage (CSC) sampling has proven to be more efficient than other approaches for mapping a continuous soil property or class (brus2018?; degruijter2006?; ma2020?; schmidinger2024?). Therefore, in the SoilFER sampling design, the first stage or first hierarchical level selects the primary sampling units (PSUs) (i.e., 2km × 2km) using a CSC sampling method, or k-means sampling if you will (i.e., model-based). K-means is an unsupervised clustering method used when no response variable is available, partitioning data into k groups by maximizing similarity within groups and differences between them. Like the selection of PSUs, the second hierarchical level selects the secondary sampling units (SSUs) (i.e., 100m × 100m) using CSC sampling and a second set of environmental covariates (i.e., model-based). Lastly, the third hierarchical levels select the tertiary sampling units using simple random sampling without replacement (i.e., design-based). This multi-stage sampling framework enhances both spatial representativeness and statistical reliability. 2.3.1 Soil Properties Understanding and assessing soil properties are critical for evaluating soil health, guiding fertilizer recommendations, and informing land management decisions. In the SoilFER project, both chemical and physical soil attributes are measured to provide a comprehensive evaluation of soil quality and health (Table 2.2). Some key chemical properties include soil organic carbon, pH, cation exchange capacity, and nutrient levels such as nitrogen, phosphorus, and potassium. On the physical side, attributes like soil texture, soil depth, rockiness of surface soil, drainage, surface cracks, soil color, soil erosion and gypsum content are analyzed to understand soil structure, compaction, and water infiltration rates. Collectively, these attributes offer valuable insights into the soil’s ability to support plant growth, resist erosion, and store carbon, making them indispensable for both mapping soil health and tailoring site-specific fertilizer recommendations. Table 2.2: Some of the chemical and physical soil properties to be quantified in the SoilFER project Category Property Unit Chemical Soil Organic Carbon % Chemical pH pH units Chemical Cation Exchange Capacity cmol(+)/kg Chemical Total Nitrogen % Chemical Available Phosphorus mg/kg Chemical Exchangeable Potassium cmol(+)/kg Chemical Micronutrients mg/kg Chemical Electrical Conductivity dS/m Physical Soil Texture % sand/silt/clay Physical Soil Depth cm Physical Rockiness % coverage Physical Drainage class Physical Surface Cracks present/absent Physical Soil Color Munsell Physical Erosion severity class Physical Gypsum Content % Note: Both wet and dry chemistry (i.e., infrared spectroscopy) will be conducted in the laboratory for soil analysis as part of the SoilFER project. These methods are not listed here, as a standard operating procedure exists for each one of them. 2.3.2 Environmental Covariates Environmental covariates, also referred to as explanatory or environmental variables or simply covariates, are essential for understanding the spatial variability of soil properties and landscape processes. The SoilFER sampling design incorporates 85 environmental covariates (Table 2.3) to optimise the identification and allocation of PSUs (2km × 2km) across diverse landscapes at a national scale. As SoilFER targets soil mapping at national scale, the first set of environmental covariates have pixel resolution varying from 250 m to 1 km. To align with the PSU size (2km × 2km), these covariates were upscaled to ensure that each PSU is represented by a single aggregated value per covariate. This resolution provides a balance between sufficient spatial detail and computational efficiency, ensuring accurate large-scale mapping while manageable computational processing requirements, ensuring efficient data handling and analysis. Moreover, this comprehensive set of covariates ensures that the selected PSUs capture the full range of environmental conditions and soil diversity within a country. Additionally, a second set of 24 environmental covariates (Table 2.4) is used to refine site selection at farm level. This high-resolution dataset (e.g., ≤ 90 m pixel resolution) was upscaled to match the SSU size (100m × 100m), ensuring that each SSU is characterized by a single representative value for each covariate. This significantly enhances the precision of SSU selection by capturing finer spatial variability. If additional high-resolution environmental covariates are available for a given country, it is highly recommended to incorporate them into the analysis to further improve sampling precision. These two sets of environmental covariates were carefully chosen based on their relevance to soil formation processes, spatial variability, and their role as proxies for key soil properties. Further information on how to retrieve these environmental covariates can be found in the SoilFER GitHub repository. Table 2.3: Some of the chemical and physical soil properties to be quantified in the SoilFER project Category Property Unit Chemical Soil Organic Carbon % Chemical pH pH units Chemical Cation Exchange Capacity cmol(+)/kg Chemical Total Nitrogen % Chemical Available Phosphorus mg/kg Chemical Exchangeable Potassium cmol(+)/kg Chemical Micronutrients mg/kg Chemical Electrical Conductivity dS/m Physical Soil Texture % sand/silt/clay Physical Soil Depth cm Physical Rockiness % coverage Physical Drainage class Physical Surface Cracks present/absent Physical Soil Color Munsell Physical Erosion severity class Physical Gypsum Content % Note: NSM, Normalized Soil Moisture; SWIR, Shortwave Infrared. Full list contains 85 variables. Table 2.4: List of environmental covariates used to delineate the secondary sampling units Source Variable Resolution SRTM Terrain Elevation 90-100m SRTM Terrain Slope 90-100m SRTM Terrain Aspect 90-100m SRTM Terrain TPI 90-100m SRTM Terrain Hillshade 90-100m Sentinel-2 Spectral B2 (Blue) 10-20m Sentinel-2 Spectral B3 (Green) 10-20m Sentinel-2 Spectral B4 (Red) 10-20m Sentinel-2 Spectral B5 (Red Edge 1) 10-20m Sentinel-2 Spectral B6 (Red Edge 2) 10-20m Sentinel-2 Spectral B7 (Red Edge 3) 10-20m Sentinel-2 Spectral B8 (NIR) 10-20m Sentinel-2 Spectral B8A (NIR Narrow) 10-20m Sentinel-2 Spectral B11 (SWIR 1) 10-20m Sentinel-2 Spectral B12 (SWIR 2) 10-20m Sentinel-2 Indices NDVI 10-20m Sentinel-2 Indices EVI 10-20m Sentinel-2 Indices NDBSI 10-20m Sentinel-2 Indices Brightness Index 10-20m Sentinel-2 Indices Redness Index 10-20m Sentinel-2 Indices VNSIR 10-20m Geomorphology Geomorphon Class 1-9 90m Geomorphology … 90m Geomorphology … 90m 2.3.3 Understanding the Methodology The SoilFER sampling design methodology follows a hierarchical sampling approach with three levels of increasing spatial resolution: Primary Sampling Units (PSUs), Secondary Sampling Units (SSUs), and Tertiary Sampling Units (TSUs) (Figure ??). This design is similar to the Soils4Africa project but differs in two key aspects. First, we use covariate space coverage (CSC) to select PSUs (brus2022?), whereas Soils4Africa employed stratified random sampling at this stage. At the first hierarchical level, CSC is used to select PSUs (2km × 2km areas), ensuring a well-distributed spread across the covariate space to maximise environmental variation and represent soil diversity across the region at national scale. Second key aspect, we select SSUs using CSC rather than simple random sampling, as done in Soils4Africa. At this second hierarchical level, CSC is applied to select SSUs (100m × 100m) based on a refined set of environmental covariates at farm scale. It is of note that each project adopted a methodology tailored to its specific objectives/purposes, ensuring that the sampling design aligns with its intended goals. Each PSU contains multiple SSUs, each measuring 100m × 100m, and each SSU consists of three TSUs, each measuring 20m × 20m. This structured yet flexible sampling design captures soil variability across different spatial scales (Figure ??). 2.3.3.1 PSU Selection Process The process of selecting the PSUs is summarized as follows: Step 1: Create 2km × 2km Grid (Figure ??) - A 2km × 2km grid covering the entire region of interest (ROI) is created - This grid serves as the foundational structure for stratification and spatial analysis Step 2: Overlay Land Use and Protected Areas (Figure ??) - Land-use and land cover (LULC) mask layer identifying cropland, forest, or grassland is overlaid - Protected area boundaries are incorporated - Grid cells where cropland, forest, or grassland cover more than 10% (or specified percentage) are retained - This ensures that only PSUs containing target land uses are selected Step 3: Apply Covariate Space Coverage (Figure ??) - PSUs are selected through CSC sampling using 85 environmental covariates - Covariates are resampled to 250m resolution - Principal Component Analysis (PCA) transforms covariates into PC layers - PC layers accounting for up to 99% of variance are retained - PC layers are upscaled to 2km × 2km resolution Step 4: Determine Optimal Sample Size - Optimal number of PSUs determined using divergence metrics - Kullback-Leibler Divergence (KLD) quantifies how well sampled data represent full environmental covariate space - Unlike Jensen-Shannon methods, KLD does not overestimate optimal sample size (saurette2023?) Step 5: Perform K-means Clustering - K-means clustering creates homogeneous clusters (strata) - Number of clusters equals total number of PSUs to be sampled - Legacy point data can be incorporated (clusters fixed at legacy locations) - Mean Squared Shortest Distance (MSSD) in covariate space is calculated - Process repeated 10 times, retaining trial with minimum MSSD Step 6: Select Target and Replacement PSUs (Figure ??) - PSUs allocated by minimizing MSSD within environmental covariate space - Output: N PSUs where N equals total number of samples - Replacement PSUs calculated by randomly selecting grid cells within same cluster class 2.3.3.2 SSU and TSU Selection Process Stage 2: Secondary Sampling Units (SSUs) The second stage involves selecting SSUs using Covariate Space Coverage Sampling (CSCS) based on 24 high-resolution environmental covariates. Each SSU represents a 100m × 100m (1 hectare) area divided into 25 regular sampling plots of 20m × 20m (Figure ??). Selection scheme: - 8 SSUs selected from each PSU - 4 target SSUs (1-4): where samples will be collected - 4 replacement SSUs (5-8): pre-identified substitutes - Replacement SSUs directly linked to targets (one-to-one basis): - SSU 5 replaces SSU 1 - SSU 6 replaces SSU 2 - SSU 7 replaces SSU 3 - SSU 8 replaces SSU 4 If all SSUs within a target PSU are exhausted, missing SSUs must be selected from the designated PSU replacement. The designated PSU replacements were selected with the same spatial variability as their PSU targets. Stage 3: Tertiary Sampling Units (TSUs) The third and final stage involves selecting TSUs using simple random sampling without replacement. Each SSU contains 25 possible TSUs of 20m × 20m, corresponding to the pixel size of the crop raster layer. Selection scheme: - 3 TSUs per SSU: - 1 target TSU (primary sampling location) - 2 replacement TSUs (alternatives) - If a TSU is unsuitable, any replacement can be used - If all TSUs within an SSU are unsuitable, entire SSU is rejected Soil sampling protocol: - Sampling conducted strictly at designated TSU locations - Maximum shift: 10 metres from defined coordinates 2.3.3.3 Site Identification System The final site ID is an alphanumeric identifier structured as follows (Figure ??): Format: AAA####-#-#X Where: - AAA = Three-letter country ISO code (e.g., KEN for Kenya) - #### = Primary Sampling Unit number (zero-padded, e.g., 0001) - First # = Secondary Sampling Unit number (1-8) - Second # = Tertiary Sampling Unit number (1-3) - X = Land use type code: - C = Cropland - G = Grassland - F = Forest Example: KEN0001-1-1C - Country: Kenya - PSU: 1 - SSU: 1 (target) - TSU: 1 (target) - Land use: Cropland 2.4 Tutorial Using R This tutorial is designed for users with a basic understanding of R programming and provides a comprehensive guide to implementing a soil sampling design for soil monitoring and mapping. All scripts and data are available at the SoilFER GitHub repository.  Note: Ensure you have the tinytex, knitr, rmarkdown, and rstudioapi R packages installed to successfully run and render R Markdown documents before running this script. Additionally, install tinytex by running tinytex::install_tinytex() to enable LaTeX support for PDF generation. 2.4.1 Setting Up the Environment First, set the working directory to the location of your R script. This ensures that all paths in the script are relative to the script’s location. setwd(dirname(rstudioapi::getActiveDocumentContext()$path)) setwd(&quot;../&quot;) # Move wd down to the main folder getwd() 2.4.1.1 Install Required Libraries The script uses several R libraries, each serving a specific purpose (Table 2.5). Table 2.5: Summary of required R libraries and their purposes for SoilFER sampling design Package Purpose sp Spatial data classes terra Raster data manipulation raster Legacy raster support sf Simple features (vector data) sgsR Spatial sampling entropy Entropy calculations tripack Triangulation tibble Data frames manipulate Interactive plotting dplyr Data manipulation synoptReg PCA for rasters doSNOW Parallel processing Rfast Fast R functions fields Distance calculations ggplot2 Visualization rassta Terrain analysis Some libraries might need to be installed from GitHub. Uncomment and run the lines below if needed: # Install synoptReg package from GitHub # install.packages(&quot;remotes&quot;) # Install remotes if not installed # remotes::install_github(&quot;lemuscanovas/synoptReg&quot;) packages &lt;- c(&quot;sp&quot;, &quot;terra&quot;, &quot;raster&quot;, &quot;sf&quot;, &quot;sgsR&quot;, &quot;entropy&quot;, &quot;tripack&quot;, &quot;tibble&quot;, &quot;manipulate&quot;, &quot;dplyr&quot;, &quot;synoptReg&quot;, &quot;doSNOW&quot;, &quot;Rfast&quot;, &quot;fields&quot;, &quot;ggplot2&quot;, &quot;rassta&quot;) invisible(lapply(packages, library, character.only = TRUE)) rm(packages) Alternative method: In RStudio, use the Packages tab in the bottom-right pane → click Install → type package name → ensure “Install dependencies” is checked → click Install. 2.4.2 Define Variables and Parameters Several variables and parameters must be defined to establish a consistent framework for the script. 2.4.3 Country ISO Code ISO.code &lt;- &quot;ZMB&quot; # Zambia example  Note: Use the Alpha-3 ISO code for your country of interest. 2.4.3.1 Land Use Type The SoilFER project focuses on three primary land-use types: croplands (80%), grasslands (15%), and forests (5%). landuse &lt;- &quot;crops&quot; # Options: &quot;crops&quot;, &quot;grassland&quot;, &quot;forest&quot; 2.4.3.2 File Paths # Path to data folders raster.path &lt;- &quot;data/rasters/&quot; shp.path &lt;- &quot;data/shapes/&quot; other.path &lt;- &quot;data/other/&quot; landuse_dir &lt;- paste0(&quot;data/results/&quot;, landuse, &quot;/&quot;) # Check if landuse directory exists; if not, create it if (!file.exists(landuse_dir)) { dir.create(landuse_dir) } results.path &lt;- landuse_dir 2.4.3.3 Coordinate Reference System epsg &lt;- &quot;EPSG:3857&quot; # WGS 84 / Pseudo-Mercator # Find your CRS at: https://epsg.io/ 2.4.3.4 Sample Size Calculation nsamples &lt;- 300 # Total sampling sites share &lt;- 0.80 # 80% for this land use (croplands) nsites &lt;- nsamples * share # Final number of sites # However, we will calculate the optimal sample size statistically later 2.4.3.5 Sampling Unit Definitions # Define the number of PSUs to sample n.psu &lt;- round(nsamples/8) # Will be optimized later # Define PSU and SSU sizes psu_size &lt;- 2000 # 2km × 2 km ssu_size &lt;- 100 # 100m × 100m = 1 ha # Define number of target and alternative SSUs at each PSU num_primary_ssus &lt;- 4 num_alternative_ssus &lt;- 4 # Define number of TSUs at each SSU number_TSUs &lt;- 3 2.4.3.6 Algorithm Parameters # Number of iterations for clustering algorithm iterations &lt;- 10 # 10 is sufficient # Minimum crop percentage in selected PSUs percent_crop &lt;- 10 # At least 10% crop coverage 2.4.4 Custom Functions 2.4.4.1 Covariate Space Coverage Function (CSIS) The CSIS function is the backbone of clustering sampling units in covariate space. It ensures optimal distribution while considering any fixed legacy data. How it works: Input Parameters: fixed: Preselected (legacy) sampling points nsup: Number of additional (new) sampling points to select nstarts: Number of random starting points for clustering mygrd: Grid of covariate data for ROI Workflow: Extract fixed points and exclude from grid Randomly initialize new sampling points Combine fixed and new centers Compute distances and assign points to nearest cluster Update cluster centers iteratively until convergence Calculate Mean Squared Shortest Distance (MSSSD) Save best clustering result # Clustering CSC function with fixed legacy data CSIS &lt;- function(fixed, nsup, nstarts, mygrd) { n_fix &lt;- nrow(fixed) p &lt;- ncol(mygrd) units &lt;- fixed$units mygrd_minfx &lt;- mygrd[-units, ] MSSSD_cur &lt;- NA for (s in 1:nstarts) { units &lt;- sample(nrow(mygrd_minfx), nsup) centers_sup &lt;- mygrd_minfx[units, ] centers &lt;- rbind(fixed[, names(mygrd)], centers_sup) repeat { D &lt;- rdist(x1 = centers, x2 = mygrd) cluster &lt;- apply(X = D, MARGIN = 2, FUN = which.min) %&gt;% as.factor(.) centers_cur &lt;- centers for (i in 1:p) { centers[, i] &lt;- tapply(mygrd[, i], INDEX = cluster, FUN = mean) } # Restore fixed centers centers[1:n_fix, ] &lt;- centers_cur[1:n_fix, ] # Check convergence sumd &lt;- diag(rdist(x1 = centers, x2 = centers_cur)) %&gt;% sum(.) if (sumd &lt; 1E-12) { D &lt;- rdist(x1 = centers, x2 = mygrd) Dmin &lt;- apply(X = D, MARGIN = 2, FUN = min) MSSSD &lt;- mean(Dmin^2) if (s == 1 | MSSSD &lt; MSSSD_cur) { centers_best &lt;- centers clusters_best &lt;- cluster MSSSD_cur &lt;- MSSSD } break } } print(paste0(s, &quot; out of &quot;, nstarts)) } list(centers = centers_best, cluster = clusters_best) }  Note: This function creates spatially balanced sampling designs by considering existing sampling points and covariate distributions. 2.4.4.2 TSU Generation Function This function generates TSUs within SSUs, ensuring random distribution while respecting land-use constraints. generate_tsu_points_within_ssu &lt;- function(ssu, number_TSUs, index, ssu_type, crops) { # Convert SSU to SpatVector for masking ssu_vect &lt;- ssu_grid_sf[index, ] # Clip the raster to the SSU boundaries clipped_lu &lt;- crop(crops, ssu_vect) # Generate random points within the clipped raster sampled_points &lt;- sample_srs(clipped_lu, nSamp = number_TSUs) # Ensure required number of TSUs while (nrow(sampled_points) &lt; number_TSUs) { sampled_points &lt;- spatSample(clipped_lu, size = number_TSUs) } # Add metadata sampled_points$PSU_ID &lt;- selected_psu$ID sampled_points$SSU_ID &lt;- index sampled_points$TSU_ID &lt;- seq_len(nrow(sampled_points)) sampled_points$SSU_Type &lt;- ssu_type sampled_points$TSU_Name &lt;- paste0(ISO.code, sprintf(&quot;%04d&quot;, sampled_points$PSU_ID), &quot;-&quot;, sampled_points$SSU_ID, &quot;-&quot;, seq_len(nrow(sampled_points))) return(sampled_points) } 2.4.5 Load Country Boundaries and Legacy Data Load and transform country boundaries and legacy soil data to the desired CRS: # Define file locations country_boundaries &lt;- file.path(paste0(shp.path, &quot;roi_epsg_3857.shp&quot;)) legacy &lt;- file.path(paste0(shp.path, &quot;zmb_legacy_v2_clipped_epsg_3857.shp&quot;)) # Load and transform country boundaries country_boundaries &lt;- sf::st_read(country_boundaries, quiet = TRUE) if (sf::st_crs(country_boundaries)$epsg != epsg) { country_boundaries &lt;- sf::st_transform(country_boundaries, crs = epsg) } # Load legacy data (if it exists) if (file.exists(legacy)) { legacy &lt;- sf::st_read(legacy, quiet = TRUE) # Transform coordinates if (sf::st_crs(legacy)$epsg != epsg) { legacy &lt;- sf::st_transform(legacy, crs = epsg) } } else { # If legacy data does not exist, delete the object rm(legacy) }  Note: Always verify that all geospatial datasets use the same CRS before performing spatial operations. Misaligned CRSs can lead to incorrect analyses. 2.4.5.1 Visualize Boundaries and Legacy Data library(ggplot2) library(ggspatial) basemap &lt;- annotation_map_tile(&quot;osm&quot;) ggplot(data = country_boundaries) + basemap + geom_sf(fill = &quot;transparent&quot;, color = &quot;black&quot;) + geom_sf(data = legacy, aes(geometry = geometry), color = &quot;black&quot;, size = 0.3) + labs(title = &quot;Country Boundaries and Legacy Data&quot;, caption = &quot;Data source: OpenStreetMap&quot;) + theme_minimal() + theme(plot.title = element_text(hjust = 0.5)) 2.4.6 Load Environmental Covariates for PSUs 2.4.6.1 Optional Auxiliary Data Additional data layers can improve PSU selection: # Non-protected areas (shapefile or raster) npa &lt;- file.path(paste0(shp.path, &quot;zmb_national_parks_zari_clipped_epsg_3857.shp&quot;)) # High slopes (binary raster: 1 = &lt;50%, NA = &gt;=50%) slope &lt;- file.path(paste0(raster.path, &quot;zmb_clipped_slope_mask_epsg_3857.tif&quot;)) # Geology geo &lt;- file.path(paste0(shp.path, &quot;zmb_geology_clipped_epsg_3857.shp&quot;)) geo.classes &lt;- &quot;GEO&quot; # Field name for geologic classes # Geomorphology geomorph &lt;- file.path(paste0(raster.path, &quot;zmb_clipped_Geomorphon.tif&quot;)) geomorph.classes &lt;- &quot;CODR&quot; # Field name for geomorphology classes 2.4.6.2 Load and Process Auxiliary Data # Non-protected areas if (file.exists(npa)) { npa &lt;- sf::st_read(npa, quiet = FALSE) npa &lt;- sf::st_union(npa) npa &lt;- sf::st_difference(country_boundaries, npa) if (sf::st_crs(npa)$epsg != epsg) { npa &lt;- sf::st_transform(npa, crs = epsg) } } else { rm(npa) } # Slope mask if (file.exists(slope)) { slope &lt;- rast(slope) if (crs(slope) != epsg) { slope &lt;- project(slope, epsg, method = &quot;near&quot;) } slope &lt;- slope / slope # Normalize to 0/1 } else { rm(slope) } # Geology if (file.exists(geo)) { geo &lt;- sf::st_read(geo, quiet = TRUE) if (sf::st_crs(geo)$epsg != epsg) { geo &lt;- sf::st_transform(geo, crs = epsg) } geo$GEO &lt;- as.numeric(as.factor(geo[[geo.classes]])) } else { rm(geo) } # Geomorphology if (file.exists(geomorph)) { file_extension &lt;- tools::file_ext(geomorph) if (file_extension == &quot;tif&quot;) { geomorph &lt;- rast(geomorph) names(geomorph) &lt;- &#39;GEOMORPH&#39; if (crs(geomorph) != epsg) { geomorph &lt;- project(geomorph, epsg, method = &quot;near&quot;) } } else if (file_extension == &quot;shp&quot;) { geomorph &lt;- sf::st_read(geomorph, quiet = TRUE) if (sf::st_crs(geomorph)$epsg != epsg) { geomorph &lt;- sf::st_transform(geomorph, crs = epsg) } geomorph$GEOMORPH &lt;- as.numeric(as.factor(geomorph[[geomorph.classes]])) } else { warning(&quot;File format not recognized. Expected .tif or .shp.&quot;) rm(geomorph) } } else { rm(geomorph) } 2.4.7 Load Main Environmental Covariates # Load covariate stack cov.dat &lt;- list.files(raster.path, pattern = &quot;covs_zam_clipped.tif$&quot;, recursive = TRUE, full.names = TRUE) cov.dat &lt;- terra::rast(cov.dat) cat(sprintf(&quot;Loaded %d environmental covariates\\n&quot;, nlyr(cov.dat))) # Display covariate names in a table cov.dat.names &lt;- matrix(names(cov.dat), ncol = 3, byrow = TRUE) knitr::kable(cov.dat.names, format = &quot;markdown&quot;, caption = &quot;Environmental covariates&quot;)  Note: Initial number of environmental covariates: 68 2.4.7.1 Load and Process Soil Climate Data (Newhall) # Load soil climate data newhall &lt;- list.files(raster.path, pattern = &quot;newhall_zam_clipped.tif$&quot;, recursive = TRUE, full.names = TRUE) newhall &lt;- terra::rast(newhall) # Display variable names newhall.names &lt;- matrix(names(newhall), ncol = 3, byrow = TRUE) knitr::kable(newhall.names, format = &quot;markdown&quot;, caption = &quot;Soil climate variables&quot;) # Remove unnecessary subdivisions newhall$regimeSubdivision1 &lt;- c() newhall$regimeSubdivision2 &lt;- c() cat(sprintf(&quot;Soil climate variables: %d (after removing subdivisions)\\n&quot;, nlyr(newhall))) 2.4.7.2 Convert Categorical Variables to Dummy Variables # Convert temperatureRegime and moistureRegime to dummy variables temperatureRegime &lt;- dummies(ca.rast = newhall$temperatureRegime, preval = 1, absval = 0) moistureRegime &lt;- dummies(ca.rast = newhall$moistureRegime, preval = 1, absval = 0) # Remove original categorical layers newhall$temperatureRegime &lt;- c() newhall$moistureRegime &lt;- c() 2.4.7.3 Merge All Covariate Layers # Merge covariates and climate data cov.dat &lt;- c(cov.dat, newhall, temperatureRegime, moistureRegime) # Project if necessary if (crs(cov.dat) != epsg) { cov.dat &lt;- terra::project(cov.dat, epsg, method = &quot;near&quot;) } # Add geology if exists if (exists(&quot;geo&quot;)) { geo &lt;- rasterize(as(geo, &quot;SpatVector&quot;), cov.dat, field = &quot;GEO&quot;) geo &lt;- dummies(ca.rast = geo$GEO, preval = 1, absval = 0) cov.dat &lt;- c(cov.dat, geo) } # Add geomorphology if exists if (exists(&quot;geomorph&quot;)) { if (!inherits(geomorph, &quot;SpatRaster&quot;)) { geomorph &lt;- rasterize(as(geomorph, &quot;SpatVector&quot;), cov.dat, field = &quot;GEOMORPH&quot;) } geomorph &lt;- dummies(ca.rast = geomorph$GEOMORPH, preval = 1, absval = 0) # Ensure matching extent and resolution if (!identical(ext(cov.dat), ext(geomorph))) { geomorph &lt;- extend(geomorph, cov.dat) } if (!all(res(cov.dat) == res(geomorph))) { geomorph &lt;- resample(geomorph, cov.dat, method = &quot;near&quot;) } cov.dat &lt;- c(cov.dat, geomorph) } # Cleanup rm(newhall, geomorph, geo) gc() cat(sprintf(&quot;\\nFinal covariate stack: %d layers\\n&quot;, nlyr(cov.dat)))  Note: Stacking is the process of combining multiple spatial data layers (raster datasets) into a single multi-layer object. This is useful when multiple variables need to be analyzed together. 2.4.7.4 Crop and Save Covariates # Crop covariates to administrative boundary cov.dat &lt;- crop(cov.dat, country_boundaries, mask = TRUE, overwrite = TRUE) writeRaster(cov.dat, paste0(raster.path, &quot;cov_dat_stack.tif&quot;), overwrite = TRUE) # Reload if needed # cov.dat &lt;- rast(paste0(raster.path, &quot;cov_dat_stack.tif&quot;)) 2.5 Principal Component Analysis (PCA) PCA reduces dimensionality while retaining the most important information. Components explaining 99% of variance are kept. # Perform PCA pca &lt;- synoptReg::raster_pca(cov.dat) # Faster than terra::princomp # Get SpatRaster layers cov.dat &lt;- pca$PCA # Display summary summary(cov.dat) # Subset to main PCs (variance explained &gt;= 0.99) n_comps &lt;- first(which(pca$summaryPCA[3,] &gt; 0.99)) cov.dat &lt;- pca$PCA[[1:n_comps]] cat(sprintf(&quot;\\nRetained %d principal components (99%% variance)\\n&quot;, n_comps)) # Save PCA rasters writeRaster(cov.dat, paste0(results.path, &quot;PCA_projected.tif&quot;), overwrite = TRUE) # Cleanup rm(pca) # Reload if needed # cov.dat &lt;- rast(paste0(results.path, &quot;PCA_projected.tif&quot;)) 2.6 Organize the Sampling Universe 2.6.1 Load and Process Land Use Data # Define land-use raster path landuse &lt;- file.path(paste0(raster.path, &quot;cropland_clipped_zmb_v1_epsg_3857.tif&quot;)) # Load binary raster (1 = target land use, NA = other) crops &lt;- rast(landuse) crops &lt;- crops / crops # Normalize names(crops) &lt;- &quot;lu&quot; # Display raster info crops 2.6.2 Exclude Protected Areas if (exists(&quot;npa&quot;)) { # If npa is a shapefile crops &lt;- mask(crops, npa) # If npa is a raster (uncomment if applicable) # npa &lt;- resample(npa, crops, method = &quot;near&quot;) # crops &lt;- crops * npa } rm(npa)  Note: Use the correct operation for your npa dataset type (shapefile or raster). 2.6.3 Restrict to Accessible Slopes if (exists(&quot;slope&quot;)) { # Resample to match crops resolution slope &lt;- resample(slope, crops, method = &quot;near&quot;) crops &lt;- crops * slope } rm(slope) 2.6.4 Aggregate to 100m Resolution Resampling to 100m resolution significantly optimizes processing speed and reduces storage requirements. # Aggregate 20m pixels to 100m (5×5 grid using modal value) lu &lt;- aggregate(crops, 5, fun = modal, cores = 4, na.rm = TRUE) lu &lt;- lu / lu # Display new resolution lu 2.6.5 Save Processed Land Use writeRaster(lu, paste0(raster.path, &quot;cropland_zmb_100m_v1_epsg_3857.tif&quot;), overwrite = TRUE) # Reload if needed lu &lt;- rast(paste0(raster.path, &quot;cropland_zmb_100m_v1_epsg_3857.tif&quot;)) 2.6.6 Filter Legacy Data if (exists(&quot;legacy&quot;)) { # Keep only legacy points within land-use areas legacy$INSIDE &lt;- terra::extract(crops, legacy) %&gt;% dplyr::select(lu) legacy &lt;- legacy[!is.na(legacy$INSIDE), ] %&gt;% dplyr::select(-&quot;INSIDE&quot;) cat(sprintf(&quot;Legacy points within land-use areas: %d\\n&quot;, nrow(legacy))) } 2.7 Create Primary Sampling Units (PSU Grid) 2.7.1 Generate 2km × 2km Grid # Create grid psu_grid &lt;- st_make_grid(country_boundaries, cellsize = c(psu_size, psu_size), square = TRUE) psu_grid &lt;- st_sf(geometry = psu_grid) psu_grid$ID &lt;- 1:nrow(psu_grid) cat(sprintf(&quot;Generated %d PSU grid cells\\n&quot;, nrow(psu_grid))) 2.7.2 Trim to Country Boundaries # This operation is computationally intensive psu_grid &lt;- psu_grid[country_boundaries[1], ] cat(sprintf(&quot;PSUs within country boundaries: %d\\n&quot;, nrow(psu_grid))) 2.7.3 Save PSU Grid write_sf(psu_grid, paste0(results.path, &quot;../grid2k.shp&quot;), overwrite = TRUE) # Reload if needed psu_grid &lt;- sf::st_read(file.path(paste0(results.path, &quot;../grid2k.shp&quot;))) 2.8 Select PSUs with Sufficient Land Use Coverage 2.8.1 Calculate Land Use Percentage # Extract land-use values for each PSU extracted_values &lt;- terra::extract(lu, psu_grid) # Calculate percentage (400 pixels = 2km × 2km PSU at 100m resolution) crop_perc &lt;- extracted_values %&gt;% group_by(ID) %&gt;% summarize(crop_perc = sum(lu, na.rm = TRUE) * 100 / 400) rm(extracted_values) # Add to PSU grid psu_grid$crop_perc &lt;- crop_perc$crop_perc cat(sprintf(&quot;Crop percentage calculated for all PSUs\\n&quot;)) 2.8.2 Filter PSUs # Keep only PSUs with sufficient crop coverage psu_grid_filtered &lt;- psu_grid[psu_grid$crop_perc &gt; percent_crop, &quot;ID&quot;] cat(sprintf(&quot;PSUs with &gt;%d%% crop coverage: %d\\n&quot;, percent_crop, nrow(psu_grid_filtered))) # Save filtered grid write_sf(psu_grid_filtered, file.path(paste0(results.path, &quot;/psu_grid_counts.shp&quot;)), overwrite = TRUE) 2.9 Rasterize PSUs for Covariate Space Coverage # Create raster template at PSU resolution template &lt;- rast(vect(psu_grid_filtered), res = psu_size) template &lt;- rasterize(vect(psu_grid_filtered), template, field = &quot;ID&quot;) # Crop covariates to filtered PSUs cov.dat &lt;- crop(cov.dat, psu_grid_filtered, mask = TRUE, overwrite = TRUE) # Resample covariates to match PSU template PSU.r &lt;- resample(cov.dat, template) cat(sprintf(&quot;Covariates resampled to PSU resolution\\n&quot;))  Why this matters: Cropping and resampling ensure that spatial resolution and extent of covariates match the PSU grid, facilitating accurate clustering. 2.10 Compute Optimal Sample Size After rasterizing PSUs for Covariate Space Coverage, calculate the optimal sample size based on divergence metrics (saurette2023?). # Load optimization script source(&quot;scripts/opt_sample.R&quot;) # Prepare covariate data psu.r.df &lt;- data.frame(PSU.r) # Define parameters initial.n &lt;- 50 # Minimum sample size final.n &lt;- 3000 # Maximum sample size by.n &lt;- 25 # Increment step iters &lt;- 4 # Iterations per trial 2.10.1 Run Optimization # Calculate optimal sample size using normalized KL-divergence opt_N_fcs &lt;- opt_sample( alg = &quot;fcs&quot;, # Feature coverage sampling s_min = initial.n, s_max = final.n, s_step = by.n, s_reps = iters, covs = psu.r.df, cpus = NULL, # Use all available cores conf = 0.95 ) # Display results print(opt_N_fcs$optimal_sites) 2.10.2 Extract Optimal Sample Size # Use Kullback-Leibler Divergence (most reliable) optimal_N_KLD &lt;- opt_N_fcs$optimal_sites[1, 2] cat(sprintf(&quot;\\n✓ Optimal sample size: %d PSUs\\n&quot;, optimal_N_KLD)) # Set final PSU count n.psu &lt;- optimal_N_KLD # Expected total sampling sites (4 per PSU) cat(sprintf(&quot;Expected total target sites: %d\\n&quot;, n.psu * 4))  Note: The optimal sample size is based on Kullback-Leibler Divergence, which does not overestimate like Jensen-Shannon methods. 2.11 Covariate Space Coverage - Computing PSUs Covariate Space Coverage (CSC) sampling ensures sampling units are distributed effectively across the multidimensional space of environmental covariates. 2.11.1 Prepare Function Parameters # Convert raster to dataframe with coordinates PSU.df &lt;- as.data.frame(PSU.r, xy = TRUE) # Get covariate names covs &lt;- names(cov.dat) # Scale covariates (mean = 0, SD = 1) mygrd &lt;- data.frame(scale(PSU.df[, covs])) cat(sprintf(&quot;Prepared %d PSUs with %d covariates for clustering\\n&quot;, nrow(mygrd), ncol(mygrd))) 2.11.2 Perform K-means Clustering # With legacy data if (exists(&quot;legacy&quot;)) { # Filter legacy to PSU grid extent legacy &lt;- st_filter(legacy, psu_grid_filtered) legacy_df &lt;- st_coordinates(legacy) # Find nearest PSU for each legacy point units &lt;- numeric(nrow(legacy_df)) for (i in 1:nrow(legacy_df)) { distances &lt;- sqrt((PSU.df$x - legacy_df[i, &quot;X&quot;])^2 + (PSU.df$y - legacy_df[i, &quot;Y&quot;])^2) units[i] &lt;- which.min(distances) } # Create fixed centers from legacy points fixed &lt;- unique(data.frame(units, scale(PSU.df[, covs])[units, ])) cat(sprintf(&quot;Incorporating %d legacy points as fixed centers\\n&quot;, nrow(fixed))) # Run constrained clustering res &lt;- CSIS(fixed = fixed, nsup = n.psu, nstarts = iterations, mygrd = mygrd) } else { # Standard k-means without legacy data res &lt;- kmeans(mygrd, centers = n.psu, iter.max = 10000, nstart = 100) cat(sprintf(&quot;Running standard k-means with %d centers\\n&quot;, n.psu)) }  Note: Progress is printed as “X out of Y” for each clustering iteration. 2.11.3 Assign Clusters and Calculate Distances # Assign clusters to PSUs PSU.df$cluster &lt;- res$cluster # Compute distances to nearest cluster center D &lt;- rdist(x1 = res$centers, x2 = scale(PSU.df[, covs])) units &lt;- apply(D, MARGIN = 1, FUN = which.min) # Calculate Mean Squared Shortest Distance (MSSSD) dmin &lt;- apply(D, MARGIN = 2, min) MSSSD &lt;- mean(dmin^2) cat(sprintf(&quot;MSSSD: %.6f\\n&quot;, MSSSD)) 2.11.4 Extract Selected PSUs # Get selected PSU coordinates and covariates myCSCsample &lt;- PSU.df[units, c(&quot;x&quot;, &quot;y&quot;, covs)] # Label as legacy or new if (exists(&quot;legacy&quot;)) { myCSCsample$type &lt;- c(rep(&quot;legacy&quot;, nrow(fixed)), rep(&quot;new&quot;, length(units) - nrow(fixed))) } else { myCSCsample$type &lt;- &quot;new&quot; } # Convert to spatial object myCSCsample &lt;- myCSCsample %&gt;% st_as_sf(coords = c(&quot;x&quot;, &quot;y&quot;), crs = epsg) # Separate legacy and new points if (exists(&quot;legacy&quot;)) { legacy &lt;- myCSCsample[myCSCsample$type == &quot;legacy&quot;, ] } new &lt;- myCSCsample[myCSCsample$type == &quot;new&quot;, ] # Get PSU IDs PSUs &lt;- sf::st_intersection(psu_grid_filtered, new) %&gt;% dplyr::select(ID) # Extract target PSUs target.PSUs &lt;- psu_grid_filtered[psu_grid_filtered$ID %in% PSUs$ID, ] %&gt;% dplyr::select(ID) cat(sprintf(&quot;\\n✓ Selected %d target PSUs\\n&quot;, nrow(target.PSUs))) # Cleanup rm(new, dmin, MSSSD) 2.11.5 Visualize in Covariate Space 2.12 Compute SSUs and TSUs Now we generate Secondary Sampling Units (100m × 100m) within each PSU and Tertiary Sampling Units (20m × 20m points) within each SSU. 2.12.1 Load High-Resolution Covariates # Load SSU-level covariates (100m resolution) cov.dat.ssu &lt;- terra::rast(paste0(raster.path, &quot;hres_data/covs_stack_ssu_1ha_ZMB.tif&quot;)) cat(sprintf(&quot;Loaded %d high-resolution covariates\\n&quot;, nlyr(cov.dat.ssu))) # Display covariate names names(cov.dat.ssu) # Remove system-specific bands if needed cov.dat.ssu &lt;- subset(cov.dat.ssu, names(cov.dat.ssu)[!names(cov.dat.ssu) %in% c(&quot;sysisen_B2&quot;, &quot;sysisen_B3&quot;, &quot;sysisen_B4&quot;, &quot;sysisen_B5&quot;, &quot;sysisen_B6&quot;, &quot;sysisen_B7&quot;, &quot;sysisen_B8&quot;, &quot;sysisen_B8A&quot;, &quot;sysisen_B11&quot;, &quot;sysisen_B12&quot;)]) # Replace NA with 0 (for categorical variables) cov.dat.ssu[is.na(cov.dat.ssu)] &lt;- 0 cat(sprintf(&quot;Final SSU covariates: %d layers\\n&quot;, nlyr(cov.dat.ssu))) 2.12.2 Process Each PSU # Initialize storage lists selected_ssus &lt;- list() all_psus_tsus &lt;- list() # Main loop: Process each PSU for (psu_id in 1:nrow(target.PSUs)) { selected_psu &lt;- target.PSUs[psu_id, ] # Generate 100m × 100m SSU grid ssu_grid &lt;- st_make_grid(selected_psu, cellsize = c(ssu_size, ssu_size), square = TRUE) ssu_grid_sf &lt;- st_sf(geometry = ssu_grid) # Convert to SpatVector for extraction ssu_grid_vect &lt;- vect(ssu_grid_sf) # Extract land-use values to filter SSUs extracted_values &lt;- extract(crops, ssu_grid_vect, fun = table) # Calculate land-use percentage (25 pixels per SSU at 20m resolution) ssu_grid_sf$lu &lt;- (extracted_values[, 2] * 100) / 25 ssu_grid_sf &lt;- ssu_grid_sf[ssu_grid_sf$lu &gt; percent_crop, ] # Progress update if (psu_id %% 10 == 0 || psu_id == nrow(target.PSUs)) { cat(sprintf(&quot;\\rProgress: %.2f%% (%d out of %d)&quot;, (psu_id / nrow(target.PSUs)) * 100, psu_id, nrow(target.PSUs))) flush.console() } # Check minimum SSU requirement total_ssus &lt;- nrow(ssu_grid_sf) if (total_ssus &lt; (num_primary_ssus + num_alternative_ssus)) { warning(paste(&quot;PSU&quot;, psu_id, &quot;does not have enough SSUs. Skipping.&quot;)) next } # Extract covariate values for each SSU ssu_covariates &lt;- terra::extract(cov.dat.ssu, vect(ssu_grid_sf), df = TRUE) # Merge with SSU spatial data ssu_data &lt;- cbind(ssu_grid_sf, ssu_covariates[, -1]) ssu_data_values &lt;- st_drop_geometry(ssu_data) # Identify columns to exclude from scaling (categorical) exclude &lt;- grep(&quot;^geomorph_|^lu$&quot;, names(ssu_data_values), value = TRUE) # Separate scalable and non-scalable data to_scale &lt;- ssu_data_values[, !names(ssu_data_values) %in% exclude] to_keep &lt;- ssu_data_values[, names(ssu_data_values) %in% exclude, drop = FALSE] # Remove NA-only columns to_scale &lt;- to_scale[, colSums(!is.na(to_scale)) &gt; 0, drop = FALSE] # Identify zero-variance columns zero_variance_cols &lt;- sapply(to_scale, function(x) sd(x, na.rm = TRUE) == 0) zero_variance_cols[is.na(zero_variance_cols)] &lt;- TRUE # Scale only non-zero-variance columns scaled_part &lt;- to_scale if (any(!zero_variance_cols)) { scaled_part[, !zero_variance_cols] &lt;- scale(to_scale[, !zero_variance_cols]) } # Recombine mygrd_ssu &lt;- cbind(to_keep, scaled_part) # Perform k-means clustering for this PSU optimal_k &lt;- num_primary_ssus # 4 clusters kmeans_result &lt;- kmeans(mygrd_ssu[, -1], centers = optimal_k, iter.max = 10000, nstart = 10) ssu_data$cluster &lt;- as.factor(kmeans_result$cluster) # CSC Sampling: Select closest SSUs to cluster centers D &lt;- rdist(x1 = kmeans_result$centers, x2 = mygrd_ssu[, -1]) # Target SSUs (closest to centers) target_units &lt;- apply(D, MARGIN = 1, FUN = function(x) order(x)[1]) target_ssus &lt;- ssu_data[target_units, ] # Replacement SSUs (second closest to centers) replacement_units &lt;- apply(D, MARGIN = 1, FUN = function(x) order(x)[2]) replacement_ssus &lt;- ssu_data[replacement_units, ] # Add metadata target_ssus$SSU_Type &lt;- &quot;Target&quot; replacement_ssus$SSU_Type &lt;- &quot;Replacement&quot; target_ssus$SSU_ID &lt;- 1:nrow(target_ssus) replacement_ssus$SSU_ID &lt;- (nrow(target_ssus) + 1):(2 * nrow(target_ssus)) # Link replacements to targets by cluster replacement_ssus$replacement_for &lt;- sapply(replacement_ssus$cluster, function(cl) { matched &lt;- target_ssus$SSU_ID[target_ssus$cluster == cl] if (length(matched) &gt; 0) return(matched[1]) else return(NA) }) target_ssus$replacement_for &lt;- NA # Add PSU ID target_ssus$PSU_ID &lt;- selected_psu$ID replacement_ssus$PSU_ID &lt;- selected_psu$ID # Store SSUs selected_ssus[[psu_id]] &lt;- rbind(target_ssus, replacement_ssus) # Generate TSUs for target SSUs primary_tsus &lt;- lapply(rownames(target_ssus), function(index) { generate_tsu_points_within_ssu( ssu_grid_sf[rownames(ssu_grid_sf) == index, ], number_TSUs, index, &quot;Target&quot;, crops ) }) # Generate TSUs for replacement SSUs alternative_tsus &lt;- lapply(rownames(replacement_ssus), function(index) { generate_tsu_points_within_ssu( ssu_grid_sf[rownames(ssu_grid_sf) == index, ], number_TSUs, index, &quot;Replacement&quot;, crops ) }) # Combine all TSUs for this PSU all_psus_tsus[[psu_id]] &lt;- do.call(rbind, c(primary_tsus, alternative_tsus)) } cat(sprintf(&quot;\\n✓ Processing complete\\n&quot;)) cat(sprintf(&quot; Successful PSUs: %d\\n&quot;, length(selected_ssus))) 2.12.3 Combine SSUs and TSUs # Combine all SSUs all_ssus &lt;- do.call(rbind, selected_ssus) all_ssus &lt;- all_ssus %&gt;% mutate_at(vars(PSU_ID, SSU_ID), as.numeric) # Combine all TSUs all_tsus &lt;- do.call(rbind, all_psus_tsus) all_tsus &lt;- all_tsus %&gt;% mutate_at(vars(PSU_ID, SSU_ID), as.numeric) # Join TSU and SSU metadata all_tsus &lt;- st_join(all_tsus, all_ssus[c(&quot;PSU_ID&quot;, &quot;SSU_ID&quot;, &quot;SSU_Type&quot;, &quot;replacement_for&quot;)]) # Reorganize columns all_tsus &lt;- all_tsus %&gt;% select(PSU_ID = PSU_ID.x, SSU_ID = SSU_ID.y, SSU_Type = SSU_Type.y, Replacement_for = replacement_for, TSU_ID, geometry) # Label TSU types all_tsus$TSU_Type &lt;- &quot;Target&quot; all_tsus[all_tsus$TSU_ID &gt; 1, &quot;TSU_Type&quot;] &lt;- &quot;Alternative&quot; all_tsus$PSU_Type &lt;- &quot;Target&quot; # Final column selection all_tsus &lt;- all_tsus %&gt;% dplyr::select(&quot;PSU_ID&quot;, &quot;SSU_ID&quot;, &quot;SSU_Type&quot;, &quot;Replacement_for&quot;, &quot;TSU_ID&quot;, &quot;TSU_Type&quot;, &quot;geometry&quot;) cat(sprintf(&quot;\\n✓ Total SSUs: %d\\n&quot;, nrow(all_ssus))) cat(sprintf(&quot;✓ Total TSUs: %d\\n&quot;, nrow(all_tsus))) cat(sprintf(&quot;✓ Target sampling sites: %d\\n&quot;, sum(all_tsus$SSU_Type == &quot;Target&quot; &amp; all_tsus$TSU_Type == &quot;Target&quot;))) 2.13 Export Sampling Units 2.13.1 Create Cluster Raster # Convert cluster information to raster dfr &lt;- PSU.df[, c(&quot;x&quot;, &quot;y&quot;, &quot;cluster&quot;)] dfr$cluster &lt;- as.numeric(dfr$cluster) dfr &lt;- rasterFromXYZ(dfr) crs(dfr) &lt;- epsg # Associate clusters with PSUs PSU_cluster.id &lt;- unlist(extract(dfr, target.PSUs)) valid.PSU_clusters &lt;- target.PSUs %&gt;% mutate(cluster = extract(dfr, target.PSUs, fun = mean, na.rm = TRUE)) all.PSU_clusters &lt;- psu_grid_filtered %&gt;% mutate(cluster = extract(dfr, psu_grid_filtered, fun = mean, na.rm = TRUE)) all.PSU_clusters &lt;- na.omit(all.PSU_clusters) 2.13.2 Join Cluster Info to TSUs # Rename cluster column valid.PSU_clusters &lt;- valid.PSU_clusters %&gt;% rename(Replace_ID = cluster) # Join to TSUs all_tsus &lt;- st_join(all_tsus, valid.PSU_clusters) # Assign sampling order all_tsus &lt;- all_tsus %&gt;% group_by(PSU_ID) %&gt;% mutate(order = match(SSU_ID, unique(SSU_ID))) %&gt;% ungroup() 2.13.3 Create Site IDs # Generate unique site IDs all_tsus$site_id &lt;- paste0(ISO.code, sprintf(&quot;%04d&quot;, all_tsus$PSU_ID), &quot;-&quot;, all_tsus$SSU_ID, &quot;-&quot;, all_tsus$TSU_ID, &quot;C&quot;) # C = Cropland cat(sprintf(&quot;Created %d unique site IDs\\n&quot;, nrow(all_tsus))) 2.13.4 Export Shapefiles # Export target PSUs write_sf(valid.PSU_clusters, paste0(results.path, &quot;/PSUs_target.shp&quot;), overwrite = TRUE) # Export target TSUs write_sf(all_tsus, paste0(results.path, &quot;/TSUs_target.shp&quot;), overwrite = TRUE) # Export all PSU clusters write_sf(all.PSU_clusters, paste0(results.path, &quot;/PSU_pattern_cl.shp&quot;), overwrite = TRUE) # Export cluster raster writeRaster(dfr, paste0(results.path, &quot;/clusters.tif&quot;), overwrite = TRUE) cat(sprintf(&quot;\\n✓ All files exported successfully\\n&quot;)) cat(sprintf(&quot; Location: %s\\n&quot;, results.path)) 2.14 Compute Alternative PSUs Generate replacement PSUs from the same environmental clusters as targets. # Filter out already selected PSUs remaining_psus &lt;- all.PSU_clusters %&gt;% filter(!(ID %in% target.PSUs$ID)) # Get unique cluster IDs from targets unique_clusters &lt;- unique(valid.PSU_clusters$Replace_ID) # Initialize list for replacements replacement_psus &lt;- data.frame() # For each target cluster, select a replacement for (clust_id in unique_clusters) { # Find candidates in same cluster candidates &lt;- remaining_psus %&gt;% filter(cluster == clust_id) if (nrow(candidates) &gt; 0) { # Random selection from candidates replacement &lt;- candidates[sample(nrow(candidates), 1), ] replacement$replaces_PSU &lt;- valid.PSU_clusters$ID[ valid.PSU_clusters$Replace_ID == clust_id ][1] replacement_psus &lt;- rbind(replacement_psus, replacement) } else { warning(sprintf(&quot;No replacement found for cluster %d&quot;, clust_id)) } } cat(sprintf(&quot;\\n✓ Generated %d replacement PSUs\\n&quot;, nrow(replacement_psus))) # Export replacement PSUs write_sf(replacement_psus, paste0(results.path, &quot;/PSUs_replacements.shp&quot;), overwrite = TRUE) Note: Follow the same SSU and TSU generation process for replacement PSUs to create complete backup sampling locations. "],["merging-results.html", "Chapter 3 Merging Results from Multiple Land Uses 3.1 Assign Country-Wide Unique IDs 3.2 Export Unified Datasets 3.3 Summary Statistics 3.4 Create Distribution Maps", " Chapter 3 Merging Results from Multiple Land Uses After running the sampling design separately for cropland, grassland, and forest, combine all results into a unified dataset with country-wide unique IDs. library(data.table) # Define land uses and codes landuses &lt;- c(&quot;cropland&quot;, &quot;grassland&quot;, &quot;forest&quot;) lu_codes &lt;- c(&quot;C&quot;, &quot;G&quot;, &quot;F&quot;) # Initialize storage lists psus_target_list &lt;- list() tsus_target_list &lt;- list() psus_repl_list &lt;- list() tsus_repl_list &lt;- list() # Load all shapefiles for (i in seq_along(landuses)) { lu &lt;- landuses[i] code &lt;- lu_codes[i] # Target PSUs file_path &lt;- sprintf(&quot;results/%s/PSUs_target.shp&quot;, lu) if (file.exists(file_path)) { temp &lt;- st_read(file_path, quiet = TRUE) temp$lulc &lt;- code psus_target_list[[lu]] &lt;- temp cat(sprintf(&quot;✓ Loaded %s target PSUs (%d)\\n&quot;, lu, nrow(temp))) } # Target TSUs file_path &lt;- sprintf(&quot;results/%s/TSUs_target.shp&quot;, lu) if (file.exists(file_path)) { temp &lt;- st_read(file_path, quiet = TRUE) temp$lulc &lt;- code tsus_target_list[[lu]] &lt;- temp cat(sprintf(&quot;✓ Loaded %s target TSUs (%d)\\n&quot;, lu, nrow(temp))) } # Replacement PSUs and TSUs # (similar loading process) } # Merge using data.table (handles different column names) psus_target &lt;- st_as_sf(rbindlist(psus_target_list, fill = TRUE)) tsus_target &lt;- st_as_sf(rbindlist(tsus_target_list, fill = TRUE)) cat(sprintf(&quot;\\n=== MERGED DATASETS ===\\n&quot;)) cat(sprintf(&quot;Target PSUs: %d\\n&quot;, nrow(psus_target))) cat(sprintf(&quot;Target TSUs: %d\\n&quot;, nrow(tsus_target))) 3.1 Assign Country-Wide Unique IDs # Assign sequential IDs to target PSUs psus_target$PSU_ID_country &lt;- 1:nrow(psus_target) # Continue numbering for replacements start_id &lt;- nrow(psus_target) + 1 psus_repl$PSU_ID_country &lt;- start_id:(start_id + nrow(psus_repl) - 1) # Transfer country-wide IDs to TSUs tsus_target$PSU_temp_ID &lt;- paste0(tsus_target$PSU_ID, &quot;-&quot;, tsus_target$lulc) psus_target$PSU_temp_ID &lt;- paste0(psus_target$ID, &quot;-&quot;, psus_target$lulc) tsus_target$PSU_ID_country &lt;- psus_target$PSU_ID_country[ match(tsus_target$PSU_temp_ID, psus_target$PSU_temp_ID) ] # Create final site IDs with country-wide PSU numbers tsus_target &lt;- tsus_target %&gt;% mutate(site_id = sprintf(&quot;%s%04d-%d-%d%s&quot;, ISO.code, PSU_ID_country, SSU_ID, TSU_ID, lulc)) # Clean up tsus_target &lt;- tsus_target %&gt;% select(PSU_ID = PSU_ID_country, SSU_ID, SSU_Type, TSU_ID, TSU_Type, site_id, lulc, geometry) psus_target &lt;- psus_target %&gt;% select(PSU_ID = PSU_ID_country, lulc, geometry) cat(sprintf(&quot;\\n✓ Assigned country-wide unique IDs\\n&quot;)) 3.2 Export Unified Datasets # Export unified datasets write_sf(psus_target, &quot;results/all/all_psus_target.shp&quot;, overwrite = TRUE) write_sf(tsus_target, &quot;results/all/all_tsus_target.shp&quot;, overwrite = TRUE) write_sf(psus_repl, &quot;results/all/all_psus_replacements.shp&quot;, overwrite = TRUE) write_sf(tsus_repl, &quot;results/all/all_tsus_replacements.shp&quot;, overwrite = TRUE) cat(sprintf(&quot;\\n✓ All unified datasets exported to results/all/\\n&quot;)) 3.3 Summary Statistics # Count target sites by land use summary_lu &lt;- tsus_target %&gt;% filter(SSU_Type == &quot;Target&quot; &amp; TSU_Type == &quot;Target&quot;) %&gt;% st_drop_geometry() %&gt;% group_by(lulc) %&gt;% summarise( n_PSUs = n_distinct(PSU_ID), n_sites = n(), sites_per_PSU = n / n_distinct(PSU_ID) ) print(summary_lu) # Total counts cat(sprintf(&quot;\\n=== FINAL SAMPLING DESIGN ===\\n&quot;)) cat(sprintf(&quot;Total PSUs: %d\\n&quot;, n_distinct(tsus_target$PSU_ID))) cat(sprintf(&quot;Total target sampling sites: %d\\n&quot;, sum(tsus_target$SSU_Type == &quot;Target&quot; &amp; tsus_target$TSU_Type == &quot;Target&quot;))) cat(sprintf(&quot; - Cropland: %d\\n&quot;, sum(tsus_target$lulc == &quot;C&quot; &amp; tsus_target$SSU_Type == &quot;Target&quot; &amp; tsus_target$TSU_Type == &quot;Target&quot;))) cat(sprintf(&quot; - Grassland: %d\\n&quot;, sum(tsus_target$lulc == &quot;G&quot; &amp; tsus_target$SSU_Type == &quot;Target&quot; &amp; tsus_target$TSU_Type == &quot;Target&quot;))) cat(sprintf(&quot; - Forest: %d\\n&quot;, sum(tsus_target$lulc == &quot;F&quot; &amp; tsus_target$SSU_Type == &quot;Target&quot; &amp; tsus_target$TSU_Type == &quot;Target&quot;))) 3.4 Create Distribution Maps library(ggplot2) # Filter to target sites only target_sites &lt;- tsus_target %&gt;% filter(SSU_Type == &quot;Target&quot; &amp; TSU_Type == &quot;Target&quot;) # Load country boundaries country &lt;- st_read(&quot;shapes/roi_country_epsg_4326.shp&quot;) # Create distribution map ggplot() + geom_sf(data = country, fill = &quot;gray95&quot;, color = &quot;gray50&quot;) + geom_sf(data = target_sites, aes(color = lulc), size = 0.5, alpha = 0.6) + scale_color_manual( name = &quot;Land Use&quot;, values = c(&quot;C&quot; = &quot;#F096FF&quot;, &quot;G&quot; = &quot;#FFFF4C&quot;, &quot;F&quot; = &quot;#006400&quot;), labels = c(&quot;C&quot; = &quot;Cropland&quot;, &quot;G&quot; = &quot;Grassland&quot;, &quot;F&quot; = &quot;Forest&quot;) ) + labs(title = &quot;SoilFER Sampling Design - Final Site Distribution&quot;, subtitle = sprintf(&quot;%d sampling locations across %d PSUs&quot;, nrow(target_sites), n_distinct(target_sites$PSU_ID))) + theme_minimal() + theme(legend.position = &quot;right&quot;) ggsave(&quot;results/all/final_site_distribution.png&quot;, width = 12, height = 10, dpi = 300) cat(sprintf(&quot;\\n✓ Distribution map saved\\n&quot;)) "],["field-protocol.html", "Chapter 4 Field Sampling Protocol 4.1 GPS Device Preparation 4.2 Sampling Procedure 4.3 Data Recording 4.4 Quality Control", " Chapter 4 Field Sampling Protocol 4.1 GPS Device Preparation 4.1.1 Export to GPX Format # Load target TSUs tsus_field &lt;- st_read(&quot;results/all/all_tsus_target.shp&quot;) # Filter to primary targets only (1 per SSU) tsus_field &lt;- tsus_field %&gt;% filter(SSU_Type == &quot;Target&quot; &amp; TSU_Type == &quot;Target&quot;) # Transform to WGS84 (required for GPS) tsus_field_wgs84 &lt;- st_transform(tsus_field, crs = 4326) # Export to GPX st_write(tsus_field_wgs84, &quot;results/all/field_sampling_points.gpx&quot;, driver = &quot;GPX&quot;, delete_dsn = TRUE) cat(sprintf(&quot;✓ Exported %d waypoints to GPX\\n&quot;, nrow(tsus_field_wgs84))) 4.1.2 Load onto GPS Device Connect GPS device to computer Copy field_sampling_points.gpx to GPS memory Load waypoints in GPS software Waypoint names will be site_id values (e.g., KEN0001-1-1C) 4.2 Sampling Procedure For each PSU (in order of PSU_ID): Navigate to TSU 1 (SSU_ID=1, TSU_ID=1) If accessible → Sample at exact GPS location If inaccessible (physical barrier) → Use TSU 2 (Alternative 1) or TSU 3 (Alternative 2) Navigate to TSU 2 (SSU_ID=2, TSU_ID=1) Repeat accessibility check and sampling Navigate to TSU 3 (SSU_ID=3, TSU_ID=1) Navigate to TSU 4 (SSU_ID=4, TSU_ID=1) If entire PSU is inaccessible: - Use replacement_PSU_ID to find backup PSU - Navigate to replacement PSU location - Sample at replacement TSUs (SSU_ID = 5, 6, 7, 8) 4.3 Data Recording Field form should capture (Table 4.1): Table 4.1: Required field data recording Field Example Notes site_id KEN0001-1-1C CRITICAL - Unique identifier date 2025-01-15 Date of sampling team Team A Field team name coordinates_actual -1.2345, 36.7890 Actual GPS coordinates accessibility Target / Alt1 / Alt2 Which TSU was sampled land_use_observed Maize field Visual confirmation soil_depth 45 cm Depth to restrictive layer photos KEN0001-1-1C_001.jpg Geotagged photos notes Rocky, 15% slope Relevant observations 4.4 Quality Control During field work: - ✅ Verify land use matches expected type - ✅ Document if alternative TSU used (and reason) - ✅ Take geotagged photos at each site - ✅ Record actual GPS coordinates - ✅ Note any anomalies or challenges Post-field QC: - Check for duplicate site_id entries - Verify all mandatory fields completed - Cross-reference photos with field records - Flag sites with large GPS deviation (&gt;30m from waypoint) "],["references-1.html", "References", " References "],["appendices.html", "Appendices Appendix A: Software and Data Sources Appendix B: Troubleshooting Common Issues Appendix C: Acronyms and Abbreviations", " Appendices Appendix A: Software and Data Sources 4.4.1 Software Requirements R (version ≥ 4.0): https://www.r-project.org/ RStudio: https://posit.co/products/open-source/rstudio/ Google Earth Engine: https://earthengine.google.com/ QGIS (optional): https://qgis.org/ 4.4.2 Data Sources Global Datasets: - CHELSA Climate: https://chelsa-climate.org/ - TerraClimate: https://www.climatologylab.org/terraclimate.html - MODIS: https://lpdaac.usgs.gov/products/mod13q1v006/ - Sentinel-2: https://scihub.copernicus.eu/ - SRTM DEM: https://earthexplorer.usgs.gov/ - ESA WorldCover: https://esa-worldcover.org/ - SoilGrids: https://soilgrids.org/ - Geomorpho90m: https://www.geomorpho90m.org/ Administrative Boundaries: - GADM: https://gadm.org/ - Natural Earth: https://www.naturalearthdata.com/ Appendix B: Troubleshooting Common Issues 4.4.3 Memory Issues Problem: R runs out of memory during processing Solution: # Increase memory limit (Windows) memory.limit(size = 32000) # 32 GB # Use terra instead of raster (more efficient) # Process data in chunks # Close unnecessary applications 4.4.4 CRS Misalignment Problem: Spatial layers don’t overlap correctly Solution: # Always check CRS sf::st_crs(your_data) terra::crs(your_raster) # Reproject if needed your_data &lt;- sf::st_transform(your_data, crs = target_crs) your_raster &lt;- terra::project(your_raster, target_crs) 4.4.5 Empty Geometry Problem: SSU or TSU generation fails Solution: # Check for valid geometries st_is_valid(your_sf_object) # Fix invalid geometries your_sf_object &lt;- st_make_valid(your_sf_object) # Remove empty geometries your_sf_object &lt;- your_sf_object[!st_is_empty(your_sf_object), ] Appendix C: Acronyms and Abbreviations Table 4.2: List of acronyms and abbreviations used in this manual Acronym Definition AOI Area of Interest AWC Available Water Capacity BAS Balanced Acceptance Sampling CHELSA Climatologies at High resolution for the Earth’s Land Surface Areas CLHS Conditioned Latin Hypercube Sampling CRS Coordinate Reference System CSC Covariate Space Coverage CSCS Covariate Space Coverage Sampling DEM Digital Elevation Model DSM Digital Soil Mapping EVI Enhanced Vegetation Index FAO Food and Agriculture Organization FPAR Fraction of Photosynthetically Active Radiation GEE Google Earth Engine GRTS Generalized Random Tessellation Stratified ISO International Organization for Standardization KLD Kullback-Leibler Divergence JSD Jensen-Shannon Divergence LULC Land Use and Land Cover MAST Mean Annual Soil Temperature MODIS Moderate Resolution Imaging Spectroradiometer MSSD Mean Squared Shortest Distance MSSSD Mean Squared Shortest Scaled Distance NDVI Normalized Difference Vegetation Index NSM Normalized Soil Moisture PCA Principal Component Analysis PC Principal Component PET Potential Evapotranspiration PSU Primary Sampling Unit ROI Region of Interest SoilFER Soil Mapping for Resilient Agrifood Systems SRS Simple Random Sampling SRTM Shuttle Radar Topography Mission SSU Secondary Sampling Unit SWIR Shortwave Infrared TPI Topographic Position Index TSU Tertiary Sampling Unit TWI Topographic Wetness Index VACS Vision for Adapted Crops and Soils For questions, technical support, or to report issues, please visit the SoilFER GitHub repository or contact the SoilFER team at FAO. "],["tbd.html", "Chapter 5 TBD", " Chapter 5 TBD "],["soil-data-preparation.html", "Chapter 6 Soil Data Preparation 6.1 Introduction 6.2 Data cleaning 6.3 Soil Data Harmonization 6.4 Concepts on digital exchange of soil-related data (ISO 28258) 6.5 Plot data 6.6 Profile data 6.7 Element data 6.8 Specimen data 6.9 Definition of laboratory procedures 6.10 Data transformation 6.11 Organizing soil data according to the GloSIS database 6.12 Tools for harmonization of soil databases within GloSIS", " Chapter 6 Soil Data Preparation 6.1 Introduction 6.2 Data cleaning 6.3 Soil Data Harmonization 6.4 Concepts on digital exchange of soil-related data (ISO 28258) 6.5 Plot data 6.6 Profile data 6.7 Element data 6.8 Specimen data 6.9 Definition of laboratory procedures 6.10 Data transformation 6.11 Organizing soil data according to the GloSIS database 6.12 Tools for harmonization of soil databases within GloSIS "],["digital-soil-mapping-and-modeling.html", "Chapter 7 Digital Soil Mapping and Modeling Part I: Theoretical Foundations 7.1 Fundamentals of Digital Soil Mapping Part II: Practical Application 7.2 Workflow Overview 7.3 Continuous Modelling (pH) 7.4 Nominal Classification (Clay_pH_Class) 7.5 Ordinal Modelling (pH_Class) 7.6 Area of Applicability (AOA) 7.7 Summary References", " Chapter 7 Digital Soil Mapping and Modeling Part I: Theoretical Foundations 7.1 Fundamentals of Digital Soil Mapping 7.1.1 Introduction to Digital Soil Mapping Digital soil mapping (DSM) is a modern approach that uses quantitative methods to create soil maps. It combines field observations, laboratory data, and environmental information within a spatial framework (Lagacherie, 2008). DSM represents a significant change from traditional soil survey, where maps were created based mainly on expert knowledge and manual interpretation of landscapes. Traditional soil survey relied on a soil surveyor’s conceptual model of the landscape. Surveyors used aerial photographs, satellite images, and field observations to identify soil patterns. However, this approach had important limitations: maps were subjective, difficult to update, and lacked quantitative measures of uncertainty (Minasny and McBratney, 2016). The evolution towards digital methods began in the 1970s when researchers started using computers to store and analyse soil data. Webster, Lessells and Hodgson (1979) demonstrated early digital soil cartography in England, while soil information systems were developed to manage soil databases. During the 1980s, the development of geographic information systems (GIS) and geostatistics provided new tools for spatial analysis. The 1990s brought advances in digital terrain modelling, data mining, and machine learning methods such as classification trees and neural networks. The success of DSM in the early 2000s resulted from several factors: increased availability of digital elevation models and satellite imagery, greater computing power, development of data mining tools, and a growing global demand for soil information with uncertainty estimates (Minasny and McBratney, 2016). Universities and research centres began producing soil maps that were previously only made by national survey agencies. Today, DSM is defined as the creation of spatial soil information systems using field and laboratory observations combined with spatial inference methods (Lagacherie and McBratney, 2006). The theoretical framework for DSM was formalised in 2003, establishing the conceptual basis that guides current practice. 7.1.2 The SCORPAN Framework The conceptual and methodological foundations of DSM were formalised by the scorpan framework (McBratney, Mendonça Santos and Minasny, 2003). This model represents a quantitative expansion of the classical factors of soil formation originally proposed by Dokuchaev in 1883 and later refined by Jenny (1941). While Jenny’s clorpt model (climate, organisms, relief, parent material, time) was developed to explain soil genesis, the scorpan framework transforms this explanatory approach into an empirical predictive tool for spatial mapping. The fundamental distinction between the two frameworks lies in their purpose. Jenny’s model describes how soils form through the interaction of environmental factors over time—it is a qualitative, theoretical framework. The scorpan approach, in contrast, does not attempt to explain soil formation mechanistically but rather exploits empirical relationships between soil and environmental variables for prediction (McBratney, Mendonça Santos and Minasny, 2003). As noted by the authors, the direction of causality is not considered: where evidence of a relationship exists, it can be used for prediction regardless of whether the factor causes or is caused by soil properties. The scorpan acronym identifies seven environmental factors that influence soil distribution: s: soil—other properties of the soil at a point, including information from prior maps, proximal or remote sensing, or expert knowledge c: climate—properties of the environment such as precipitation, temperature, and evapotranspiration o: organisms—vegetation, fauna, or human activity, often represented by land cover or spectral indices r: relief—topographic attributes derived from digital elevation models p: parent material—lithology and geological substrate a: age—the time factor representing soil development duration or geomorphic surface age n: space—spatial position expressed as geographic coordinates Two additional factors distinguish scorpan from Jenny’s original formulation. First, soil itself (s) is included as a predictor because soil properties can be predicted from other soil attributes measured at the same location or from existing soil maps. Second, the spatial factor (n) explicitly incorporates geographic position, which captures spatial trends and autocorrelation not explained by the other environmental factors (McBratney, Mendonça Santos and Minasny, 2003). The spatial coordinates can be used directly or transformed into derived variables such as distance to coast or distance from discharge areas. The general pedometric model is formulated as: \\[S_c = f(s, c, o, r, p, a, n) + \\varepsilon\\] or for soil attributes: \\[S_a = f(s, c, o, r, p, a, n) + \\varepsilon\\] This can be expressed more compactly as: \\[S = f(Q) + \\varepsilon\\] where \\(S\\) denotes a soil class (\\(S_c\\)) or attribute (\\(S_a\\)) at a specific location, \\(f(Q)\\) represents a deterministic function of the scorpan covariates, and \\(\\varepsilon\\) represents the spatially autocorrelated residual. The complete methodological approach is termed scorpan-SSPFe (Soil Spatial Prediction Function with spatially autocorrelated errors), where deterministic predictions from \\(f(Q)\\) are complemented by geostatistical modelling of residuals to account for spatial structure not captured by the covariates (McBratney, Mendonça Santos and Minasny, 2003). Each scorpan factor is represented by one or more environmental covariates. For example, climate (c) may be represented by mean annual precipitation and temperature; relief (r) by slope, aspect, curvature, and terrain wetness index; and organisms (o) by satellite-derived vegetation indices such as NDVI. The selection of covariates depends on data availability, mapping scale, and the soil property being predicted. The scorpan-SSPFe approach represents a paradigm shift in soil mapping (McBratney, Mendonça Santos and Minasny, 2003). While the conventional Jenny model follows a deductive-nomological framework, the scorpan approach follows an inductive-statistical model of explanation. Both frameworks share the same ontological basis—soil as a function of environment—but differ fundamentally in methodology and apparatus. The scorpan approach requires digital data, computing resources, and statistical methods for fitting \\(f()\\), whereas traditional mapping relies primarily on expert mental models and field interpretation. 7.1.3 Statistical Theory for Predictive Soil Mapping The spatial heterogeneity of soil properties results from complex interactions among soil-forming factors that are often only partially characterised. While the deterministic influences of climate, organisms, relief, parent material, and time are well-recognised, their synergistic interaction over pedogenic timescales presents significant challenges for mechanistic modelling (Heuvelink and Webster, 2001). Soil varies continuously in space and time, and any description of this variation is inevitably incomplete. Soil scientists must therefore represent this variation using models that combine deterministic and stochastic components (Heuvelink and Webster, 2001). 7.1.3.1 Mechanistic versus Empirical Approaches Mechanistic modelling (also referred to as process-based or white-box modelling) involves the mathematical representation of soil systems based on known physical, chemical, and biological processes. Unlike empirical models that rely on statistical correlations, mechanistic models attempt to simulate the actual mechanisms of soil formation and function—such as water movement, solute transport, organic matter decomposition, and soil erosion—over time and space (Wadoux, Minasny and McBratney, 2021). These models typically use systems of differential equations to describe the state of the soil system and its evolution. Wadoux, Minasny and McBratney (2021) distinguish between mechanistic and functional (empirical) models for soil mapping. Mechanistic models have structures based on mechanisms derived from knowledge of physical processes and soil chemical and biological reactions. Examples include soil erosion and deposition models, soil-landscape evolution models, and soil carbon dynamics models. However, the main problems with mechanistic approaches are that they require adequate mechanistic understanding of major soil processes, need extensive input data that are often unavailable, have many parameters that are difficult to infer, are computationally challenging, and typically do not quantify prediction uncertainty. Although advancements have been made in modelling vertical soil variation through process-based approaches, these methods remain primarily experimental and are not yet suitable for large-scale operational soil mapping (Zhang et al., 2024). Consequently, most operational DSM follows an empirical approach. Instead of simulating every physical process, empirical models use statistical relationships between soil properties and environmental covariates to make predictions (McBratney, Mendonça Santos and Minasny, 2003). Recent research has explored hybrid approaches that integrate process-oriented (PO) and machine learning (ML) models. Zhang et al. (2024) proposed a framework where PO model outputs serve as additional covariates for ML models, combining the ability of PO models to capture temporal dynamics with the spatial prediction accuracy of ML models. This integration represents a promising avenue for dynamic soil mapping that addresses one of the ten challenges for the future of pedometrics (Wadoux, Minasny and McBratney, 2021). 7.1.3.2 Sources of Residual Variance Regression models frequently account for only a portion of the total soil variance. This limitation arises from several factors (Hengl, Heuvelink and Rossiter, 2007; Wadoux, Minasny and McBratney, 2021): Model structure limitations: The deterministic model structure may fail to represent actual mechanistic pedogenic processes adequately. Missing causal factors: Models often exclude significant causal factors that influence soil variation. Covariate limitations: Environmental covariates serve as incomplete proxies for true soil-forming factors (see section below). Measurement errors: Covariates contain inherent measurement errors or suffer from scale (support) mismatches relative to soil observations. Spatial scale effects: Soil properties vary at spatial scales from the atomic to the global, and the factors causing variation at one scale may differ from those at other scales (Wadoux, Minasny and McBratney, 2021). Given these constraints, soil spatial models often exhibit substantial residual variance. When these residuals demonstrate spatial autocorrelation—quantifiable through variogram analysis—the application of kriging techniques to the residuals can significantly improve prediction accuracy (Hengl, Heuvelink and Rossiter, 2007; McBratney, Mendonça Santos and Minasny, 2003). 7.1.3.3 Covariates as Proxies of Soil-Forming Factors In DSM, we use the terms proxy and covariate when referring to environmental data layers used to predict soil properties. A proxy is an indirect measurement of a variable that is difficult to measure directly. Since we cannot measure the exact historical climate or the precise biological activity that has occurred at every location over millennia, we use available data that correlates with these processes (McBratney, Mendonça Santos and Minasny, 2003). For example, a Normalized Difference Vegetation Index (NDVI) map derived from satellite imagery is not the “organisms” factor itself but serves as a proxy for vegetation biomass and productivity, which influences organic matter inputs to the soil. While the scorpan framework is based on soil-forming factors, the digital layers we input into models are mathematical representations rather than the physical factors themselves. There are four primary reasons for this distinction (Kempen et al., 2009; Lagacherie, 2008): Temporal mismatch: Soil formation occurs over centuries or millennia, but most climate covariates are based on the last 30 to 50 years of data. These represent proxies for the long-term climate that actually formed the soil. Information loss and simplification: A Digital Elevation Model represents relief, but it is a grid of numbers representing average heights within pixels. It misses fine-scale terrain nuances that influence water flow and erosion. Measurement error: Every digital layer contains inherent errors. Satellite sensors have noise, and climate station data are interpolated across spatial gaps. Indirect correlation: Often, a covariate represents multiple factors simultaneously. Elevation (relief) correlates strongly with temperature (climate), making it a statistical surrogate rather than a pure physical factor. 7.1.3.4 The Universal Model of Soil Variation In DSM, we mathematically decompose soil variation into distinct components to better understand and predict soil patterns. This decomposition follows the Universal Model of Soil Variation (Heuvelink and Webster, 2001; Webster and Oliver, 2007): \\[Z(s) = m(s) + \\varepsilon&#39;(s) + \\varepsilon&#39;&#39;(s)\\] Where: \\(Z(s)\\): The value of a soil property at a specific location (\\(s\\)) \\(m(s)\\): The deterministic component (trend)—the part of soil variation explained using environmental covariates \\(\\varepsilon&#39;(s)\\): The spatially correlated stochastic component—variation that follows a spatial pattern but is not captured by covariates \\(\\varepsilon&#39;&#39;(s)\\): The pure noise—measurement errors and fine-scale variation This decomposition provides the theoretical foundation for hybrid interpolation methods such as regression-kriging, where the trend \\(m(s)\\) is modelled as a function of environmental covariates and the spatially correlated residual \\(\\varepsilon&#39;(s)\\) is interpolated using kriging (Hengl, Heuvelink and Rossiter, 2007; Odeh, McBratney and Chittleborough, 1995). Hengl, Heuvelink and Rossiter (2007) demonstrated that regression-kriging explicitly separates trend estimation from residual interpolation, allowing the use of arbitrarily complex regression forms. The method can be expressed as: \\[\\hat{Z}(s_0) = \\hat{m}(s_0) + \\hat{\\varepsilon}&#39;(s_0)\\] where \\(\\hat{m}(s_0)\\) is predicted from the regression model and \\(\\hat{\\varepsilon}&#39;(s_0)\\) is obtained by kriging the regression residuals. 7.1.3.5 Extending the Model: Space, Depth, and Time (3D+T) While a 2D map describes soil variation at a single depth layer, soils are three-dimensional bodies that change over time. The Universal Model can be generalised to include depth (\\(d\\)) and time (\\(t\\)) (Heuvelink and Webster, 2001): \\[Z(s, d, t) = m(s, d, t) + \\varepsilon&#39;(s, d, t) + \\varepsilon&#39;&#39;(s, d, t)\\] This 3D+T (spatio-temporal) model allows tracking how soil properties change with depth and evolve over time. Heuvelink et al. (2016) noted that 3D kriging, where depth is treated as a third dimension, has important advantages because predictions at any depth interval can be made. However, modelling vertical variation realistically is challenging due to zonal and geometric anisotropies and discontinuities at horizon boundaries. Soil observations are typically averages over depth intervals and cannot be treated as vertical points. Mass-preserving splines have been developed to address this issue (Bishop, McBratney and Laslett, 1999; Orton et al., 2014), though they introduce additional uncertainties. Example: Weekly Soil Moisture Mapping A classic example of the need for spatio-temporal modelling is soil moisture. Unlike soil texture (which changes very slowly over decades), soil moisture fluctuates daily or weekly based on rainfall and evaporation. Creating weekly maps of soil moisture requires a space-time framework considering location (\\(s\\)), time (\\(t\\)), and depth (\\(d\\)) simultaneously. However, moving from 2D to 3D+T significantly increases analytical complexity. Each additional dimension requires more model parameters to be estimated, and the data requirements increase substantially. 2D+T and 3D+T models: Because data requirements for 2D+T and 3D+T models are high, they remain experimental in many regions. For most projects, focusing on high-quality 2D or 3D (depth-only) mapping is the standard starting point. Recent developments in integrating process-oriented models with machine learning offer promising approaches for capturing temporal dynamics while maintaining spatial prediction accuracy (Zhang et al., 2024). 7.1.4 Types of Soil Variables Soil variables are classified into two fundamental categories based on their mathematical nature, and this distinction has important implications for the choice of prediction methods (McBratney, Mendonça Santos and Minasny, 2003): Continuous Variables (Soil Properties): These are attributes measured on a numerical scale that can take any value within a range. Common examples include clay content (%), soil organic carbon concentration (g kg⁻¹), pH, bulk density (g cm⁻³), and cation exchange capacity (cmol kg⁻¹). From a pedological perspective, these properties typically change gradually across space, reflecting the continuous nature of soil variation (Heuvelink and Webster, 2001). However, some properties may exhibit abrupt changes at lithological boundaries or landscape discontinuities. Categorical Variables (Soil Classes): These represent qualitative groups or discrete soil attributes. Examples include soil taxonomic units (e.g., Mollisol, Alfisol), drainage classes, or horizon designations. Traditional soil classification follows a “top-down” model where soils are divided into mutually exclusive classes with sharp conceptual boundaries (Odgers, McBratney and Minasny, 2011). However, the intrinsic variability of soil means that soil usually does not exist as discrete bodies with sharp boundaries between them—either physical boundaries in the natural landscape or conceptual boundaries in the feature space (Heuvelink and Webster, 2001; Odgers, McBratney and Minasny, 2011). The Soil Continuum Problem: In reality, soil grades more or less gradually from one class to another, both in the landscape and in feature space (Odgers, McBratney and Minasny, 2011). This has led to the development of continuous classification approaches using fuzzy set theory, where objects can belong to multiple classes with membership values that sum to one (Heuvelink and Webster, 2001). Fuzzy classification provides a more realistic representation of soil variation by quantifying the degree of similarity between soil profiles and class centroids. 7.1.5 Prediction Methods The mathematical nature of soil variables determines the type of statistical method used to create predictive maps (McBratney, Mendonça Santos and Minasny, 2003): Regression for Continuous Properties: For continuous variables, regression algorithms establish numerical relationships between soil properties and environmental covariates. Methods range from linear approaches (ordinary least squares, generalised linear models) to non-linear techniques (generalised additive models, regression trees, neural networks, and ensemble methods such as Random Forest). The advantage of tree-based methods over linear models is their ability to handle nonlinearity and non-additive behaviour without requiring interactions to be pre-specified (Breiman et al., 1984; McBratney, Mendonça Santos and Minasny, 2003). Classification for Soil Classes: For categorical variables, classification algorithms calculate the probability of a location belonging to specific soil classes. Methods include discriminant analysis, logistic regression, classification trees, and machine learning classifiers. Rather than producing a single “hard” classification, modern approaches often output class membership probabilities for each location, providing a richer representation of prediction uncertainty (Kempen et al., 2009; McBratney, Mendonça Santos and Minasny, 2003). Hybrid Approaches: Some methods can handle both continuous and categorical data simultaneously. Classification and regression trees (CART) exemplify this flexibility, automatically determining splitting variables and points based on the data structure (Breiman et al., 1984). Tree-based models are particularly valued for their interpretability compared to methods like neural networks or generalised additive models (McBratney, Mendonça Santos and Minasny, 2003). 7.1.6 Nonlinearity and Uncertainty A fundamental characteristic of soil-environment relationships is that they are often non-linear and complex (Minasny and McBratney, 2016). Environmental factors do not always influence soil properties in simple, linear ways. Terrain attributes may have threshold effects, climate interactions may be multiplicative, and biological processes introduce additional complexity. Because predictive models are simplified representations of natural processes, they are never fully accurate. Unlike traditional soil maps that provide a single deterministic view, digital soil mapping enables the quantification of prediction uncertainty. This means that for every prediction, we can provide measures of confidence indicating where the model is reliable and where additional data might be needed (Padarian and McBratney, 2023; Styc and Lagacherie, 2021). Uncertainty can be expressed through: Prediction intervals: Upper and lower bounds within which the true value is expected to fall with a specified probability (e.g., 90% confidence interval) Standard deviation or variance: Measures of prediction spread around the mean estimate Prediction interval coverage probability (PICP): Assessment of whether stated confidence intervals actually contain the expected proportion of true values (Styc and Lagacherie, 2021) However, Padarian and McBratney (2023) noted that uncertainty estimates remain underutilised in practice. Around 50% of DSM studies still do not report uncertainty assessments, and end users often find uncertainty maps difficult to interpret alongside target variable maps. This challenge has motivated new approaches for communicating uncertainty, including variable-resolution maps where pixel size encodes prediction confidence. Part II: Practical Application 7.2 Workflow Overview This section guides you through the interpretation of results from the Digital Soil Mapping workflow. We apply the theoretical concepts from Part I to predict three types of soil variables in Kansas, USA, each requiring a different modelling approach: Variable type Example Approach What the map shows Continuous pH (numeric scale) Quantile Regression Forest Predicted value + uncertainty at each pixel Nominal Clay_pH_Class (unordered categories) Classification Random Forest Most likely class + class probabilities Ordinal pH_Class (ordered categories) Integer-encoded Regression Predicted class + instability near boundaries The type of soil variable determines model choice, validation metrics, and how we interpret uncertainty. The workflow also demonstrates the Area of Applicability (AOA) analysis to identify where predictions can be trusted. # ── Check and load required packages ── required_pkgs &lt;- c(&quot;tidyverse&quot;, &quot;caret&quot;, &quot;knitr&quot;) missing_pkgs &lt;- required_pkgs[!sapply(required_pkgs, requireNamespace, quietly = TRUE)] if (length(missing_pkgs) &gt; 0) { message(&quot;Installing missing packages: &quot;, paste(missing_pkgs, collapse = &quot;, &quot;)) install.packages(missing_pkgs, repos = &quot;https://cloud.r-project.org&quot;) } library(tidyverse) library(caret) library(knitr) 7.3 Continuous Modelling (pH) 7.3.1 Nature of the Variable Soil pH is a continuous variable: it takes numeric values on a scale (typically 4–9 in mineral soils). We predict the actual pH value at each location rather than placing observations into categories. Following the scorpan framework, we model pH as a function of environmental covariates that represent climate, organisms, relief, and other soil-forming factors. The relationship is empirical—we do not simulate the chemical processes that determine pH, but rather exploit correlations between pH and measurable environmental variables. What this approach produces: A map of predicted pH values across the study area An uncertainty map (standard deviation) showing where predictions are more or less reliable Validation metrics that measure prediction error in pH units What this approach cannot do: It cannot tell you why pH varies (only which covariates correlate with it) It does not guarantee accurate predictions in areas unlike the training data It does not account for temporal changes in pH 7.3.2 Data and Covariates The workflow begins by loading environmental covariates representing the scorpan factors: terrain attributes derived from digital elevation models (r), climate variables (c), and remote sensing indices representing vegetation (o). Figure 7.1: Example environmental covariate layer from the 250m resolution stack. Each covariate layer represents one environmental variable at 100m resolution. These are proxies for the true soil-forming factors—they correlate with soil pH but do not directly represent the chemical processes involved. The model learns which covariates best explain pH variation from the training points, then applies that relationship to predict pH across the entire area. 7.3.3 Feature Selection (Boruta) Not all covariates contribute equally to prediction. Some may be irrelevant, redundant, or introduce noise. Boruta is an algorithm that identifies which covariates genuinely help predict pH by comparing each variable’s importance against randomly shuffled “shadow” versions of itself (Kursa and Rudnicki, 2010). selected_features &lt;- readRDS(file.path(OUTPUTS_DIR, &quot;models&quot;, &quot;selected_features_pH.rds&quot;)) print(paste(&quot;Number of selected features:&quot;, length(selected_features))) ## [1] &quot;Number of selected features: 29&quot; print(&quot;Selected features:&quot;) ## [1] &quot;Selected features:&quot; print(selected_features) ## [1] &quot;bio1&quot; &quot;bio12&quot; &quot;bio13&quot; ## [4] &quot;bio14&quot; &quot;bio16&quot; &quot;bio17&quot; ## [7] &quot;bio5&quot; &quot;bio6&quot; &quot;ngd10&quot; ## [10] &quot;pet_penman_max&quot; &quot;pet_penman_mean&quot; &quot;pet_penman_min&quot; ## [13] &quot;pet_penman_range&quot; &quot;sfcWind_max&quot; &quot;sfcWind_mean&quot; ## [16] &quot;sfcWind_range&quot; &quot;fpar_030405_500m_mean&quot; &quot;fpar_030405_500m_sd&quot; ## [19] &quot;fpar_060708_500m_mean&quot; &quot;fpar_060708_500m_sd&quot; &quot;fpar_091011_500m_mean&quot; ## [22] &quot;fpar_091011_500m_sd&quot; &quot;fpar_120102_500m_mean&quot; &quot;fpar_120102_500m_sd&quot; ## [25] &quot;dtm_curvature_250m&quot; &quot;dtm_elevation_250m&quot; &quot;dtm_slope_250m&quot; ## [28] &quot;dtm_tpi_250m&quot; &quot;dtm_twi_500m&quot; Figure 7.2: Boruta feature selection results. Green = confirmed important, red = rejected, yellow = tentative. How to read this plot: Green boxes: Variables confirmed as important. Their importance exceeds random noise. Red boxes: Variables rejected. They do not help more than randomly shuffled data. Yellow boxes: Tentative (borderline). We include them to avoid discarding potentially useful information. Check if the selected features make pedological sense. For pH, you might expect climate (precipitation, temperature) and terrain (slope position, wetness) to be important because they influence weathering and drainage. 7.3.4 Model Training We use Random Forest to predict pH. Random Forest is an ensemble method that builds many decision trees, each trained on a random subset of the data, and averages their predictions (Breiman, 2001). This approach aligns with the empirical tradition in DSM—we fit the function \\(f(Q)\\) from the scorpan model using data-driven methods rather than process-based equations. Why Random Forest for DSM: It handles nonlinear relationships between covariates and soil properties automatically It captures interactions between covariates without requiring explicit specification It does not require the analyst to specify the functional form of relationships It provides measures of variable importance for interpretation What Random Forest does NOT solve: It cannot extrapolate beyond the range of covariate values in training data It does not account for spatial autocorrelation in model fitting (the \\(\\varepsilon&#39;(s)\\) term from the Universal Model) It may underperform if training data are spatially clustered or biased toward certain environments model_cont &lt;- readRDS(file.path(OUTPUTS_DIR, &quot;models&quot;, &quot;ranger_model_pH.rds&quot;)) # Show model summary print(model_cont) ## Random Forest ## ## 424 samples ## 29 predictor ## ## No pre-processing ## Resampling: Cross-Validated (10 fold, repeated 5 times) ## Summary of sample sizes: 381, 382, 380, 383, 381, 381, ... ## Resampling results across tuning parameters: ## ## mtry splitrule RMSE Rsquared MAE ## 5 variance 0.6956261 0.3833561 0.5501236 ## 5 extratrees 0.6884646 0.3947113 0.5477938 ## 10 variance 0.6980853 0.3798045 0.5522413 ## 10 extratrees 0.6883220 0.3947413 0.5471498 ## 15 variance 0.6992832 0.3777471 0.5527784 ## 15 extratrees 0.6909598 0.3901496 0.5481261 ## ## Tuning parameter &#39;min.node.size&#39; was held constant at a value of 5 ## RMSE was used to select the optimal model using the smallest value. ## The final values used for the model were mtry = 10, splitrule = extratrees ## and min.node.size = 5. # Best hyperparameters print(&quot;Best hyperparameters:&quot;) ## [1] &quot;Best hyperparameters:&quot; print(model_cont$bestTune) ## mtry splitrule min.node.size ## 4 10 extratrees 5 The model was trained using 10-fold cross-validation repeated 5 times. The data were split into 10 parts; the model was trained on 9 parts and tested on the held-out part, rotating through all combinations. This process was repeated 5 times with different random splits. What cross-validation evaluates: How well the model predicts pH for observations not used in training. What cross-validation does NOT evaluate: Whether predictions are reliable in geographic areas far from training points. This is a fundamental limitation—cross-validation assumes that held-out observations are representative of prediction locations. If your training points are spatially clustered, the model may perform worse in unsampled regions than cross-validation suggests (Roberts et al., 2017). 7.3.5 Variable Importance Figure 7.3: Variable importance from the trained Random Forest model. This plot ranks covariates by how much they contribute to predictions. Variables at the top reduce prediction error the most when used in tree splits. Variable importance helps link empirical predictions back to pedological understanding. If a climate variable ranks highest, soil pH variation may be driven primarily by regional precipitation or temperature gradients. If a terrain variable ranks highest, local topographic position and drainage may dominate. However, importance does not imply causation—it reflects statistical association, not mechanistic influence. 7.3.6 Validation Metrics metrics &lt;- read_csv(file.path(OUTPUTS_DIR, &quot;validation&quot;, &quot;metrics_pH.csv&quot;), show_col_types = FALSE) kable(metrics, digits = 3, caption = &quot;Cross-validation metrics for pH prediction&quot;) Table 7.1: Cross-validation metrics for pH prediction Variable ME RMSE R2 NSE pH -0.01 0.691 0.383 0.383 What each metric tells you: Metric Meaning Interpretation ME (Mean Error) Average difference between predicted and observed. Values near 0 indicate no systematic bias. Positive ME = model overpredicts on average. RMSE Root Mean Square Error—typical size of prediction errors in pH units. Compare to the range of pH in your data. RMSE of 0.5 in data ranging 5–8 is different from RMSE of 0.5 in data ranging 6–7. R² Proportion of variance explained by the model. R² = 0.32 means the model captures about one-third of pH variation. The remainder is unexplained variance (\\(\\varepsilon&#39;\\) and \\(\\varepsilon&#39;&#39;\\) from the Universal Model). NSE Nash-Sutcliffe Efficiency. Values &gt; 0 mean the model is better than simply predicting the mean everywhere. An R² of ~0.32 is modest but not unusual in DSM. This reflects the sources of residual variance discussed in Part I: measurement error, missing covariates, scale mismatches, and fine-scale soil variation that cannot be captured at 100m resolution. Figure 7.4: Observed vs Predicted pH from cross-validation. The red line shows perfect 1:1 prediction. How to read this scatterplot: Points on the red line are perfectly predicted. Vertical spread around the line shows prediction uncertainty. If points curve away from the line at high or low values, the model may be compressing extreme predictions toward the mean. This is a common Random Forest behaviour because predictions are averages of tree outputs and cannot exceed the range of training data. 7.3.7 Prediction Maps 7.3.7.1 Mean pH Map Figure 7.5: Predicted mean pH across the study area. This map represents the deterministic component \\(\\hat{m}(s)\\) from the Universal Model—the predicted pH value at each 100m pixel based on environmental covariates. Look for patterns that match your understanding of soil-landscape relationships. Higher pH in flat, poorly-drained areas may indicate carbonate accumulation. Lower pH on steep slopes may reflect greater leaching. These patterns should be consistent with the scorpan logic: relief influences drainage, which affects weathering and pH. Warning signs of artefacts: Abrupt straight-line boundaries may indicate covariate artefacts (e.g., satellite image edges, DEM tile boundaries) Uniform areas with no variation may indicate limited covariate information in those regions 7.3.7.2 Uncertainty Map (Standard Deviation) Figure 7.6: Prediction uncertainty (SD) - higher values indicate more uncertain predictions. For continuous predictions, we express uncertainty as a standard deviation (SD). This comes from Quantile Regression Forest: each of the trees in the forest makes its own prediction. Where trees agree, SD is low. Where trees disagree, SD is high. This SD map provides spatially explicit uncertainty—a key advantage of DSM over traditional mapping. High SD areas are where the model is uncertain, often occurring where: Covariate combinations are unlike the training data Soil patterns are complex or highly variable Training data are sparse Use this map to prioritise where additional sampling would improve predictions and to communicate confidence to map users. 7.3.7.3 Coefficient of Variation Map Figure 7.7: Coefficient of Variation (%) - relative uncertainty. CV expresses uncertainty relative to the predicted value: CV = (SD / Mean) × 100. Caution: For pH, CV is meaningful because pH values are never near zero. For variables like soil organic carbon that can approach zero, CV becomes artificially inflated where predicted values are low and should be interpreted carefully. 7.4 Nominal Classification (Clay_pH_Class) 7.4.1 Nature of the Variable Clay_pH_Class is a nominal variable: it consists of unordered categories created by combining clay content classes (low, medium, high) with pH classes (acid, neutral, alkaline). The result is up to 9 combinations like “low_clay_acid” or “high_clay_alkaline”. These categories have no inherent order. “low_clay_acid” is not “less than” “medium_clay_neutral” in any meaningful sense. This distinguishes nominal from ordinal variables and affects both the model and how we evaluate predictions. What this approach produces: A map showing the most likely class at each pixel Probability maps for each class An uncertainty map based on classification confidence What this approach cannot do: It cannot capture gradual transitions (a pixel is assigned to exactly one class) It struggles when some classes have few training samples Accuracy metrics treat all misclassifications equally, whether the error is “neighbouring” or distant in attribute space Important: Classifying variables that are naturally continuous (like clay content and pH) introduces artificial boundaries. Observations near class thresholds could easily fall into adjacent classes with slightly different measurements—this is the “soil continuum problem” discussed in Part I. 7.4.2 Feature Selection selected_features_nom &lt;- readRDS(file.path(OUTPUTS_DIR, &quot;models&quot;, &quot;selected_features_Clay_pH_Class.rds&quot;)) print(paste(&quot;Number of selected features:&quot;, length(selected_features_nom))) ## [1] &quot;Number of selected features: 28&quot; print(&quot;Selected features:&quot;) ## [1] &quot;Selected features:&quot; print(selected_features_nom) ## [1] &quot;bio1&quot; &quot;bio12&quot; &quot;bio13&quot; ## [4] &quot;bio14&quot; &quot;bio16&quot; &quot;bio17&quot; ## [7] &quot;bio5&quot; &quot;bio6&quot; &quot;ngd10&quot; ## [10] &quot;pet_penman_max&quot; &quot;pet_penman_mean&quot; &quot;pet_penman_min&quot; ## [13] &quot;pet_penman_range&quot; &quot;sfcWind_max&quot; &quot;sfcWind_mean&quot; ## [16] &quot;sfcWind_range&quot; &quot;fpar_030405_500m_mean&quot; &quot;fpar_030405_500m_sd&quot; ## [19] &quot;fpar_060708_500m_mean&quot; &quot;fpar_060708_500m_sd&quot; &quot;fpar_091011_500m_mean&quot; ## [22] &quot;fpar_091011_500m_sd&quot; &quot;fpar_120102_500m_mean&quot; &quot;dtm_curvature_250m&quot; ## [25] &quot;dtm_elevation_250m&quot; &quot;dtm_slope_250m&quot; &quot;dtm_tpi_250m&quot; ## [28] &quot;dtm_twi_500m&quot; The same Boruta procedure is applied, but now selecting features that help distinguish between soil classes rather than predicting a numeric value. 7.4.3 Model Training model_nom &lt;- readRDS(file.path(OUTPUTS_DIR, &quot;models&quot;, &quot;ranger_model_Clay_pH_Class.rds&quot;)) print(model_nom) ## Random Forest ## ## 424 samples ## 28 predictor ## 9 classes: &#39;high_clay_acid&#39;, &#39;high_clay_alkaline&#39;, &#39;high_clay_neutral&#39;, &#39;low_clay_acid&#39;, &#39;low_clay_alkaline&#39;, &#39;low_clay_neutral&#39;, &#39;medium_clay_acid&#39;, &#39;medium_clay_alkaline&#39;, &#39;medium_clay_neutral&#39; ## ## No pre-processing ## Resampling: Cross-Validated (10 fold, repeated 5 times) ## Summary of sample sizes: 382, 380, 382, 382, 379, 382, ... ## Resampling results across tuning parameters: ## ## mtry splitrule Accuracy Kappa ## 5 gini 0.4009844 0.2448634 ## 5 extratrees 0.4167866 0.2618934 ## 9 gini 0.4055288 0.2509430 ## 9 extratrees 0.4179784 0.2639396 ## 13 gini 0.3972352 0.2421422 ## 13 extratrees 0.4065326 0.2513619 ## ## Tuning parameter &#39;min.node.size&#39; was held constant at a value of 5 ## Kappa was used to select the optimal model using the largest value. ## The final values used for the model were mtry = 9, splitrule = extratrees ## and min.node.size = 5. print(&quot;Best hyperparameters:&quot;) ## [1] &quot;Best hyperparameters:&quot; print(model_nom$bestTune) ## mtry splitrule min.node.size ## 4 9 extratrees 5 For classification, Random Forest assigns each observation to the class that receives the most “votes” from individual trees. Each tree classifies the observation, and the forest aggregates these votes into class probabilities. This provides a form of fuzzy classification—rather than a single hard assignment, we obtain probabilities for each class (McBratney, Mendonça Santos and Minasny, 2003). 7.4.4 Validation metrics_nom &lt;- read_csv(file.path(OUTPUTS_DIR, &quot;validation&quot;, &quot;metrics_Clay_pH_Class.csv&quot;), show_col_types = FALSE) kable(metrics_nom, digits = 3, caption = &quot;Classification metrics for Clay_pH_Class&quot;) Table 7.2: Classification metrics for Clay_pH_Class Variable Type Accuracy Kappa Clay_pH_Class Nominal 0.418 0.264 What each metric tells you: Metric Meaning Interpretation Accuracy Proportion of observations correctly classified. With 9 classes, random guessing gives ~11% accuracy. Values above this indicate the model learned meaningful patterns. Kappa Agreement beyond chance, accounting for class frequencies. Kappa 0.2–0.4 is “fair” agreement; 0.4–0.6 is “moderate”. Accuracy of ~39% is better than random but limited. This is typical when classifying continuous variables into categories, especially with class imbalance (some classes have many more observations than others). Figure 7.8: Confusion matrix showing predicted vs observed classes. Diagonal cells are correct predictions. How to read the confusion matrix: Diagonal cells (top-left to bottom-right): Correct predictions. Higher numbers = better. Off-diagonal cells: Misclassifications. Look for patterns—which classes are confused with each other? Empty rows or columns: Classes with no correct predictions. This indicates the model ignores minority classes, a common problem with imbalanced data. If “high_clay_acid” is often misclassified as “medium_clay_acid”, the model struggles to distinguish clay content but captures pH better. This suggests clay-related covariates may be weaker predictors than pH-related covariates. 7.4.5 Prediction Maps 7.4.5.1 Predicted Class Map Figure 7.9: Predicted class based on maximum probability. Each pixel is assigned to the class with the highest predicted probability. Check if spatial patterns match landscape expectations: high clay in floodplains? Acid soils in high-rainfall uplands? 7.4.5.2 Maximum Probability Map Figure 7.10: Maximum class probability - higher values indicate more confident predictions. Why probability is uncertainty for nominal variables: For classification, the model outputs a probability for each class. These probabilities reflect the votes from individual trees. If the maximum probability is 0.9, the model is confident—most trees agree. If it is 0.2 (close to 1/9 = 0.11 for random guessing with 9 classes), the model is uncertain—trees disagree about which class applies. Low probability areas are “transition zones” where the model cannot confidently assign a single class. These often correspond to gradual soil transitions in the landscape—precisely the areas where the categorical representation is most problematic. 7.4.5.3 Uncertainty Map Figure 7.11: Classification uncertainty (1 - max probability). This map shows 1 minus the maximum probability. High values mean the model is uncertain about class assignment. 7.4.5.4 Confident Predictions Map Figure 7.12: Only predictions where max probability &gt;= 0.4 are shown. This map applies a confidence threshold: predictions below 40% probability are masked. Areas in white/NA are where the model is not confident enough to assign a class. When delivering classification maps to users, consider providing probability maps alongside class maps, or masking uncertain areas to avoid false precision. 7.5 Ordinal Modelling (pH_Class) 7.5.1 Nature of the Variable pH_Class is an ordinal variable: categories with a natural order. The levels are: very_acid &lt; acid &lt; neutral &lt; alkaline &lt; very_alkaline. Unlike nominal classes, order matters. Predicting “acid” when the truth is “neutral” (one class away) is a smaller error than predicting “very_acid” (two classes away). This ordering changes how we validate predictions and interpret errors. What this approach produces: A map of predicted pH classes An instability map showing where predictions are near class boundaries Validation metrics that account for class order (distance-weighted errors) What this approach cannot do: It assumes equal spacing between classes (the “distance” from very_acid to acid equals acid to neutral), which may not reflect true chemical differences It does not preserve the original continuous pH information 7.5.2 Ordinal Encoding ordinal_encoding &lt;- readRDS(file.path(OUTPUTS_DIR, &quot;models&quot;, &quot;ordinal_encoding.rds&quot;)) print(&quot;Ordinal level lookup:&quot;) ## [1] &quot;Ordinal level lookup:&quot; print(ordinal_encoding$lookup) ## level integer ## 1 very_acid 1 ## 2 acid 2 ## 3 neutral 3 ## 4 alkaline 4 ## 5 very_alkaline 5 print(&quot;Reclassification thresholds:&quot;) ## [1] &quot;Reclassification thresholds:&quot; print(ordinal_encoding$reclassify_matrix) ## NULL The approach: Convert ordered classes to integers: very_acid = 1, acid = 2, neutral = 3, alkaline = 4, very_alkaline = 5 Train a regression model to predict these integers as if they were continuous Round predictions back to classes using midpoint thresholds (1.5, 2.5, 3.5, 4.5) Limitation: This assumes equal spacing between classes. In reality, the chemical difference between pH 4 and 5 may not equal the difference between pH 6 and 7. The ordinal approach is a compromise between continuous prediction and nominal classification. 7.5.3 Feature Selection selected_features_ord &lt;- readRDS(file.path(OUTPUTS_DIR, &quot;models&quot;, &quot;selected_features_pH_Class.rds&quot;)) print(paste(&quot;Number of selected features:&quot;, length(selected_features_ord))) ## [1] &quot;Number of selected features: 29&quot; print(&quot;Selected features:&quot;) ## [1] &quot;Selected features:&quot; print(selected_features_ord) ## [1] &quot;bio1&quot; &quot;bio12&quot; &quot;bio13&quot; ## [4] &quot;bio14&quot; &quot;bio16&quot; &quot;bio17&quot; ## [7] &quot;bio5&quot; &quot;bio6&quot; &quot;ngd10&quot; ## [10] &quot;pet_penman_max&quot; &quot;pet_penman_mean&quot; &quot;pet_penman_min&quot; ## [13] &quot;pet_penman_range&quot; &quot;sfcWind_max&quot; &quot;sfcWind_mean&quot; ## [16] &quot;sfcWind_range&quot; &quot;fpar_030405_500m_mean&quot; &quot;fpar_030405_500m_sd&quot; ## [19] &quot;fpar_060708_500m_mean&quot; &quot;fpar_060708_500m_sd&quot; &quot;fpar_091011_500m_mean&quot; ## [22] &quot;fpar_091011_500m_sd&quot; &quot;fpar_120102_500m_mean&quot; &quot;fpar_120102_500m_sd&quot; ## [25] &quot;dtm_curvature_250m&quot; &quot;dtm_elevation_250m&quot; &quot;dtm_slope_250m&quot; ## [28] &quot;dtm_tpi_250m&quot; &quot;dtm_twi_500m&quot; 7.5.4 Model Training model_ord &lt;- readRDS(file.path(OUTPUTS_DIR, &quot;models&quot;, &quot;ranger_model_pH_Class_ordinal.rds&quot;)) print(model_ord) ## Random Forest ## ## 424 samples ## 29 predictor ## ## No pre-processing ## Resampling: Cross-Validated (10 fold, repeated 5 times) ## Summary of sample sizes: 382, 382, 380, 382, 382, 380, ... ## Resampling results across tuning parameters: ## ## mtry splitrule RMSE Rsquared MAE ## 5 variance 0.7677947 0.3509020 0.6090019 ## 5 extratrees 0.7595814 0.3613039 0.6040114 ## 10 variance 0.7712644 0.3465458 0.6116310 ## 10 extratrees 0.7595455 0.3619105 0.6033011 ## 15 variance 0.7716605 0.3469278 0.6122184 ## 15 extratrees 0.7631243 0.3569775 0.6059889 ## ## Tuning parameter &#39;min.node.size&#39; was held constant at a value of 5 ## RMSE was used to select the optimal model using the smallest value. ## The final values used for the model were mtry = 10, splitrule = extratrees ## and min.node.size = 5. print(&quot;Best hyperparameters:&quot;) ## [1] &quot;Best hyperparameters:&quot; print(model_ord$bestTune) ## mtry splitrule min.node.size ## 4 10 extratrees 5 We use Random Forest in regression mode, treating the integer-coded classes as a continuous target. This allows the model to produce predictions like 2.7, which is then rounded to class 3 (neutral). The regression approach respects the ordering of classes—predicting 2.7 is closer to truth=3 than predicting 1.2. 7.5.5 Validation metrics_ord &lt;- read_csv(file.path(OUTPUTS_DIR, &quot;validation&quot;, &quot;metrics_pH_Class_ordinal.csv&quot;), show_col_types = FALSE) kable(metrics_ord, digits = 3, caption = &quot;Ordinal classification metrics for pH_Class&quot;) Table 7.3: Ordinal classification metrics for pH_Class Variable Type MAE_rank Mean_distance pH_Class Ordinal 0.554 0.554 What each metric tells you: Metric Meaning Interpretation MAE_rank Mean Absolute Error in class units—average distance between observed and predicted class. MAE_rank = 0.58 means predictions are typically about half a class off. Mean_distance Weighted average error accounting for all misclassifications. Lower values = predictions closer to truth on the ordinal scale. Why ordinal metrics differ from nominal: In ordinal validation, not all errors are equal. Predicting class 3 when truth is class 4 (distance = 1) is less severe than predicting class 1 (distance = 3). The metrics weight errors by their distance on the ordinal scale. Figure 7.13: Ordinal confusion matrix. Errors near the diagonal are minor; errors far from diagonal are severe. How to read the ordinal confusion matrix: Diagonal: Correct predictions Adjacent cells (±1): Minor errors, one class off Distant cells: Severe errors, multiple classes off Look for systematic patterns: does the model consistently underpredict (errors below diagonal) or overpredict (errors above diagonal)? 7.5.6 Prediction Maps 7.5.6.1 Latent Values Map Figure 7.14: Continuous ‘latent’ predictions before reclassification to ordinal classes. This map shows the raw regression output before rounding to classes. Values range from ~1 to ~5. This reveals continuous spatial gradients that are lost when we assign discrete classes. 7.5.6.2 Reclassified Ordinal Class Map Figure 7.15: Ordinal classes derived from latent predictions using midpoint thresholds. Classes 1–5 correspond to: very_acid, acid, neutral, alkaline, very_alkaline. Compare this to the continuous pH map—patterns should be similar but expressed as discrete classes. 7.5.6.3 Standard Deviation Map Figure 7.16: Prediction uncertainty (SD) from the ordinal regression model. As with continuous predictions, SD reflects disagreement among trees. High SD means the model is uncertain about the latent value. 7.5.6.4 Class Instability Map Figure 7.17: Class instability - high values where predictions are near class boundaries. Why ordinal uncertainty is about instability: For ordinal variables, a unique source of uncertainty is class boundary instability. A prediction of 2.48 is very close to the 2.5 threshold between classes 2 and 3. Small changes in input data or model parameters could flip the assigned class. Instability combines prediction uncertainty (SD) with proximity to class boundaries. High instability means: “The model predicts a value near a threshold AND is uncertain about that value.” Areas with high instability should be flagged as uncertain classifications, even if they receive a class label in the map. This is a form of uncertainty specific to ordinal classification that does not exist for continuous predictions. 7.6 Area of Applicability (AOA) 7.6.1 Why AOA is Necessary Cross-validation tells us how well the model predicts where we have training data. But we apply the model to the entire study area, including locations with covariate combinations unlike any training point. This relates to a fundamental limitation of empirical models: they cannot extrapolate reliably beyond the range of training data. The scorpan model \\(S = f(Q) + \\varepsilon\\) is fitted using available observations, and predictions outside the training domain are unreliable. The Area of Applicability (AOA) identifies which parts of the study area have covariate combinations similar to the training data (Meyer and Pebesma, 2021). Outside the AOA, the model is extrapolating, and predictions may be unreliable regardless of cross-validation performance. 7.6.2 Dissimilarity Index (DI) Figure 7.18: Dissimilarity Index - how different each location is from training data in covariate space. DI measures how different each pixel’s covariate values are from the nearest training point in multivariate covariate space. It is calculated using weighted Euclidean distance, where weights reflect variable importance from the model. DI near 0 means the pixel is similar to training data. High DI means the covariate combination was not well represented in training—the model is extrapolating. 7.6.3 AOA Binary Mask Figure 7.19: Area of Applicability mask. Green = within AOA (trustworthy), Grey = outside AOA (unreliable). Pixels are classified as inside (1) or outside (0) the AOA based on a DI threshold derived from cross-validation. Green areas: Model has seen similar covariate combinations. Predictions are supported by training data. Grey areas: Covariate combinations not represented in training. Predictions are extrapolations and should not be trusted. ## AOA coverage: 91.5 % of study area 7.6.4 Local Point Density (LPD) Figure 7.20: Local Point Density - number of training points supporting predictions at each location. LPD counts how many training points are “nearby” in covariate space. Even within the AOA, some areas have more training support than others. LPD = 0 means no training points are nearby—a strong warning. Higher LPD means multiple training observations support the prediction, increasing confidence. 7.6.5 Predictions Masked to AOA Figure 7.21: pH predictions only shown within the Area of Applicability. This map shows predictions only where the model can be trusted. White/NA areas are outside the AOA. Practical recommendations: Always report AOA coverage alongside prediction maps Mask or flag predictions outside AOA when delivering products to users Use areas outside AOA to prioritise future sampling—these are gaps in training data coverage 7.7 Summary Session Variable Type Approach Key Metric Uncertainty Type 1 Continuous (pH) QRF regression R², RMSE Standard deviation from tree disagreement 2 Nominal (Clay_pH_Class) Classification RF Accuracy, Kappa Probability of assigned class 3 Ordinal (pH_Class) Integer regression MAE_rank Instability near class boundaries 4 AOA Dissimilarity analysis Coverage % Inside/outside applicability Key points: Match the model to the variable type. Continuous, nominal, and ordinal variables require different approaches and produce different outputs. The choice follows from the mathematical nature of the variable and how errors should be evaluated. Models are empirical, not mechanistic. We fit statistical relationships between soil and covariates (\\(S = f(Q) + \\varepsilon\\)) rather than simulating physical processes. This is powerful but limited—models cannot explain causation or extrapolate reliably. Cross-validation has limits. It evaluates prediction at training locations, not in unsampled areas. Spatial clustering of training data can lead to overly optimistic error estimates. Always report uncertainty. The form of uncertainty depends on variable type: standard deviation for continuous variables, class probabilities for nominal variables, and instability for ordinal variables. Use AOA to identify reliable predictions. Do not trust predictions in areas with covariate combinations unlike the training data. References References "],["soil-data-sharing.html", "Chapter 8 Soil Data Sharing 8.1 Data sharing and export formats 8.2 Metadata 8.3 Web Services", " Chapter 8 Soil Data Sharing 8.1 Data sharing and export formats 8.2 Metadata 8.3 Web Services "],["references-3.html", "References", " References "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
